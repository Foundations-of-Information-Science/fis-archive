<?xml version="1.0" encoding="windows-1252"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: [Fis] Is Shannon's Entropy of General Applicability?</title>
<meta name="Author" content="Michael Devereux (dbar_x@cybermesa.com)" />
<meta name="Subject" content="[Fis] Is Shannon's Entropy of General Applicability?" />
<meta name="Date" content="2004-04-20" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>[Fis] Is Shannon's Entropy of General Applicability?</h1>
<!-- received="Tue Apr 20 08:22:01 2004" -->
<!-- isoreceived="20040420062201" -->
<!-- sent="Tue, 20 Apr 2004 00:20:20 -0600" -->
<!-- isosent="20040420062020" -->
<!-- name="Michael Devereux" -->
<!-- email="dbar_x@cybermesa.com" -->
<!-- subject="[Fis] Is Shannon's Entropy of General Applicability?" -->
<!-- id="4084C124.708@cybermesa.com" -->
<!-- charset="windows-1252" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1441.html" title="Karl Javorszky: &quot;AW: [Fis]  Data, observations and distributions / was: Re: Probabilistic Entropy&quot;">Next message</a> ]
[ <a href="1439.html" title="Loet Leydesdorff: &quot;RE: [Fis]  Data, observations and distributions / was: Re: Probabilistic Entropy&quot;">Previous message</a> ]
<!-- unextthread="start" -->
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Michael Devereux &lt;<a href="mailto:dbar_x&#64;cybermesa.com?Subject=Re:%20[Fis]%20Is%20Shannon's%20Entropy%20of%20General%20Applicability?">dbar_x@cybermesa.com</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Tue 20 Apr 2004 - 08:20:20 CEST</span><br />
</address>
<p>
Dear Michel and colleagues,
<br />
<p>Thanks for your comments. I think I may understand, as Loet wrote, that 
<br />
it doesn’t make much sense to discuss exchanges of words, like truth or 
<br />
power, in terms of the thermodynamics of these exchanges. And certainly 
<br />
not, if one considers thermodynamics, just as its name implies, to be 
<br />
all about heat flow.
<br />
The idea that impressed me most, while studying thermodynamics as a 
<br />
student, was disorder, its relationship to the possible unitary 
<br />
direction of evolution of the natural world, and characterization of the 
<br />
term entropy as a measure of disorder. That characterization is usual in 
<br />
elementary physics textbooks on thermodynamics. Of course, one also 
<br />
reads that entropy may be determined, in certain situations, by 
<br />
Clausius’ formulation Delta Q / T.
<br />
There does seem to be a marked distinction between the use of the 
<br />
entropy concept to engineer heat engines, or refrigerators, or even, 
<br />
say, to describe the evolution of a star; and its use to predict the 
<br />
correspondence between the pattern of dots and dashes sent by a 
<br />
telegraph operator, with the noisy, deteriorated pattern received at the 
<br />
other end of the telegraph wire. Unless, that is, one considers all 
<br />
these phenomena within the context of disorder or uncertainty.
<br />
I agree with you, Michel, about some of your remarks, and disagree about 
<br />
others. I don’t agree that “Shannon’s formula is derived from 
<br />
probability theory, outside any physical field of application....” 
<br />
Shannon was considering an application to discrete communication systems 
<br />
and the correlation (largely positive) between the binary signal of dots 
<br />
and dashes sent by a telegraph operator and those dots and dashes 
<br />
received on the other end of the line. ( Shannon, Bell Sys. Tech. J. 27, 
<br />
3, 1948, p. 382 ff.) He derived his famous mathematical formula for that 
<br />
particular physical application.
<br />
You also wrote that “Shannon’s entropy does not generalize 
<br />
thermodynamical entropy. They may be related in a limited number of 
<br />
situations. One is of mathematical nature, the other is of physical 
<br />
nature.” By Shannon’s entropy we mean the uncertainty of some system 
<br />
which can be described (with some degree of accuracy) by the formula H = 
<br />
- Sum p log(p).
<br />
I’m sure you’re aware that this mathematical formula, H, can be traced 
<br />
back, historically, at least as far as Boltzmann’s renowned H theorem. 
<br />
That’s during the nineteenth century, likely before Shannon was born. 
<br />
Unquestionably, I would say, Boltzmann was using H to explain, and to 
<br />
generalize, thermodynamic entropy. There was, after all, no information 
<br />
theory then. We chemists, engineers and physicists claimed ownership of 
<br />
the H formulation long before anyone else.
<br />
Of course science incorporates mathematics, as well as observation, into 
<br />
it’s methods of inquiry. H is a mathematical formula, of a mathematical 
<br />
nature, as you say, and it was used by Boltzmann to describe the entropy 
<br />
(a state function) of physical systems, such as heat engines and other 
<br />
tangible material devices.
<br />
Shannon, himself, acknowledged that the formula, when he derived it for 
<br />
discrete communication systems, was identical with that entropy formula 
<br />
already in use by the physical sciences: “The form of H will be 
<br />
recognized as that of entropy as defined in certain formulations of 
<br />
statistical mechanics..... H is then, for example, the H in Boltzmann’s 
<br />
famous H theorem.” (Shannon, Bell Sys. Tech. J., 27, 3, 1948, p. 393)
<br />
I’d like to reiterate the entirely general scope of the H formulation to 
<br />
ALL natural phenomena. We often call it the von Neumann-Shannon formula 
<br />
because it was again derived, in 1932, sixteen years before Shannon’s 
<br />
publication, by von Neumann. (Mathematische Grundlagen der 
<br />
Quantenmechanik).
<br />
Von Neumann’s formula describes the connection between the mathematical 
<br />
model of quantum mechanics and what humans observe of the physical 
<br />
world. That’s all of the physical world, as far as one can determine, 
<br />
because quantum mechanics seems to describe everything. And the 
<br />
quantum-mechanical explanation is entirely probabilistic. That’s all 
<br />
there is, as Feynman said. So, von Neumann’s formula correlates the 
<br />
probable result for an ensemble of measurements, with the wave function 
<br />
that models a physical system.
<br />
I agree with you, Michel, that the original thermodynamic definition of 
<br />
entropy was introduced, historically, in terms of heat, and not 
<br />
probability. But the founders of thermodynamics, including Boltzmann and 
<br />
Gibbs, recognized, almost immediately, that the more general physical 
<br />
principle was probabilistic. Liouville’s theorem, as applied to 
<br />
Boltzmann’s phase-space conception for entropy, is a theorem of 
<br />
probabilities.
<br />
You ask, Michel, “In science, is there any example of calculation 
<br />
without input data? So, the precision of the model is judged from the 
<br />
departure of predicted from observed, the predicted being also depending 
<br />
on the precision of the input data, which are measured data.?” I know of 
<br />
no example where observed values can be calculated without input 
<br />
parameters from observational data, or perhaps, a guess at those input 
<br />
parameters.
<br />
But, not all mathematical models incorporated into established 
<br />
scientific theory, or scientific law, are judged empirically. I realize 
<br />
that science is unique, among modes of inquiry, in its dependence on 
<br />
observation. But, it also depends upon mathematics, whose truths, I take 
<br />
it, are what Immanuel Kant called apodictic. For example, 1 + 1 = 2, not 
<br />
because it says so in an arithmetic primer, but, rather, because our 
<br />
mind declares those concepts must be true. No experiment need be performed.
<br />
We only know that Newton’s law, F = m a, describes nature quite 
<br />
precisely because such has been observed. Likewise, for Einstein’s 
<br />
famous equation, E = m c^2, and nearly all the other established laws of 
<br />
science. But, not all. For example, we call it a law of hydrodynamics 
<br />
(fluid dynamics) that flux is conserved; known also as the equation of 
<br />
continuity. This law states that over some closed surface, the rate of 
<br />
flow of a fluid in or out of that surface must equal the rate at which 
<br />
fluid is created or destroyed within the volume enclosed by that 
<br />
surface. The equation of continuity is pure reason, and no observations 
<br />
need confirm it.
<br />
Another example is Gauss’ law for electrodynamics. We say that if a 
<br />
static electric field varies as the inverse square of distance from its 
<br />
source, then the electric field flux (integral of electric field vector 
<br />
times surface area element) over any closed surface is equal to the 
<br />
enclosed electric charge divided by the physical constant epsilon. 
<br />
Gauss’ law requires no experimental confirmation. It’s exact.
<br />
I understand that these laws are just mathematics borrowed by science. 
<br />
But, that’s how science works. Science’s methods are not exclusively 
<br />
observational.
<br />
If you think I’m hinting that the Second Law ot Thermodynamics (which 
<br />
describes how the entropy of a physical system can change) may depend on 
<br />
the mathematics of probability, you’re right.
<br />
Thanks to everyone for the comments.
<br />
Cordially,
<br />
Michael Devereux dbar_x&#64;cybermesa.com
<br />
<p><p><p><p><p>_______________________________________________
<br />
fis mailing list
<br />
fis&#64;listas.unizar.es
<br />
<a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
<br />
<span id="received"><dfn>Received on</dfn> Tue Apr 20 08:22:01 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1441.html" title="Next message in the list">Karl Javorszky: "AW: [Fis]  Data, observations and distributions / was: Re: Probabilistic Entropy"</a></li>
<li><dfn>Previous message</dfn>: <a href="1439.html" title="Previous message in the list">Loet Leydesdorff: "RE: [Fis]  Data, observations and distributions / was: Re: Probabilistic Entropy"</a></li>
<!-- lnextthread="start" -->
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1440" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1440" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1440" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1440" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:46 CET
</em></small></p>
</body>
</html>
