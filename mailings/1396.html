<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: Re: [Fis] FIS / introductory text / 5 April 2004</title>
<meta name="Author" content="Dr. Shu-Kun Lin (lin@mdpi.org)" />
<meta name="Subject" content="Re: [Fis] FIS / introductory text / 5 April 2004" />
<meta name="Date" content="2004-04-05" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>Re: [Fis] FIS / introductory text / 5 April 2004</h1>
<!-- received="Mon Apr  5 16:27:40 2004" -->
<!-- isoreceived="20040405142740" -->
<!-- sent="Mon, 05 Apr 2004 16:22:10 +0200" -->
<!-- isosent="20040405142210" -->
<!-- name="Dr. Shu-Kun Lin" -->
<!-- email="lin@mdpi.org" -->
<!-- subject="Re: [Fis] FIS / introductory text / 5 April 2004" -->
<!-- id="40716B92.6050708@mdpi.org" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="407164D4.9020207&#64;mdpi.org" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1397.html" title="Loet Leydesdorff: &quot;RE: [Fis] FIS / introductory text / 5 April 2004&quot;">Next message</a> ]
[ <a href="1395.html" title="Dr. Shu-Kun Lin: &quot;Re: [Fis] FIS / introductory text / 5 April 2004&quot;">Previous message</a> ]
[ <a href="1394.html" title="Dr. Shu-Kun Lin: &quot;Re: [Fis] FIS / introductory text / 5 April 2004&quot;">In reply to</a> ]
<!-- unextthread="start" -->
[ <a href="1401.html" title="Pedro C. Marijuán: &quot;Re: [Fis] FIS / introductory text / 5 April 2004&quot;">Next in thread</a> ]
 [ <a href="#replies">Replies</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Dr. Shu-Kun Lin &lt;<a href="mailto:lin&#64;mdpi.org?Subject=Re:%20[Fis]%20FIS%20/%20introductory%20text%20/%205%20April%202004">lin@mdpi.org</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Mon 05 Apr 2004 - 16:22:10 CEST</span><br />
</address>
<p>
&quot;&gt;&quot; should be &quot;=&quot;. I mean &quot;Delta S = - Delta (information)&quot;,
<br />
information loss is related to entropy increase. Sorry.
<br />
<p>Dr. Shu-Kun Lin wrote:
<br />
<p><em class="quotelev1">&gt; Dear Loet,
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; You mean entropy S is equal to information (I), or almost equal. Can 
</em><br />
<em class="quotelev1">&gt; you still give
</em><br />
<em class="quotelev1">&gt; a little bit of sympathy to the relation that Delta S &gt; - Delta 
</em><br />
<em class="quotelev1">&gt; (information) and make some
</em><br />
<em class="quotelev1">&gt; comments on this different relation? If we can agree on the relation that
</em><br />
<em class="quotelev1">&gt; Delta S &gt; - Delta (information),
</em><br />
<em class="quotelev1">&gt; then we are ready to ask
</em><br />
<em class="quotelev1">&gt; &quot;why information loss is related to entropy&quot;, a question asked by 
</em><br />
<em class="quotelev1">&gt; physicists at the
</em><br />
<em class="quotelev1">&gt; <a href="../../www.lns.cornell.edu/spr/2000-12/msg0030047.html">http://www.lns.cornell.edu/spr/2000-12/msg0030047.html</a> website,
</em><br />
<em class="quotelev1">&gt; and try to answer.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Michel, thank you for your introduction.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Shu-Kun
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Loet Leydesdorff wrote:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Dear Michel,
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; The relation between thermodynamic entropy and the information is
</em><br />
<em class="quotelev2">&gt;&gt; provided by the Szilard-Brillouin relation as follows:
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Delta S &gt;= k(B) Delta H
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; (W. Ebeling. Chaos, Ordnung und Information. Frankfurt a.M.: Harri
</em><br />
<em class="quotelev2">&gt;&gt; Deutsch Thun, 1991, at p. 60.)
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; k(B) in this formula is the Boltzmann constant. Thus, a physical change
</em><br />
<em class="quotelev2">&gt;&gt; of the system can provide an information, but it does not have to.
</em><br />
<em class="quotelev2">&gt;&gt; Unlike the thermodynamic entropy, probabilistic entropy has no
</em><br />
<em class="quotelev2">&gt;&gt; dimensionality (because it is mathematically defined). The Boltmann
</em><br />
<em class="quotelev2">&gt;&gt; constant takes care of the correction in the dimensionality in the
</em><br />
<em class="quotelev2">&gt;&gt; equation.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; When applied as a statistics to other systems (e.g., biological ones)
</em><br />
<em class="quotelev2">&gt;&gt; one obtains another (specific) theory of communication in which one can
</em><br />
<em class="quotelev2">&gt;&gt; perhaps find another relation between the (in this case biological)
</em><br />
<em class="quotelev2">&gt;&gt; information and the probabilistic entropy. This can be elaborated for
</em><br />
<em class="quotelev2">&gt;&gt; each specific domain.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; With kind regards,
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Loet
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;  
</em><br />
<em class="quotelev2">&gt;&gt;  _____ 
</em><br />
<em class="quotelev2">&gt;&gt; Loet Leydesdorff
</em><br />
<em class="quotelev2">&gt;&gt; Science &amp; Technology Dynamics, University of Amsterdam
</em><br />
<em class="quotelev2">&gt;&gt; Amsterdam School of Communications Research (ASCoR)
</em><br />
<em class="quotelev2">&gt;&gt; Kloveniersburgwal 48, 1012 CX  Amsterdam
</em><br />
<em class="quotelev2">&gt;&gt; Tel.: +31-20-525 6598; fax: +31-20-525 3681
</em><br />
<em class="quotelev2">&gt;&gt; &lt;mailto:loet&#64;leydesdorff.net&gt; loet&#64;leydesdorff.net;
</em><br />
<em class="quotelev2">&gt;&gt; &lt;<a href="../../www.leydesdorff.net/default.htm">http://www.leydesdorff.net/</a>&gt; <a href="../../www.leydesdorff.net/default.htm">http://www.leydesdorff.net</a>
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; -----Original Message-----
</em><br />
<em class="quotelev2">&gt;&gt; From: fis-bounces&#64;listas.unizar.es [mailto:fis-bounces&#64;listas.unizar.es]
</em><br />
<em class="quotelev2">&gt;&gt; On Behalf Of Michel Petitjean
</em><br />
<em class="quotelev2">&gt;&gt; Sent: Monday, April 05, 2004 9:15 AM
</em><br />
<em class="quotelev2">&gt;&gt; To: fis&#64;listas.unizar.es
</em><br />
<em class="quotelev2">&gt;&gt; Subject: [Fis] FIS / introductory text / 5 April 2004
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; 2004 FIS session introductory text.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Dear FISers,
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; I would like to thank Pedro Marijuan for his kind invitation
</em><br />
<em class="quotelev2">&gt;&gt; to chair the 2004 FIS session. The session is focussed on &quot;Entropy and
</em><br />
<em class="quotelev2">&gt;&gt; Information&quot;. It is vast, that I am afraid to be able only to evoke some
</em><br />
<em class="quotelev2">&gt;&gt; general aspects, discarding specific technical developments.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;     Entropy and Information: two polymorphic concepts.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Although these two concepts are undoubtly related, they have different
</em><br />
<em class="quotelev2">&gt;&gt; stories.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Let us consider first the Information concept.
</em><br />
<em class="quotelev2">&gt;&gt; There was many discussions in the FIS list about the meaning
</em><br />
<em class="quotelev2">&gt;&gt; of Information. Clearly, there are several definitions.
</em><br />
<em class="quotelev2">&gt;&gt; The information concept that most people have in mind is outside the
</em><br />
<em class="quotelev2">&gt;&gt; scope of this text: is it born with Computer Sciences, or is it born
</em><br />
<em class="quotelev2">&gt;&gt; with Press, or does it exist since a so long time that nobody could date
</em><br />
<em class="quotelev2">&gt;&gt; it? Neglecting the definitions from the dictionnaries (for each language
</em><br />
<em class="quotelev2">&gt;&gt; and culture), I would say that anybody has his own concept. Philosophers
</em><br />
<em class="quotelev2">&gt;&gt; and historians have to look. The content of the FIS archives suggests
</em><br />
<em class="quotelev2">&gt;&gt; that the field is vast.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Now let us look to scientific definitions. Those arising from
</em><br />
<em class="quotelev2">&gt;&gt; mathematics are rigorous, but have different meanings. An example is the
</em><br />
<em class="quotelev2">&gt;&gt; information concept emerging from information theory (Hartley, Wiener,
</em><br />
<em class="quotelev2">&gt;&gt; Shannon, Renyi,...). This concept, which arises from probability theory,
</em><br />
<em class="quotelev2">&gt;&gt; has little connections with the Fisher information, which arises also
</em><br />
<em class="quotelev2">&gt;&gt; from probability theory. The same word is used, but two rigorous
</em><br />
<em class="quotelev2">&gt;&gt; concepts are defined. One is mostly related to coding theory, and the
</em><br />
<em class="quotelev2">&gt;&gt; other is related to estimation theory. One deals mainly with non
</em><br />
<em class="quotelev2">&gt;&gt; numerical finite discrete distributions, and the other is based on
</em><br />
<em class="quotelev2">&gt;&gt; statistics from samples of parametrized family of distributions. Even
</em><br />
<em class="quotelev2">&gt;&gt; within the framework of information theory, there are several
</em><br />
<em class="quotelev2">&gt;&gt; definitions of information (e.g. see the last chapter of Renyi's book on
</em><br />
<em class="quotelev2">&gt;&gt; Probability Theory). This situation arises often in mathematics: e.g.,
</em><br />
<em class="quotelev2">&gt;&gt; there are several concepts of &quot;distance&quot;, and, despite the basic axioms
</em><br />
<em class="quotelev2">&gt;&gt; they all satisfy, nobody would say that they have the same meaning, even
</em><br />
<em class="quotelev2">&gt;&gt; when they are defined on a common space.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Then, mathematical tools are potential (and sometimes demonstrated)
</em><br />
<em class="quotelev2">&gt;&gt; simplified models for physical phenomenons. On the other hand,
</em><br />
<em class="quotelev2">&gt;&gt; scientists may publish various definitions of information for physical
</em><br />
<em class="quotelev2">&gt;&gt; situations. It does not mean that any of these definitions should be
</em><br />
<em class="quotelev2">&gt;&gt; confused between themselves and confused with the mathematical ones. In
</em><br />
<em class="quotelev2">&gt;&gt; many papers, the authors insist on the analogies between their own
</em><br />
<em class="quotelev2">&gt;&gt; concepts and those previously published by other authors: this attitude
</em><br />
<em class="quotelev2">&gt;&gt; may convince the reviewers of the manuscript that the work has interest,
</em><br />
<em class="quotelev2">&gt;&gt; but contribute to the general confusion, particularly when the confusing
</em><br />
<em class="quotelev2">&gt;&gt; terms are recorded in the bibliographic databases. Searching in
</em><br />
<em class="quotelev2">&gt;&gt; databases with the keyword &quot;information&quot; would lead to a considerable
</em><br />
<em class="quotelev2">&gt;&gt; number of hits: nobody would try it without constraining the search with
</em><br />
<em class="quotelev2">&gt;&gt; other terms (did some of you tried?).
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; We consider now the Entropy concepts. The two main ones are the
</em><br />
<em class="quotelev2">&gt;&gt; informational entropy and the thermodynamical entropy. The first one has
</em><br />
<em class="quotelev2">&gt;&gt; non ambiguous relations with information (in the sense of information
</em><br />
<em class="quotelev2">&gt;&gt; theory), since both are defined within the framework of a common theory.
</em><br />
<em class="quotelev2">&gt;&gt; Let us look now to the thermodynamical entropy, which was defined by
</em><br />
<em class="quotelev2">&gt;&gt; Rudolf Clausius in 1865. It is a physical concept, usually introduced
</em><br />
<em class="quotelev2">&gt;&gt; from the Carnot Cycle. The existence of entropy is postulated, and it is
</em><br />
<em class="quotelev2">&gt;&gt; a state function of the system. Usual variables are temperature and
</em><br />
<em class="quotelev2">&gt;&gt; pressure. Entropy calculations are sometimes made discarding the
</em><br />
<em class="quotelev2">&gt;&gt; implicit assumptions done for an idealized Carnot Cycle. Here come
</em><br />
<em class="quotelev2">&gt;&gt; difficulties. E.g., the whole universe is sometimes considered as a
</em><br />
<em class="quotelev2">&gt;&gt; system for which the the entropy is assumed to have sense. Does the
</em><br />
<em class="quotelev2">&gt;&gt; equilibrium of such a system has sense? Does thermodynamical state
</em><br />
<em class="quotelev2">&gt;&gt; functions make sense here? And what about &quot;the&quot; temperature? These
</em><br />
<em class="quotelev2">&gt;&gt; latter variable, even when viewed as a function of coordinates and/or
</em><br />
<em class="quotelev2">&gt;&gt; time, has sense only for a restricted number of situations. These
</em><br />
<em class="quotelev2">&gt;&gt; difficulties appear for many other systems. At other scales, they may
</em><br />
<em class="quotelev2">&gt;&gt; appear for microscopic systems, and for macroscopic systems unrelated to
</em><br />
<em class="quotelev2">&gt;&gt; thermochemistry.
</em><br />
<em class="quotelev2">&gt;&gt; In fact, what is often implicitly postulated is that the thermodynamical
</em><br />
<em class="quotelev2">&gt;&gt; entropy theory could work outside thermodynamics.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Statistical mechanics creates a bridge between microscopic and
</em><br />
<em class="quotelev2">&gt;&gt; macroscopic models, as evidenced from the work of Boltzmann. These two
</em><br />
<em class="quotelev2">&gt;&gt; models are different. One is a mathematical model for an idealized
</em><br />
<em class="quotelev2">&gt;&gt; physical situation (punctual balls, elastic collisions, distribution of
</em><br />
<em class="quotelev2">&gt;&gt; states, etc..), and the other is a simplified physical model, working
</em><br />
<em class="quotelev2">&gt;&gt; upon a restricted number of conditions. The expression of the entropy,
</em><br />
<em class="quotelev2">&gt;&gt; calculated via statistical mechanics methods, is formally similar to the
</em><br />
<em class="quotelev2">&gt;&gt; informational entropy. This latter has appeared many decades after the
</em><br />
<em class="quotelev2">&gt;&gt; former. Thus, the pioneers of information theory (Shannon, von Neumann)
</em><br />
<em class="quotelev2">&gt;&gt; who retain the term &quot;entropy&quot;, are undoubtly responsible of the
</em><br />
<em class="quotelev2">&gt;&gt; historical link between &lt;&lt;Entropy&gt;&gt; and &lt;&lt;Information&gt;&gt; (e.g. see
</em><br />
<em class="quotelev2">&gt;&gt; <a href="../../www.bartleby.com/64/C004/024.html">http://www.bartleby.com/64/C004/024.html</a>).
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Although &quot;entropy&quot; is a well known term in information theory, and used
</em><br />
<em class="quotelev2">&gt;&gt; coherently with the term &quot;information&quot; in this area, the situation is
</em><br />
<em class="quotelev2">&gt;&gt; different in science. I do not know what is &quot;information&quot; in
</em><br />
<em class="quotelev2">&gt;&gt; theermodynamics (does anybody know?). However, &quot;chemical information&quot; is
</em><br />
<em class="quotelev2">&gt;&gt; a well known area of chemistry, which covers many topics, including data
</em><br />
<em class="quotelev2">&gt;&gt; mining in chemical data bases. In fact, chemical information was
</em><br />
<em class="quotelev2">&gt;&gt; reognized as a major field when the ACS decided in 1975 to rename one of
</em><br />
<em class="quotelev2">&gt;&gt; its journals &quot;Journal of Chemical Information and Computer Sciences&quot;: it
</em><br />
<em class="quotelev2">&gt;&gt; was previously named the &quot;Journal of Chemical Documentation&quot;. There are
</em><br />
<em class="quotelev2">&gt;&gt; little papers in this journal which are connected with entropy
</em><br />
<em class="quotelev2">&gt;&gt; (thermodunamical of informational). An example is the 1996 paper of
</em><br />
<em class="quotelev2">&gt;&gt; Shu-Kun Lin, relating entropy with similarity and symmetry. Similarity
</em><br />
<em class="quotelev2">&gt;&gt; is itself a major area in chemical information, but I consider that the
</em><br />
<em class="quotelev2">&gt;&gt; main area of chemical information is related to chemical databases, such
</em><br />
<em class="quotelev2">&gt;&gt; that the chemical information is represented by the nodes and edges
</em><br />
<em class="quotelev2">&gt;&gt; graph associated to a structural formula. Actually, mathematical tools
</em><br />
<em class="quotelev2">&gt;&gt; able to work on this kind of chemical information are lacking,
</em><br />
<em class="quotelev2">&gt;&gt; particulary for statistics (did anyone performed statistics on graphs?).
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; In 1999, the links between information sciences and entropy were again
</em><br />
<em class="quotelev2">&gt;&gt; recognized, when Shu-Kun lin created the open access journal &quot;Entropy&quot;:
</em><br />
<em class="quotelev2">&gt;&gt; &lt;&lt;An International and Interdisciplinary Journal of Entropy and
</em><br />
<em class="quotelev2">&gt;&gt; Information Studies&gt;&gt;. Although most pluridisciplinary journals are at
</em><br />
<em class="quotelev2">&gt;&gt; the intersection of two areas, Shu-Kun Lin is a pionneer in the field of
</em><br />
<em class="quotelev2">&gt;&gt; transdisciplinarity, permitting the publication in a single journal of
</em><br />
<em class="quotelev2">&gt;&gt; works related to entropy and/or information theory, originating from
</em><br />
<em class="quotelev2">&gt;&gt; mathematics, physics, chemistry, biology, economy, and philosophy.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; The concept of information exists in other sciences for which the term
</em><br />
<em class="quotelev2">&gt;&gt; entropy is used. Bioinformation is a major concept in bioinformatics,
</em><br />
<em class="quotelev2">&gt;&gt; for which I am not specialist. Thus I hope that Pedro Marijuan would
</em><br />
<em class="quotelev2">&gt;&gt; like to help us to understand what are the links between bioinformation
</em><br />
<em class="quotelev2">&gt;&gt; and entropy. Entropy and information are known from economists and
</em><br />
<em class="quotelev2">&gt;&gt; philosophers. I also hope they add their voice to those of scientists
</em><br />
<em class="quotelev2">&gt;&gt; and mathematicians, to enlight our discussions during the session.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Now I would like to draw some provocative conclusions. Analogies between
</em><br />
<em class="quotelev2">&gt;&gt; concepts or between formal expressions of quantities are useful for the
</em><br />
<em class="quotelev2">&gt;&gt; spirit, for the quality of the papers, and sometimes they are used by
</em><br />
<em class="quotelev2">&gt;&gt; modellers to demonstrate why their work merit funds (does anybody never
</em><br />
<em class="quotelev2">&gt;&gt; do that?). The number of new concepts in sciences (includes mathematics,
</em><br />
<em class="quotelev2">&gt;&gt; economy, humanities, and so on) is increasing, and new terms are picked
</em><br />
<em class="quotelev2">&gt;&gt; in our natural language: the task of the teachers becomes harder and
</em><br />
<em class="quotelev2">&gt;&gt; harder. Entropy and Information are like the &quot;fourth dimension&quot;, one
</em><br />
<em class="quotelev2">&gt;&gt; century ago: they offer in common the ability to provide exciting topics
</em><br />
<em class="quotelev2">&gt;&gt; to discuss. Unfortunately, Entropy and Information are much more
</em><br />
<em class="quotelev2">&gt;&gt; difficult to handle.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt; Michel Petitjean                      Email: petitjean&#64;itodys.jussieu.fr
</em><br />
<em class="quotelev2">&gt;&gt; Editor-in-Chief of Entropy                   entropy&#64;mdpi.org
</em><br />
<em class="quotelev2">&gt;&gt; ITODYS (CNRS, UMR 7086)                      ptitjean&#64;ccr.jussieu.fr
</em><br />
<em class="quotelev2">&gt;&gt; 1 rue Guy de la Brosse                Phone: +33 (0)1 44 27 48 57
</em><br />
<em class="quotelev2">&gt;&gt; 75005 Paris, France.                  FAX  : +33 (0)1 44 27 68 14
</em><br />
<em class="quotelev2">&gt;&gt; <a href="../../www.mdpi.net/default.htm">http://www.mdpi.net</a>                   <a href="../../www.mdpi.org/default.htm">http://www.mdpi.org</a>
</em><br />
<em class="quotelev2">&gt;&gt; <a href="../../petitjeanmichel.free.fr/itoweb.petitjean.html">http://petitjeanmichel.free.fr/itoweb.petitjean.html</a>
</em><br />
<em class="quotelev2">&gt;&gt; <a href="../../petitjeanmichel.free.fr/itoweb.petitjean.freeware.html">http://petitjeanmichel.free.fr/itoweb.petitjean.freeware.html</a>
</em><br />
<em class="quotelev2">&gt;&gt; _______________________________________________
</em><br />
<em class="quotelev2">&gt;&gt; fis mailing list
</em><br />
<em class="quotelev2">&gt;&gt; <a href="mailto:fis&#64;listas.unizar.es?Subject=Re:%20[Fis]%20FIS%20/%20introductory%20text%20/%205%20April%202004">fis@listas.unizar.es</a> <a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;  
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<p><pre>
-- 
Dr. Shu-Kun Lin
Molecular Diversity Preservation International (MDPI)
Matthaeusstrasse 11, CH-4057 Basel, Switzerland
Tel. +41 61 683 7734 (office)
Tel. +41 79 322 3379 (mobile)
Fax +41 61 302 8918
E-mail: lin&#64;mdpi.org
<a href="../../www.mdpi.org/lin/default.htm">http://www.mdpi.org/lin/</a>
_______________________________________________
fis mailing list
fis&#64;listas.unizar.es
<a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
</pre>
<span id="received"><dfn>Received on</dfn> Mon Apr  5 16:27:40 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1397.html" title="Next message in the list">Loet Leydesdorff: "RE: [Fis] FIS / introductory text / 5 April 2004"</a></li>
<li><dfn>Previous message</dfn>: <a href="1395.html" title="Previous message in the list">Dr. Shu-Kun Lin: "Re: [Fis] FIS / introductory text / 5 April 2004"</a></li>
<li><dfn>In reply to</dfn>: <a href="1394.html" title="Message to which this message replies">Dr. Shu-Kun Lin: "Re: [Fis] FIS / introductory text / 5 April 2004"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="1401.html" title="Next message in this discussion thread">Pedro C. Marijuán: "Re: [Fis] FIS / introductory text / 5 April 2004"</a></li>
<li><a name="replies" id="replies"></a>
<dfn>Reply</dfn>: <a href="1401.html" title="Message sent in reply to this message">Pedro C. Marijuán: "Re: [Fis] FIS / introductory text / 5 April 2004"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1396" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1396" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1396" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1396" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:46 CET
</em></small></p>
</body>
</html>
