<?xml version="1.0" encoding="iso-8859-7"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-7" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: RE: [Fis] Replies and new questions / part 1</title>
<meta name="Author" content="Loet Leydesdorff (loet@leydesdorff.net)" />
<meta name="Subject" content="RE: [Fis] Replies and new questions / part 1" />
<meta name="Date" content="2004-04-10" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>RE: [Fis] Replies and new questions / part 1</h1>
<!-- received="Sat Apr 10 08:34:45 2004" -->
<!-- isoreceived="20040410063445" -->
<!-- sent="Sat, 10 Apr 2004 08:31:21 +0200" -->
<!-- isosent="20040410063121" -->
<!-- name="Loet Leydesdorff" -->
<!-- email="loet@leydesdorff.net" -->
<!-- subject="RE: [Fis] Replies and new questions / part 1" -->
<!-- id="004b01c41ec5$724d5dc0$1302a8c0@loet" -->
<!-- charset="iso-8859-7" -->
<!-- inreplyto="40767CA9.1020503&#64;unibas.ch" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1418.html" title="Loet Leydesdorff: &quot;RE: [Fis] Replies and new questions / part 1&quot;">Next message</a> ]
[ <a href="1416.html" title="hoelzer@unr.edu: &quot;Re: [Fis] About entropy metaphors&quot;">Previous message</a> ]
[ <a href="1413.html" title="Dr. Shu-Kun Lin: &quot;Re: [Fis] Replies and new questions / part 1&quot;">In reply to</a> ]
<!-- unextthread="start" -->
[ <a href="1418.html" title="Loet Leydesdorff: &quot;RE: [Fis] Replies and new questions / part 1&quot;">Next in thread</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Loet Leydesdorff &lt;<a href="mailto:loet&#64;leydesdorff.net?Subject=RE:%20[Fis]%20Replies%20and%20new%20questions%20/%20part%201">loet@leydesdorff.net</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Sat 10 Apr 2004 - 08:31:21 CEST</span><br />
</address>
<p>
<em class="quotelev1">&gt; If order (nonsymmetry) is represented by information I,
</em><br />
<em class="quotelev1">&gt; disorder (symmetry) by entropy S, you are talking about I/N
</em><br />
<em class="quotelev1">&gt; and S/N or about additivity or extensivity. For convenience,
</em><br />
<em class="quotelev1">&gt; let us put L=I+S. If the structure is not changed, I/S will
</em><br />
<em class="quotelev1">&gt; not change if only N increases. This means I and S will
</em><br />
<em class="quotelev1">&gt; increase or decrease together.
</em><br />
[...]
<br />
<em class="quotelev1">&gt; In many cased, additivity does not hold true. For example, if
</em><br />
<em class="quotelev1">&gt; we assess the biodiversity of microorganism. Among the
</em><br />
<em class="quotelev1">&gt; 1000000 samples (N=1000000) we found a new bacteria. Its
</em><br />
<em class="quotelev1">&gt; information is I=log1000000 per sample for this sample. This
</em><br />
<em class="quotelev1">&gt; means we calculate the maximum information (Its symbol has
</em><br />
<em class="quotelev1">&gt; been put as L, L=I+S) as L= x log N = N log N, where the variable x=N.
</em><br />
&nbsp;
<br />
<p>In other words: L is the maximum information, S is the (Shannon) entropy
<br />
H, and I (information) is equal to what is usually called the
<br />
redundancy. I am not in favour of this confusion of terms, particularly
<br />
because S is often used for the thermodynamic entropy (unlike the
<br />
Shannon entropy H). And I is used by some authors (e.g., Theil, 1972)
<br />
for the dynamic extension of Shannon's H. I'll follow the standard
<br />
notation below.
<br />
<p>Let me provide some relevant derivations.
<br />
<p>1. statistical entropy (Boltzmann)
<br />
<p>Boltzmann explained that in entropy can be provided with a statistical
<br />
interpretation (in addition to the traditional one of Clausius). The
<br />
entropy of a system (S) is related to the number of equally possible
<br />
states (W) with the Boltzmann constant k(B). As follows:
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;S = k(B). ln(W)                                        (1)
<br />
<p>k(B) is the so-called Boltzmann constant: k(B) = 1,381. 10^-23 J/K. 
<br />
Notet the dimensionality because the number of possible states (W) is
<br />
only a dimensionless number while S is also defined as Q/T (in classical
<br />
thermodynamics). It can be derived that the number of equally possible
<br />
states W is equal to (N!)/ N(1)! N(2)! .... N(i)! and therefore:
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;ln(W) = ln(N!) - Sigma(i) ln(Ni!)            (2)
<br />
<p>(i) is the ith compartment and N(i) is the number of particles in that
<br />
compartment. 
<br />
<p>It can be shown (Stirling) that 
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;ln(N!) = N ln(N) - N                                    (3)
<br />
<p>It follows:
<br />
<p>ln W = ln N! - Ó ln Ni!
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=  (N ln N - N) - Ó ( Ni ln Ni - Ni )
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= N ln N - Ó ( Ni ln Ni)
<br />
<p>Then: 
<br />
<p>ln W = N ln N - Ó ( Ni ln Ni)
<br />
<p>-ln W = Ó ( Ni ln Ni) - N ln N 
<br />
<p>(- ln W) / N = Ó ( Ni/N  ln Ni) - ln N
<br />
<p>If fi = Ni/N ,  then:
<br />
<p>(-ln W) / N  = Ó fi  ln Ni - ln N
<br />
<p>-ln W      =  N Ó fi ln fi
<br />
<p>Statistical entropy is thus defined:
<br />
<p>&nbsp;
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;S = - k(B) N Ó fi ln fi 
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;S = k(B) N H   and H = - Ó fi ln fi
<br />
<p>H is Shannon's probablistic entropy. Note that H is dimensionless. It
<br />
can therefore be applied to other systems as a statistics. The relation
<br />
formulates that a change of the distribution may cost energy. 
<br />
<p>However, not all energy changes imply changes in the information flux
<br />
and therefore the Szilard-Brillouin relation:
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;DS &gt;= k(B) DH
<br />
<p>2. probabilistic entropy (Shannon)
<br />
<p>H is a measure for the expected information content of a probability
<br />
distribution at each moment in time. If the distribution if
<br />
equiprobable, H is maximal because :
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;p(i) = f(i) / N
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;H = - Ó p(i) log p(i) 
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= - Ó f(i)/N log f(i)/N
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= Ó f(i)/N log N  - Ó f(i)/N log f(i)
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= log(N) - Ó f(i)/N log f(i)
<br />
<p>Thus, the maximum information content of a distribution is log(N). This
<br />
is equal to equiprobability.
<br />
When the probabilities are &quot;asymmetrical&quot;, there is less information
<br />
content. This sounds counter-intuitive, but the interpretation is as
<br />
follows.
<br />
<p>The distribution {1,1} or {50,50} provides us with: 
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;H = - 0.5 2log(0.5) - 0.5 2log(0.5) = 0.5 + 0.5 = 1 bit of
<br />
information
<br />
<p>The distribution {1,0} provides us with:
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;H = - 1 log(1) - 0 log(0) = 0 + 0 = 0 bit of information.
<br />
<p>In this latter case we are completely certain in the prediction that the
<br />
one is the case and the other not. Thus, there can be no surprise. In
<br />
the former case, we are completely uncertain; thus, information is
<br />
expected.
<br />
<p>This becomes intuitively more clearly when one uses the dynamic
<br />
extension of the Shannon entropy: the expected information content of a
<br />
change of the distribution, that is, an event has happened and an
<br />
probabilistic entropy was generated:
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;I = - Ó q(i) log q(i)/p(i)
<br />
<p>In this formula Ó q(i) represents the a posteriori distribution and Ó
<br />
p(i) the a priori one. If one of the terms p(i) is zero, the prediction
<br />
is that someting will not happen. If it happens yet, and the
<br />
corresponding q(i) is therefore larger than zero, the event is a
<br />
complete surprise and the information becomes infinite. Emergence can
<br />
therefore not be predicted. It is a complete surprise.
<br />
<p>When we consider time as a degree of freedom, we can also invert the
<br />
time axis and then evaluate emergence from an ex post perspective. The
<br />
positions of q(i) and p(i) are then exchanged. One can then backtrack
<br />
the roots of the emergent order to the chaos (uncertainty) from which it
<br />
emerged. Therefore, Prigogine used the title &quot;Order out of chaos&quot;. The
<br />
emergent order can thus be evaluated quantitatively from an ex post
<br />
perspective. The ex post perspective, however, is knowledge-based
<br />
(unlike the ex-ante perspective which is based in the historically given
<br />
or natural order of the events).
<br />
<p>A system which uses time as a degree of freedom can also be considered
<br />
as an anticipatory system (Rosen, 1985). It uses a model of itself for
<br />
the prediction (from an ex post or reflexive perspective). These systems
<br />
can be studied empirically using information theory and they can be
<br />
simulated (Dubois). Aleks Jakulin has elaborated on these two options in
<br />
a previous mailing. 
<br />
<p>I hope that this contributes to the clarification of the foundation of
<br />
information systems as the topic of this list. 
<br />
With kind regards, 
<br />
<p><p>Loet
<br />
<p>&nbsp;&nbsp;_____  
<br />
<p>Loet Leydesdorff 
<br />
Amsterdam School of Communications Research (ASCoR)
<br />
Kloveniersburgwal 48, 1012 CX Amsterdam
<br />
Tel.: +31-20- 525 6598; fax: +31-20- 525 3681 
<br />
&nbsp;&lt;mailto:loet&#64;leydesdorff.net&gt; loet&#64;leydesdorff.net ;
<br />
&lt;<a href="../../www.leydesdorff.net/default.htm">http://www.leydesdorff.net/</a>&gt; <a href="../../www.leydesdorff.net/default.htm">http://www.leydesdorff.net/</a> 
<br />
&nbsp;
<br />
&nbsp;&lt;<a href="../../www.upublish.com/books/leydesdorff-sci.htm">http://www.upublish.com/books/leydesdorff-sci.htm</a>&gt; The Challenge of
<br />
Scientometrics ;  &lt;<a href="../../www.upublish.com/books/leydesdorff.htm">http://www.upublish.com/books/leydesdorff.htm</a>&gt; The
<br />
Self-Organization of the Knowledge-Based Society
<br />
<p>&nbsp;
<br />
<span id="received"><dfn>Received on</dfn> Sat Apr 10 08:34:45 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1418.html" title="Next message in the list">Loet Leydesdorff: "RE: [Fis] Replies and new questions / part 1"</a></li>
<li><dfn>Previous message</dfn>: <a href="1416.html" title="Previous message in the list">hoelzer@unr.edu: "Re: [Fis] About entropy metaphors"</a></li>
<li><dfn>In reply to</dfn>: <a href="1413.html" title="Message to which this message replies">Dr. Shu-Kun Lin: "Re: [Fis] Replies and new questions / part 1"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="1418.html" title="Next message in this discussion thread">Loet Leydesdorff: "RE: [Fis] Replies and new questions / part 1"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1417" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1417" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1417" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1417" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:46 CET
</em></small></p>
</body>
</html>
