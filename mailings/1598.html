<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: RE: [Fis] Shannon said Information is Physical Entropy</title>
<meta name="Author" content="Loet Leydesdorff (loet@leydesdorff.net)" />
<meta name="Subject" content="RE: [Fis] Shannon said Information is Physical Entropy" />
<meta name="Date" content="2004-06-17" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>RE: [Fis] Shannon said Information is Physical Entropy</h1>
<!-- received="Thu Jun 17 09:57:01 2004" -->
<!-- isoreceived="20040617075701" -->
<!-- sent="Thu, 17 Jun 2004 09:55:13 +0200" -->
<!-- isosent="20040617075513" -->
<!-- name="Loet Leydesdorff" -->
<!-- email="loet@leydesdorff.net" -->
<!-- subject="RE: [Fis] Shannon said Information is Physical Entropy" -->
<!-- id="005c01c45440$6cc1d920$1302a8c0@loet" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="40D11D66.8010805&#64;cybermesa.com" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1599.html" title="jakulin@acm.org: &quot;RE: [Fis] Shannon said Information is Physical Entropy&quot;">Next message</a> ]
[ <a href="1597.html" title="Gyorgy Darvas: &quot;Re: [Fis] 2004 FIS session: concluding comments&quot;">Previous message</a> ]
[ <a href="1596.html" title="Michael Devereux: &quot;[Fis] Shannon said Information is Physical Entropy&quot;">In reply to</a> ]
<!-- unextthread="start" -->
[ <a href="1599.html" title="jakulin@acm.org: &quot;RE: [Fis] Shannon said Information is Physical Entropy&quot;">Next in thread</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Loet Leydesdorff &lt;<a href="mailto:loet&#64;leydesdorff.net?Subject=RE:%20[Fis]%20Shannon%20said%20Information%20is%20Physical%20Entropy">loet@leydesdorff.net</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Thu 17 Jun 2004 - 09:55:13 CEST</span><br />
</address>
<p>
Dear Michael and colleagues,
<br />
&nbsp;
<br />
The terminology &quot;Shannon entropy&quot; itself may be part of the confusion.
<br />
It is perhaps better to name this &quot;probabilistic entropy&quot; or
<br />
Shannon-type information. However it may be named, H remains to be
<br />
considered as the expected information content of a distribution. 
<br />
&nbsp;
<br />
The Shannon formulas get meaning when one specifies (theoretically) what
<br />
is the substance that is distributed (and redistributed when the
<br />
information is communicated). The mathematical theory of communication
<br />
can thus be paired with a substantive theory: what is communicated when
<br />
the system operates? One can expect analogies and differences among
<br />
entropical systems because the different types of systems share the
<br />
mathematical formalisms (including the Second Law), but they differ in
<br />
terms of the substantive theories. Thus, special theories of
<br />
communications can be developed at each systems level. It seems to me
<br />
that this is an extermely rich configuration.
<br />
&nbsp;
<br />
I myself am most interested in systems which communicate meaning in
<br />
addition to information. This can be modeled as a system which
<br />
communicates using two channels for the communication at each moment of
<br />
time. The two channels are not hardwired (like in physical
<br />
transmissions) and therefore one expects interaction terms and feedbacks
<br />
(e.g. in human language). I learned from this discussion with you that
<br />
one should not call the mathematical apparatus that the theory of
<br />
communication provides &quot;Shannon entropy&quot; because of the historical
<br />
connotation--but I never did so. I am happy with the term probabilistic
<br />
entropy. But we should not confuse the historical origins of these
<br />
concepts with their epistemological status.
<br />
&nbsp;
<br />
Why does the distinction between thermodynamic (physical) and
<br />
probabilistic entropy not solve the issue?
<br />
&nbsp;
<br />
With kind regards, 
<br />
&nbsp;
<br />
&nbsp;
<br />
Loet 
<br />
&nbsp;&nbsp;_____  
<br />
<p>Loet Leydesdorff 
<br />
Amsterdam School of Communications Research (ASCoR)
<br />
Kloveniersburgwal 48, 1012 CX Amsterdam
<br />
Tel.: +31-20- 525 6598; fax: +31-20- 525 3681 
<br />
&nbsp;&lt;mailto:loet&#64;leydesdorff.net&gt; loet&#64;leydesdorff.net ;
<br />
&lt;<a href="../../www.leydesdorff.net/default.htm">http://www.leydesdorff.net/</a>&gt; <a href="../../www.leydesdorff.net/default.htm">http://www.leydesdorff.net/</a> 
<br />
<p>&nbsp;
<br />
&nbsp;&lt;<a href="../../www.upublish.com/books/leydesdorff-sci.htm">http://www.upublish.com/books/leydesdorff-sci.htm</a>&gt; The Challenge of
<br />
Scientometrics ;  &lt;<a href="../../www.upublish.com/books/leydesdorff.htm">http://www.upublish.com/books/leydesdorff.htm</a>&gt; The
<br />
Self-Organization of the Knowledge-Based Society
<br />
<p><p><p><em class="quotelev1">&gt; -----Original Message-----
</em><br />
<em class="quotelev1">&gt; From: fis-bounces&#64;listas.unizar.es
</em><br />
<em class="quotelev1">&gt; [mailto:fis-bounces&#64;listas.unizar.es] On Behalf Of Michael Devereux
</em><br />
<em class="quotelev1">&gt; Sent: Thursday, June 17, 2004 6:26 AM
</em><br />
<em class="quotelev1">&gt; To: FIS Mailing List
</em><br />
<em class="quotelev1">&gt; Subject: [Fis] Shannon said Information is Physical Entropy
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Dear Werner, Loet and colleagues,
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; I hope I don't seem merely argumentative. It's not my
</em><br />
<em class="quotelev1">&gt; intention to keep
</em><br />
<em class="quotelev1">&gt; saying the same thing over and over until no one else is willing to
</em><br />
<em class="quotelev1">&gt; rebut my assertions. The ideas we're discussing are most
</em><br />
<em class="quotelev1">&gt; significant to
</em><br />
<em class="quotelev1">&gt; me because, among other things, they play a crucial role in
</em><br />
<em class="quotelev1">&gt; calculations
</em><br />
<em class="quotelev1">&gt; I'm completing on the entropy cost of information processing in
</em><br />
<em class="quotelev1">&gt; Szilard's engine. I hope to be able to determine, definitively, the
</em><br />
<em class="quotelev1">&gt; entropy cost of manipulating individual information bits.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; So, I'd like to sustain those claims I continue to believe are
</em><br />
<em class="quotelev1">&gt; scientifically valid and truly important. I'm not looking for
</em><br />
<em class="quotelev1">&gt; agreement
</em><br />
<em class="quotelev1">&gt; with all my colleagues, but, rather, the good science of
</em><br />
<em class="quotelev1">&gt; agreement with
</em><br />
<em class="quotelev1">&gt; physical reality. And, I welcome the informed and considered
</em><br />
<em class="quotelev1">&gt; criticism
</em><br />
<em class="quotelev1">&gt; of everyone who disagrees.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; In that light, I think the most important question is whether Shannon
</em><br />
<em class="quotelev1">&gt; entropy (as Shannon derived and understood it, not simply as
</em><br />
<em class="quotelev1">&gt; that term
</em><br />
<em class="quotelev1">&gt; may be used in each research investigation) is the same thing as
</em><br />
<em class="quotelev1">&gt; physical (thermodynamic) entropy. I understand that many in
</em><br />
<em class="quotelev1">&gt; this forum
</em><br />
<em class="quotelev1">&gt; would answer this question with a no. Werner, you've written that
</em><br />
<em class="quotelev1">&gt; Shannon entropy and Boltzmann's physical entropy are &quot;not completely
</em><br />
<em class="quotelev1">&gt; separated, but related.&quot;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Shannon co-wrote a book with Warren Weaver a year after
</em><br />
<em class="quotelev1">&gt; publication of
</em><br />
<em class="quotelev1">&gt; his famous equation derivation. It's called &quot;The Mathematical
</em><br />
<em class="quotelev1">&gt; Theory of
</em><br />
<em class="quotelev1">&gt; Communication&quot; (University of Illinois Press, Chicago, 1949). Weaver
</em><br />
<em class="quotelev1">&gt; writes that &quot;The quantity which uniquely meets the natural
</em><br />
<em class="quotelev1">&gt; requirements
</em><br />
<em class="quotelev1">&gt; that one sets up for 'information' turns out to be exactly
</em><br />
<em class="quotelev1">&gt; that which is
</em><br />
<em class="quotelev1">&gt; known in thermodynamics as entropy....That information be measured by
</em><br />
<em class="quotelev1">&gt; entropy, is, after all, natural when we remember that information, in
</em><br />
<em class="quotelev1">&gt; communication theory, is associated with the amount of
</em><br />
<em class="quotelev1">&gt; freedom of choice
</em><br />
<em class="quotelev1">&gt; we have in constructing messages.&quot; (pp. 12-13). Weaver tells us that
</em><br />
<em class="quotelev1">&gt; this information is modeled by Shannon's infamous equation, H
</em><br />
<em class="quotelev1">&gt; = - Sum p
</em><br />
<em class="quotelev1">&gt; log (p) (page 14).
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; I previously cited Grandy's resource letter (Am. J. Phys. 65,
</em><br />
<em class="quotelev1">&gt; 6, 1997,
</em><br />
<em class="quotelev1">&gt; p. 466). He tells us that the &quot;rigorous connection of
</em><br />
<em class="quotelev1">&gt; information theory
</em><br />
<em class="quotelev1">&gt; to physics&quot; is due to Jaynes (Phys. Rev. A 106, 1957, p. 620)
</em><br />
<em class="quotelev1">&gt; who showed
</em><br />
<em class="quotelev1">&gt; that S (physical entropy) &quot;measures the amount of information
</em><br />
<em class="quotelev1">&gt; about the
</em><br />
<em class="quotelev1">&gt; microstate conveyed by data on macroscopic thermodynamic variables,&quot;
</em><br />
<em class="quotelev1">&gt; where S is the &quot;experimental entropy of Clausius. Quantum
</em><br />
<em class="quotelev1">&gt; mechanically
</em><br />
<em class="quotelev1">&gt; one employs the density matrix rho and von Neumann's form of the
</em><br />
<em class="quotelev1">&gt; entropy&quot; (p. 469).
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; I've continued to maintain in this forum Landauer's contention that
</em><br />
<em class="quotelev1">&gt; information is physical ( Landauer, Phys. Today, May 1991, Landauer,
</em><br />
<em class="quotelev1">&gt; Phys. Lett. A, 217, 1996, p. 188. Sorry, Aleks, I had thought these
</em><br />
<em class="quotelev1">&gt; publications were decades earlier.) Other authors have made the same
</em><br />
<em class="quotelev1">&gt; argument. And, if it's true that information is physical,
</em><br />
<em class="quotelev1">&gt; then Shannon,
</em><br />
<em class="quotelev1">&gt; obviously, was deriving an equation for something physical. I've
</em><br />
<em class="quotelev1">&gt; previously quoted Shannon here in this forum: &quot;We wish to consider
</em><br />
<em class="quotelev1">&gt; certain general problems involving communication systems. To
</em><br />
<em class="quotelev1">&gt; do this it
</em><br />
<em class="quotelev1">&gt; is first necessary to represent the various elements involved as
</em><br />
<em class="quotelev1">&gt; mathematical entities, suitably idealized from their physical
</em><br />
<em class="quotelev1">&gt; counterparts.&quot;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; I know it's a repeat of a repeat, but I think the distinction between
</em><br />
<em class="quotelev1">&gt; the thing being described (the thing itself, whether the energy of a
</em><br />
<em class="quotelev1">&gt; physical particle or, say, the idiosyncratic behavior of some
</em><br />
<em class="quotelev1">&gt; species of
</em><br />
<em class="quotelev1">&gt; bird, or Shannon entropy), and the mathematical model of that
</em><br />
<em class="quotelev1">&gt; thing, is
</em><br />
<em class="quotelev1">&gt; an essential distinction to maintain. As Michel has said, we
</em><br />
<em class="quotelev1">&gt; all seem to
</em><br />
<em class="quotelev1">&gt; be able to recognize that difference. Would you disagree, Werner? You
</em><br />
<em class="quotelev1">&gt; write that Shannon entropy is truly pure mathematics, that &quot;it is a
</em><br />
<em class="quotelev1">&gt; mathematical expression which can be applied to many systems having
</em><br />
<em class="quotelev1">&gt; probability distributions.&quot; (I emphasize the word &quot;entropy&quot;,
</em><br />
<em class="quotelev1">&gt; as opposed
</em><br />
<em class="quotelev1">&gt; to &quot;equation&quot;, or &quot;mathematical expression&quot;.) Perhaps you
</em><br />
<em class="quotelev1">&gt; don't permit a
</em><br />
<em class="quotelev1">&gt; difference between Shannon's equation and Shannon entropy, Werner.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; As an analogy, energy is a measurable property of tangible
</em><br />
<em class="quotelev1">&gt; objects, and
</em><br />
<em class="quotelev1">&gt; is not the same thing as the mathematical formula which describes
</em><br />
<em class="quotelev1">&gt; kinetic energy or electrical energy, etc. (We physicists cringe when
</em><br />
<em class="quotelev1">&gt; &quot;New-Age Astrologers&quot;, for example, predict the heightened energy of
</em><br />
<em class="quotelev1">&gt; personal relationships that must result from the conjunction of Venus
</em><br />
<em class="quotelev1">&gt; and Mars in Virgo. We don't permit the word energy with that
</em><br />
<em class="quotelev1">&gt; meaning in
</em><br />
<em class="quotelev1">&gt; science. I believe that restriction is entirely worthwhile within our
</em><br />
<em class="quotelev1">&gt; own scientific disciplines because it fosters precise
</em><br />
<em class="quotelev1">&gt; understanding of
</em><br />
<em class="quotelev1">&gt; the concept, which can then be modeled mathematically.)
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; I understand, Loet, that &quot;Shannon's formula is a mathematical
</em><br />
<em class="quotelev1">&gt; expression
</em><br />
<em class="quotelev1">&gt; that is formally similar to the Boltzmann equation.&quot; But, as
</em><br />
<em class="quotelev1">&gt; Shannon and
</em><br />
<em class="quotelev1">&gt; Weaver, and Jaynes, and Landauer have shown us, Shannon
</em><br />
<em class="quotelev1">&gt; entropy is the
</em><br />
<em class="quotelev1">&gt; same physical thing as thermodynamic entropy. So, with Shannon's
</em><br />
<em class="quotelev1">&gt; entropy, anyway, I don't agree that &quot;the reference to a
</em><br />
<em class="quotelev1">&gt; physical system
</em><br />
<em class="quotelev1">&gt; is possible, but not necessary.&quot;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; And, if Shannon entropy is actually physical entropy then it
</em><br />
<em class="quotelev1">&gt; must obey
</em><br />
<em class="quotelev1">&gt; the Second Law of Thermodynamics. If it's not increasing
</em><br />
<em class="quotelev1">&gt; monotonically
</em><br />
<em class="quotelev1">&gt; with increasing N, it''s not really Shannon entropy. And if won't
</em><br />
<em class="quotelev1">&gt; satisfy the monotonically increasing postulate that Shannon
</em><br />
<em class="quotelev1">&gt; imposed on
</em><br />
<em class="quotelev1">&gt; his derivation.
</em><br />
<em class="quotelev1">&gt; .
</em><br />
<em class="quotelev1">&gt; Is it perhaps dogmatic, Loet, to insist that Shannon entropy is what
</em><br />
<em class="quotelev1">&gt; Shannon said it was? I believe restriction to that meaning avoids
</em><br />
<em class="quotelev1">&gt; confusion in our research. To my mind, &quot;Shannon entropy&quot; is a
</em><br />
<em class="quotelev1">&gt; technical
</em><br />
<em class="quotelev1">&gt; term, just like energy, or cell mitosis. And we promote
</em><br />
<em class="quotelev1">&gt; understanding,
</em><br />
<em class="quotelev1">&gt; rather than confusion, by calling something Shannon entropy
</em><br />
<em class="quotelev1">&gt; only if it
</em><br />
<em class="quotelev1">&gt; means what Shannon meant. That was my point in arguing that a
</em><br />
<em class="quotelev1">&gt; Devereux
</em><br />
<em class="quotelev1">&gt; theory, or a Leydesdorff theory, ought to be what you or I
</em><br />
<em class="quotelev1">&gt; have stated
</em><br />
<em class="quotelev1">&gt; our own theory to be. My feeling is that we ought to label it with a
</em><br />
<em class="quotelev1">&gt; different name if it isn't actually the author's meaning we intend by
</em><br />
<em class="quotelev1">&gt; that name.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; You've written, Werner, that &quot;applying Shannon's formula to physical
</em><br />
<em class="quotelev1">&gt; systems not necessarily gives thermodynamic entropy&quot;. I think
</em><br />
<em class="quotelev1">&gt; that can
</em><br />
<em class="quotelev1">&gt; be a confusing problem if one labels the property of that physical
</em><br />
<em class="quotelev1">&gt; system Shannon entropy. Simply employing Shannon's formula on
</em><br />
<em class="quotelev1">&gt; the system
</em><br />
<em class="quotelev1">&gt; does not guarantee that the property being modeled is Shannon
</em><br />
<em class="quotelev1">&gt; entropy.
</em><br />
<em class="quotelev1">&gt; That's one of the arguments I was trying to defend in recent
</em><br />
<em class="quotelev1">&gt; postings.
</em><br />
<em class="quotelev1">&gt; You said &quot;there are applications to physical systems....which lead to
</em><br />
<em class="quotelev1">&gt; some entropy but not to the measurable thermodynamic entropy.&quot; And I
</em><br />
<em class="quotelev1">&gt; suggest, as scientists, we never call that thing Shannon entropy.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; I'm grateful for the comments which prompt me to reconsider, and,
</em><br />
<em class="quotelev1">&gt; hopefully, further understand these important concepts.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Cordially,
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; Michael Devereux
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; _______________________________________________
</em><br />
<em class="quotelev1">&gt; fis mailing list
</em><br />
<em class="quotelev1">&gt; <a href="mailto:fis&#64;listas.unizar.es?Subject=RE:%20[Fis]%20Shannon%20said%20Information%20is%20Physical%20Entropy">fis@listas.unizar.es</a> <a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<span id="received"><dfn>Received on</dfn> Thu Jun 17 09:57:01 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1599.html" title="Next message in the list">jakulin@acm.org: "RE: [Fis] Shannon said Information is Physical Entropy"</a></li>
<li><dfn>Previous message</dfn>: <a href="1597.html" title="Previous message in the list">Gyorgy Darvas: "Re: [Fis] 2004 FIS session: concluding comments"</a></li>
<li><dfn>In reply to</dfn>: <a href="1596.html" title="Message to which this message replies">Michael Devereux: "[Fis] Shannon said Information is Physical Entropy"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="1599.html" title="Next message in this discussion thread">jakulin@acm.org: "RE: [Fis] Shannon said Information is Physical Entropy"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1598" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1598" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1598" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1598" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:47 CET
</em></small></p>
</body>
</html>
