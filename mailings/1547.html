<?xml version="1.0" encoding="us-ascii"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: RE: [Fis] Information and communication</title>
<meta name="Author" content="jakulin@acm.org (jakulin@acm.org)" />
<meta name="Subject" content="RE: [Fis] Information and communication" />
<meta name="Date" content="2004-06-01" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>RE: [Fis] Information and communication</h1>
<!-- received="Tue Jun  1 14:10:58 2004" -->
<!-- isoreceived="20040601121058" -->
<!-- sent="Tue, 1 Jun 2004 14:09:18 +0200" -->
<!-- isosent="20040601120918" -->
<!-- name="jakulin@acm.org" -->
<!-- email="jakulin@acm.org" -->
<!-- subject="RE: [Fis] Information and communication" -->
<!-- id="20040601120923.071C97DA8@mail.fri.uni-lj.si" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="017601c447be$2b651660$1302a8c0&#64;loet" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1548.html" title="Robert Ulanowicz: &quot;Re: [Fis] Information and communication&quot;">Next message</a> ]
[ <a href="1546.html" title="Michel Petitjean: &quot;[Fis] Re: Minkowski space measurable ? (was: [Fis] Re: Information and communication)&quot;">Previous message</a> ]
[ <a href="1545.html" title="Loet Leydesdorff: &quot;[Fis] Information and communication&quot;">In reply to</a> ]
<!-- unextthread="start" -->
[ <a href="1548.html" title="Robert Ulanowicz: &quot;Re: [Fis] Information and communication&quot;">Next in thread</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: &lt;<a href="mailto:jakulin&#64;acm.org?Subject=RE:%20[Fis]%20Information%20and%20communication">jakulin@acm.org</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Tue 01 Jun 2004 - 14:09:18 CEST</span><br />
</address>
<p>
Hello all.
<br />
<p>Michel considers time and space to be mixed via stochastic processes in
<br />
non-relativistic models, and points out that sigma-algebras are needed in
<br />
Minkowski spaces before we can speak of probability. I agree with his
<br />
concerns, but I'm not a specialist of relativity either. I think we can
<br />
model any function y = f(x) using a more general concept of a probability
<br />
function P(x,y) using the equivalence between P(x,y|f) &gt; 0 and y=f(x);
<br />
P(x,y|f)=0 and y &lt;&gt; f(x).
<br />
<p>Loet is working on rather well-defined problems that only capture a few
<br />
aspects of reality, but not everything. In such a context, it is not
<br />
necessary to ponder physical models, but just define some probability mass
<br />
function that models the relationship between the given variables. I agree
<br />
with this simplification, but I've encountered a few issues which are
<br />
connected directly with Michel's concerns.
<br />
<p>1. Probability mass functions are what we need to properly deal with
<br />
Shannon's entropy. As you know, entropy can be defined on probability
<br />
density functions, but this entropy has different properties from the
<br />
entropy defined on probability mass functions. For that reason we refer to
<br />
entropy on a density function as &quot;differential entropy&quot; (Cover&amp;Thomas), it
<br />
can be negative or zero, and the value of entropy changes when the space is
<br />
transformed. Although one can do various interesting things with
<br />
differential entropy, let us try to focus on probability mass functions.
<br />
<p>2. A probability mass function gives the probability of occurrence of a
<br />
certain concrete configuration. The configuration can also be considered an
<br />
event. To someone using entropy, the underlying alphabet of concrete
<br />
configurations has to be completely clear. For thermodynamics, the
<br />
configuration is a microstate of a physical system given a particular energy
<br />
level (so we speak of the probability of a particular configuration of
<br />
microparticles). For Shannon's application of entropy to communication, the
<br />
configuration is a letter from an alphabet, or a message composed of several
<br />
letters. In my applications of entropy to medicine, the configuration is a
<br />
particular patient. In Loet's application of entropy to analysis of the
<br />
economy, the configuration is a particular company. The meaning of entropy
<br />
would change if I instead considered an employee to be a configuration
<br />
instead. It would further change if I instead considered a Euro measured at
<br />
the 20th of May, 2004, to be a configuration. 
<br />
<p>3. It was proven in statistical thermodynamics that, at the same energy
<br />
level, (changes into) the high entropy configurations are a lot more likely
<br />
than (changes into) low entropy configurations.  But whatever was proved for
<br />
thermodynamical models has not been proved for other probabilistic models
<br />
that we can compute entropy on. Quite the opposite! Therefore, we cannot
<br />
infer ever-increasing entropy for Shannon's probabilistic model of
<br />
communication, because this model has nothing to do with thermodynamical
<br />
models. Other fields introduce entropy and refer to thermodynamical laws,
<br />
but often neglect to show that their underlying models show the same
<br />
properties as those of statistical mechanics.
<br />
<p>4. Energy level can be considered to be a single variable. If we include
<br />
other variables, we can investigate the contribution to the entropy that
<br />
arises from interactions between there variables and the energy level using
<br />
mutual information. This way, mutual information (and its generalizations)
<br />
can be considered to be a decomposition of entropy. Inferring interaction or
<br />
causal influence from mutual information is possible, but may be tricky:
<br />
there are situations where variables that interact have zero mutual
<br />
information (parity problem), while variables that do not interact have
<br />
non-zero mutual information (Simpson's paradox). There are further problems
<br />
associated with probabilistic modeling: is some mutual information due to
<br />
true interaction or merely due to the &quot;noise&quot; or random perturbation.
<br />
<p>Therefore, although I endorse the use of mutual information to infer
<br />
interactions between variables (it's my research topic after all!), I
<br />
recommend a careful approach, as the methodology has not been polished yet.
<br />
<p>Best regards,
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Aleks
<br />
<p><pre>
--
mag. Aleks Jakulin
<a href="../../www.ailab.si/aleks/default.htm">http://www.ailab.si/aleks/</a>
Artificial Intelligence Laboratory, 
Faculty of Computer and Information Science, University of Ljubljana. 
_______________________________________________
fis mailing list
fis&#64;listas.unizar.es
<a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
</pre>
<span id="received"><dfn>Received on</dfn> Tue Jun  1 14:10:58 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1548.html" title="Next message in the list">Robert Ulanowicz: "Re: [Fis] Information and communication"</a></li>
<li><dfn>Previous message</dfn>: <a href="1546.html" title="Previous message in the list">Michel Petitjean: "[Fis] Re: Minkowski space measurable ? (was: [Fis] Re: Information and communication)"</a></li>
<li><dfn>In reply to</dfn>: <a href="1545.html" title="Message to which this message replies">Loet Leydesdorff: "[Fis] Information and communication"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="1548.html" title="Next message in this discussion thread">Robert Ulanowicz: "Re: [Fis] Information and communication"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1547" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1547" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1547" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1547" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:46 CET
</em></small></p>
</body>
</html>
