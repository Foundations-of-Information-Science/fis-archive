<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: [Fis] FIS / introductory text / 5 April 2004</title>
<meta name="Author" content="Michel Petitjean (ptitjean@itodys.jussieu.fr)" />
<meta name="Subject" content="[Fis] FIS / introductory text / 5 April 2004" />
<meta name="Date" content="2004-04-05" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>[Fis] FIS / introductory text / 5 April 2004</h1>
<!-- received="Mon Apr  5 09:22:34 2004" -->
<!-- isoreceived="20040405072234" -->
<!-- sent="Mon, 5 Apr 2004 09:14:30 +0200 (MEST)" -->
<!-- isosent="20040405071430" -->
<!-- name="Michel Petitjean" -->
<!-- email="ptitjean@itodys.jussieu.fr" -->
<!-- subject="[Fis] FIS / introductory text / 5 April 2004" -->
<!-- id="200404050714.i357EUSj397451@ds10.itodys.jussieu.fr" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1393.html" title="Loet Leydesdorff: &quot;RE: [Fis] FIS / introductory text / 5 April 2004&quot;">Next message</a> ]
[ <a href="1391.html" title="Pedro C. Marijuán: &quot;[Fis] visible world&quot;">Previous message</a> ]
<!-- unextthread="start" -->
[ <a href="1393.html" title="Loet Leydesdorff: &quot;RE: [Fis] FIS / introductory text / 5 April 2004&quot;">Next in thread</a> ]
 [ <a href="#replies">Replies</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Michel Petitjean &lt;<a href="mailto:ptitjean&#64;itodys.jussieu.fr?Subject=Re:%20[Fis]%20FIS%20/%20introductory%20text%20/%205%20April%202004">ptitjean@itodys.jussieu.fr</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Mon 05 Apr 2004 - 09:14:30 CEST</span><br />
</address>
<p>
2004 FIS session introductory text.
<br />
<p>Dear FISers,
<br />
<p>I would like to thank Pedro Marijuan for his kind invitation
<br />
to chair the 2004 FIS session. The session is focussed on
<br />
&quot;Entropy and Information&quot;. It is vast, that I am afraid
<br />
to be able only to evoke some general aspects, discarding
<br />
specific technical developments.
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Entropy and Information: two polymorphic concepts.
<br />
<p>Although these two concepts are undoubtly related, they have
<br />
different stories.
<br />
<p>Let us consider first the Information concept.
<br />
There was many discussions in the FIS list about the meaning
<br />
of Information. Clearly, there are several definitions.
<br />
The information concept that most people have in mind is
<br />
outside the scope of this text: is it born with Computer Sciences,
<br />
or is it born with Press, or does it exist since a so long time
<br />
that nobody could date it? Neglecting the definitions from
<br />
the dictionnaries (for each language and culture), I would say
<br />
that anybody has his own concept. Philosophers and historians
<br />
have to look. The content of the FIS archives suggests that
<br />
the field is vast.
<br />
<p>Now let us look to scientific definitions. Those arising from
<br />
mathematics are rigorous, but have different meanings. An example
<br />
is the information concept emerging from information theory (Hartley,
<br />
Wiener, Shannon, Renyi,...). This concept, which arises from probability
<br />
theory, has little connections with the Fisher information, which
<br />
arises also from probability theory. The same word is used, but two
<br />
rigorous concepts are defined. One is mostly related to coding theory,
<br />
and the other is related to estimation theory. One deals mainly
<br />
with non numerical finite discrete distributions, and the other
<br />
is based on statistics from samples of parametrized family of
<br />
distributions. Even within the framework of information theory,
<br />
there are several definitions of information (e.g. see the last
<br />
chapter of Renyi's book on Probability Theory). This situation
<br />
arises often in mathematics: e.g., there are several concepts of
<br />
&quot;distance&quot;, and, despite the basic axioms they all satisfy, nobody
<br />
would say that they have the same meaning, even when they are defined
<br />
on a common space.
<br />
<p>Then, mathematical tools are potential (and sometimes demonstrated)
<br />
simplified models for physical phenomenons. On the other hand,
<br />
scientists may publish various definitions of information for physical
<br />
situations. It does not mean that any of these definitions should be
<br />
confused between themselves and confused with the mathematical ones.
<br />
In many papers, the authors insist on the analogies between their
<br />
own concepts and those previously published by other authors: this
<br />
attitude may convince the reviewers of the manuscript that the work
<br />
has interest, but contribute to the general confusion, particularly
<br />
when the confusing terms are recorded in the bibliographic databases.
<br />
Searching in databases with the keyword &quot;information&quot; would lead to
<br />
a considerable number of hits: nobody would try it without constraining
<br />
the search with other terms (did some of you tried?).
<br />
<p>We consider now the Entropy concepts. The two main ones are the
<br />
informational entropy and the thermodynamical entropy. The first
<br />
one has non ambiguous relations with information (in the sense of
<br />
information theory), since both are defined within the framework of
<br />
a common theory. Let us look now to the thermodynamical entropy,
<br />
which was defined by Rudolf Clausius in 1865.
<br />
It is a physical concept, usually introduced from the Carnot Cycle.
<br />
The existence of entropy is postulated, and it is a state function
<br />
of the system. Usual variables are temperature and pressure. Entropy
<br />
calculations are sometimes made discarding the implicit assumptions
<br />
done for an idealized Carnot Cycle. Here come difficulties. E.g., the
<br />
whole universe is sometimes considered as a system for which the
<br />
the entropy is assumed to have sense. Does the equilibrium of
<br />
such a system has sense? Does thermodynamical state functions
<br />
make sense here? And what about &quot;the&quot; temperature? These latter
<br />
variable, even when viewed as a function of coordinates and/or
<br />
time, has sense only for a restricted number of situations.
<br />
These difficulties appear for many other systems. At other scales,
<br />
they may appear for microscopic systems, and for macroscopic
<br />
systems unrelated to thermochemistry. 
<br />
In fact, what is often implicitly postulated is that the
<br />
thermodynamical entropy theory could work outside thermodynamics.
<br />
<p>Statistical mechanics creates a bridge between microscopic and
<br />
macroscopic models, as evidenced from the work of Boltzmann.
<br />
These two models are different. One is a mathematical model
<br />
for an idealized physical situation (punctual balls, elastic
<br />
collisions, distribution of states, etc..), and the other is
<br />
a simplified physical model, working upon a restricted number
<br />
of conditions. The expression of the entropy, calculated via
<br />
statistical mechanics methods, is formally similar to the
<br />
informational entropy. This latter has appeared many decades after
<br />
the former. Thus, the pioneers of information theory (Shannon,
<br />
von Neumann) who retain the term &quot;entropy&quot;, are undoubtly responsible
<br />
of the historical link between &lt;&lt;Entropy&gt;&gt; and &lt;&lt;Information&gt;&gt;
<br />
(e.g. see <a href="../../www.bartleby.com/64/C004/024.html">http://www.bartleby.com/64/C004/024.html</a>).
<br />
<p>Although &quot;entropy&quot; is a well known term in information theory, and used
<br />
coherently with the term &quot;information&quot; in this area, the situation
<br />
is different in science. I do not know what is &quot;information&quot; in
<br />
theermodynamics (does anybody know?). However, &quot;chemical information&quot;
<br />
is a well known area of chemistry, which covers many topics, including
<br />
data mining in chemical data bases. In fact, chemical information
<br />
was reognized as a major field when the ACS decided in 1975 to rename
<br />
one of its journals &quot;Journal of Chemical Information and Computer Sciences&quot;:
<br />
it was previously named the &quot;Journal of Chemical Documentation&quot;.
<br />
There are little papers in this journal which are connected with
<br />
entropy (thermodunamical of informational). An example is the
<br />
1996 paper of Shu-Kun Lin, relating entropy with similarity and
<br />
symmetry. Similarity is itself a major area in chemical information,
<br />
but I consider that the main area of chemical information is related
<br />
to chemical databases, such that the chemical information is represented
<br />
by the nodes and edges graph associated to a structural formula.
<br />
Actually, mathematical tools able to work on this kind of chemical
<br />
information are lacking, particulary for statistics (did anyone
<br />
performed statistics on graphs?).
<br />
<p>In 1999, the links between information sciences and entropy were again
<br />
recognized, when Shu-Kun lin created the open access journal &quot;Entropy&quot;:
<br />
&lt;&lt;An International and Interdisciplinary Journal of Entropy and
<br />
Information Studies&gt;&gt;. Although most pluridisciplinary journals
<br />
are at the intersection of two areas, Shu-Kun Lin is a pionneer
<br />
in the field of transdisciplinarity, permitting the publication
<br />
in a single journal of works related to entropy and/or information
<br />
theory, originating from mathematics, physics, chemistry, biology,
<br />
economy, and philosophy.
<br />
<p>The concept of information exists in other sciences for which the
<br />
term entropy is used. Bioinformation is a major concept in bioinformatics,
<br />
for which I am not specialist. Thus I hope that Pedro Marijuan would like
<br />
to help us to understand what are the links between bioinformation and
<br />
entropy. Entropy and information are known from economists and philosophers.
<br />
I also hope they add their voice to those of scientists and mathematicians,
<br />
to enlight our discussions during the session.
<br />
<p>Now I would like to draw some provocative conclusions. Analogies between
<br />
concepts or between formal expressions of quantities are useful
<br />
for the spirit, for the quality of the papers, and sometimes they
<br />
are used by modellers to demonstrate why their work merit funds
<br />
(does anybody never do that?). The number of new concepts in
<br />
sciences (includes mathematics, economy, humanities, and so on)
<br />
is increasing, and new terms are picked in our natural language:
<br />
the task of the teachers becomes harder and harder. Entropy
<br />
and Information are like the &quot;fourth dimension&quot;, one century ago:
<br />
they offer in common the ability to provide exciting topics
<br />
to discuss. Unfortunately, Entropy and Information are much more
<br />
difficult to handle.
<br />
<p>Michel Petitjean                      Email: petitjean&#64;itodys.jussieu.fr
<br />
Editor-in-Chief of Entropy                   entropy&#64;mdpi.org
<br />
ITODYS (CNRS, UMR 7086)                      ptitjean&#64;ccr.jussieu.fr
<br />
1 rue Guy de la Brosse                Phone: +33 (0)1 44 27 48 57
<br />
75005 Paris, France.                  FAX  : +33 (0)1 44 27 68 14
<br />
<a href="../../www.mdpi.net/default.htm">http://www.mdpi.net</a>                   <a href="../../www.mdpi.org/default.htm">http://www.mdpi.org</a>
<br />
<a href="../../petitjeanmichel.free.fr/itoweb.petitjean.html">http://petitjeanmichel.free.fr/itoweb.petitjean.html</a>
<br />
<a href="../../petitjeanmichel.free.fr/itoweb.petitjean.freeware.html">http://petitjeanmichel.free.fr/itoweb.petitjean.freeware.html</a>
<br />
_______________________________________________
<br />
fis mailing list
<br />
fis&#64;listas.unizar.es
<br />
<a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
<br />
<span id="received"><dfn>Received on</dfn> Mon Apr  5 09:22:34 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1393.html" title="Next message in the list">Loet Leydesdorff: "RE: [Fis] FIS / introductory text / 5 April 2004"</a></li>
<li><dfn>Previous message</dfn>: <a href="1391.html" title="Previous message in the list">Pedro C. Marijuán: "[Fis] visible world"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="1393.html" title="Next message in this discussion thread">Loet Leydesdorff: "RE: [Fis] FIS / introductory text / 5 April 2004"</a></li>
<li><a name="replies" id="replies"></a>
<dfn>Reply</dfn>: <a href="1393.html" title="Message sent in reply to this message">Loet Leydesdorff: "RE: [Fis] FIS / introductory text / 5 April 2004"</a></li>
<li><dfn>Reply</dfn>: <a href="1406.html" title="Message sent in reply to this message">Stanley N. Salthe: "Re: [Fis] FIS / introductory text / 5 April 2004"</a></li>
<li><dfn>Maybe reply</dfn>: <a href="1408.html" title="Message sent in reply to this message">Pedro C. Marijuán: "Re: [Fis] FIS / introductory text / 5 April 2004"</a></li>
<li><dfn>Reply</dfn>: <a href="1413.html" title="Message sent in reply to this message">Dr. Shu-Kun Lin: "Re: [Fis] Replies and new questions / part 1"</a></li>
<li><dfn>Reply</dfn>: <a href="1421.html" title="Message sent in reply to this message">Robert Ulanowicz: "Re: [Fis] FIS / introductory text / 5 April 2004"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1392" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1392" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1392" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1392" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:46 CET
</em></small></p>
</body>
</html>
