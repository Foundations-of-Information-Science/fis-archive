<?xml version="1.0" encoding="us-ascii"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: RE: [Fis] Only One Entropy</title>
<meta name="Author" content="Loet Leydesdorff (loet@leydesdorff.net)" />
<meta name="Subject" content="RE: [Fis] Only One Entropy" />
<meta name="Date" content="2004-05-03" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>RE: [Fis] Only One Entropy</h1>
<!-- received="Tue May  4 10:07:50 2004" -->
<!-- isoreceived="20040504080750" -->
<!-- sent="Mon, 3 May 2004 08:39:19 +0200" -->
<!-- isosent="20040503063919" -->
<!-- name="Loet Leydesdorff" -->
<!-- email="loet@leydesdorff.net" -->
<!-- subject="RE: [Fis] Only One Entropy" -->
<!-- id="000b01c430d9$5cef2730$1302a8c0@loet" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="4095DE3D.5090408&#64;cybermesa.com" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1484.html" title="Karl Javorszky: &quot;[Fis] SMB meeting Invitation to speak&quot;">Next message</a> ]
[ <a href="1482.html" title="Stanley N. Salthe: &quot;Re: [Fis] Re: Only One Entropy&quot;">Previous message</a> ]
[ <a href="1479.html" title="Michael Devereux: &quot;[Fis] Only One Entropy&quot;">In reply to</a> ]
<!-- unextthread="start" -->
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Loet Leydesdorff &lt;<a href="mailto:loet&#64;leydesdorff.net?Subject=RE:%20[Fis]%20Only%20One%20Entropy">loet@leydesdorff.net</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Mon 03 May 2004 - 08:39:19 CEST</span><br />
</address>
<p>
Dear Michael, 
<br />
&nbsp;
<br />
Thus, you provoke me to burn up one of my two options on Monday!
<br />
&nbsp;
<br />
<em class="quotelev1">&gt; According to V. J. Emery, author of the article Quantum
</em><br />
<em class="quotelev1">&gt; Statistical Mechanics in the Encyclopedia of Physics, The central
</em><br />
<em class="quotelev1">&gt; concept in dealing with incomplete information is the microscopic
</em><br />
<em class="quotelev1">&gt; definition of entropy or uncertainty introduced by von Neumann and by
</em><br />
<em class="quotelev1">&gt; Shannon: S = -k Sum[p-sub-alpha log(p-sub-alpha)]....Notice
</em><br />
<em class="quotelev1">&gt; that in all
</em><br />
<em class="quotelev1">&gt; of these summations over alpha, every member of a set of degenerate
</em><br />
<em class="quotelev1">&gt; levels is to be included. (page 994) I followed Emerys rule in my
</em><br />
<em class="quotelev1">&gt; calculation. Does anyone dispute his prescription? What about
</em><br />
<em class="quotelev1">&gt; you, Loet? Id also like to reiterate, and, hopefully,
</em><br />
<em class="quotelev1">&gt; reinforce my previous
</em><br />
<em class="quotelev1">&gt; arguments that there is no difference between informational
</em><br />
<em class="quotelev1">&gt; entropy and
</em><br />
<em class="quotelev1">&gt; the entropy used by scientists to describe any other physical system.
</em><br />
<p>I agree with S = k * H . (Actually, I gave the derivation in a previous
<br />
email.)
<br />
&nbsp;
<br />
S is the thermodynamic entropy and H the probabilistic (Shannon)
<br />
entropy. But I don't follow your conclusion that there is consequently
<br />
only one entropy. k is a constant, but it is not dimensionless. The
<br />
dimensionality of k (Joule/Kelvin) provides physical meaning to the
<br />
statistical apparatus of H. H is a generalized measure for the
<br />
dividedness, while S is a physical entity.
<br />
&nbsp;
<br />
For example, in statistical decomposition analysis one can use H as a
<br />
measure for the segregation in schools, classes, neighbourhoods, etc.
<br />
The decomposition algorithm can be derived from the &quot;microscopic&quot;
<br />
formula as: 
<br />
&nbsp;
<br />
H = H(o) + Sigma(g) P(g) H(g)
<br />
&nbsp;
<br />
H(o) is the in-between group uncertainty, while H(g) is the uncertainty
<br />
which prevails in each of the compartments. P(g) weights these
<br />
uncertainties. The relative weight of the in-between group uncertainty
<br />
(H(o)) can teach us something about the dynamics of the system. (Is it
<br />
an aggregate of lower-level groupings or are interaction terms
<br />
important?) This mathematical apparatus applied to the segregation of
<br />
social systems has no longer a physical interpretation in terms of S
<br />
because this H cannot be multiplied with k. The system of reference is
<br />
different.
<br />
&nbsp;
<br />
Each system of reference provides us with another substance that is
<br />
distributed. In physics the momenta and energies are distributed and
<br />
communicated (either conservative or dissipative). In other systems
<br />
something else is distributed. The specification of this hypothesis
<br />
spans a specific system of communication. For example, when atoms are
<br />
distributed and redistributed a chemistry is generated. When molecules
<br />
are distributed and redistributed one deals with a biology. My interest
<br />
is particularly in systems which distribute and redistribute symbolic
<br />
media of communication (meaning). These systems cannot be easily
<br />
observed, but we can measure the probabilistic entropy when such systems
<br />
operate.
<br />
&nbsp;
<br />
For example, money can be considered as a symbolically generalized
<br />
medium of communication. The flow of money in an economy can be measured
<br />
using entropy measures. In this case, one can also observe the
<br />
communication, but in other cases the (geometrical) observation is
<br />
sometimes more difficult than the (algorithmic) measurement. For
<br />
example, at the Internet one can measure the flows of communication in
<br />
terms of their expected information contents. 
<br />
&nbsp;
<br />
In summary, I seem not to agree with Emery despite the status of this
<br />
author in physics. The argument is not convincing.
<br />
&nbsp;
<br />
With kind regards, 
<br />
&nbsp;
<br />
&nbsp;
<br />
Loet 
<br />
&nbsp;&nbsp;_____  
<br />
<p>Loet Leydesdorff 
<br />
Amsterdam School of Communications Research (ASCoR)
<br />
Kloveniersburgwal 48, 1012 CX Amsterdam
<br />
Tel.: +31-20- 525 6598; fax: +31-20- 525 3681 
<br />
&nbsp;&lt;mailto:loet&#64;leydesdorff.net&gt; loet&#64;leydesdorff.net ;
<br />
&lt;<a href="../../www.leydesdorff.net/default.htm">http://www.leydesdorff.net/</a>&gt; <a href="../../www.leydesdorff.net/default.htm">http://www.leydesdorff.net/</a> 
<br />
<p>&nbsp;
<br />
&nbsp;&lt;<a href="../../www.upublish.com/books/leydesdorff-sci.htm">http://www.upublish.com/books/leydesdorff-sci.htm</a>&gt; The Challenge of
<br />
Scientometrics ;  &lt;<a href="../../www.upublish.com/books/leydesdorff.htm">http://www.upublish.com/books/leydesdorff.htm</a>&gt; The
<br />
Self-Organization of the Knowledge-Based Society
<br />
&nbsp;
<br />
<em class="quotelev1">&gt; Though youve said thermodynamic entropy differs from informational
</em><br />
<em class="quotelev1">&gt; entropy, Michel, a review of the literature seems to indicate
</em><br />
<em class="quotelev1">&gt; this is no
</em><br />
<em class="quotelev1">&gt; longer a distinction supported by scientific experts in this
</em><br />
<em class="quotelev1">&gt; field. Ill cite below some references to recent scientific
</em><br />
<em class="quotelev1">&gt; work that, I think,
</em><br />
<em class="quotelev1">&gt; make the argument decisively. Ive not been able to find any
</em><br />
<em class="quotelev1">&gt; contemporary research from the literature which disputes the
</em><br />
<em class="quotelev1">&gt; idea that
</em><br />
<em class="quotelev1">&gt; informational entropy is actually physical (thermodynamic)
</em><br />
<em class="quotelev1">&gt; entropy. Have
</em><br />
<em class="quotelev1">&gt; I failed to find those results somewhere?
</em><br />
<em class="quotelev1">&gt; First, Id point to the really concise, copiously annotated,
</em><br />
<em class="quotelev1">&gt; recent and
</em><br />
<em class="quotelev1">&gt; decisive account given of the relationship between entropy and
</em><br />
<em class="quotelev1">&gt; information written by W. T. Grandy, Jr. in Am. J. Phys. 65, 6, June
</em><br />
<em class="quotelev1">&gt; 1997, p. 466. This is a Resource Letter designed to provide
</em><br />
<em class="quotelev1">&gt; scientists
</em><br />
<em class="quotelev1">&gt; a very succinct, and comprehensive description of the history,
</em><br />
<em class="quotelev1">&gt; development, and scientific results about a particular
</em><br />
<em class="quotelev1">&gt; subject. Grandy
</em><br />
<em class="quotelev1">&gt; lists more than 160 articles and books reaching as far back as
</em><br />
<em class="quotelev1">&gt; Boltzmanns seminal paper of 1877. Its a great source for the entire
</em><br />
<em class="quotelev1">&gt; history of the development of information theory and its
</em><br />
<em class="quotelev1">&gt; relationship to
</em><br />
<em class="quotelev1">&gt; entropy.
</em><br />
<em class="quotelev1">&gt; Grandy says the principle rigorous connection of information
</em><br />
<em class="quotelev1">&gt; theory to
</em><br />
<em class="quotelev1">&gt; physics (p. 468) is based on whats known as the principle
</em><br />
<em class="quotelev1">&gt; of maximum
</em><br />
<em class="quotelev1">&gt; entropy (PME), developed by Edwin Jaynes shortly after
</em><br />
<em class="quotelev1">&gt; Shannon did his
</em><br />
<em class="quotelev1">&gt; famous work. Grandy writes that one can now safely relate the
</em><br />
<em class="quotelev1">&gt; theoretical (maximum) entropy to the fundamental entropy of Clausius.
</em><br />
<em class="quotelev1">&gt; Quantum mechanically, one employs the density matrix, rho, and von
</em><br />
<em class="quotelev1">&gt; Neumanns form of the entropy... (p. 469). Grandy doesnt equivocate
</em><br />
<em class="quotelev1">&gt; about the equality of all these forms of entropy, does he?
</em><br />
<em class="quotelev1">&gt; I was first convinced that information is tangible, so that
</em><br />
<em class="quotelev1">&gt; informational entropy is actually physical entropy, when I
</em><br />
<em class="quotelev1">&gt; read one of
</em><br />
<em class="quotelev1">&gt; Rolf Landauers papers from 1991. I expect that many of us
</em><br />
<em class="quotelev1">&gt; know Landauer
</em><br />
<em class="quotelev1">&gt; as the chief scientist at the IBM Watson laboratory in New
</em><br />
<em class="quotelev1">&gt; York for more
</em><br />
<em class="quotelev1">&gt; than twenty years, and source of Landauers Principle which may
</em><br />
<em class="quotelev1">&gt; describe the entropy cost of erasing one information bit.
</em><br />
<em class="quotelev1">&gt; He wrote a short and relatively simple article titled Information Is
</em><br />
<em class="quotelev1">&gt; Physical (Phys. Today, May, 1991, p.23) that argues,
</em><br />
<em class="quotelev1">&gt; strangely enough,
</em><br />
<em class="quotelev1">&gt; that information is actually physical. I still accept his
</em><br />
<em class="quotelev1">&gt; discussion and
</em><br />
<em class="quotelev1">&gt; conclusion. Does anyone dispute Landauers arguments? If not,
</em><br />
<em class="quotelev1">&gt; I believe
</em><br />
<em class="quotelev1">&gt; we must accept that the entropy of information is actually
</em><br />
<em class="quotelev1">&gt; the entropy
</em><br />
<em class="quotelev1">&gt; of a physical system. The same entropy we calculate for the operating
</em><br />
<em class="quotelev1">&gt; fluid in an engine, for example.
</em><br />
<em class="quotelev1">&gt; Shu-Kun, are you willing to weigh in with your view on this
</em><br />
<em class="quotelev1">&gt; topic? Ive
</em><br />
<em class="quotelev1">&gt; been reading some of your articles on the similarity principle, and
</em><br />
<em class="quotelev1">&gt; youve written that entropy in thermodynamics is a special kind of
</em><br />
<em class="quotelev1">&gt; dynamic entropy. Do you believe there is a difference between
</em><br />
<em class="quotelev1">&gt; thermodynamic and informational entropy?
</em><br />
<em class="quotelev1">&gt; Thanks, again, for this opportunity to express my understanding about
</em><br />
<em class="quotelev1">&gt; these ideas, and to hear others views.
</em><br />
<em class="quotelev1">&gt; Cordially,
</em><br />
<em class="quotelev1">&gt; Michael Devereux
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt; _______________________________________________
</em><br />
<em class="quotelev1">&gt; fis mailing list
</em><br />
<em class="quotelev1">&gt; <a href="mailto:fis&#64;listas.unizar.es?Subject=RE:%20[Fis]%20Only%20One%20Entropy">fis@listas.unizar.es</a> <a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
</em><br />
<em class="quotelev1">&gt; 
</em><br />
<span id="received"><dfn>Received on</dfn> Tue May  4 10:07:50 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1484.html" title="Next message in the list">Karl Javorszky: "[Fis] SMB meeting Invitation to speak"</a></li>
<li><dfn>Previous message</dfn>: <a href="1482.html" title="Previous message in the list">Stanley N. Salthe: "Re: [Fis] Re: Only One Entropy"</a></li>
<li><dfn>In reply to</dfn>: <a href="1479.html" title="Message to which this message replies">Michael Devereux: "[Fis] Only One Entropy"</a></li>
<!-- lnextthread="start" -->
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1483" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1483" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1483" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1483" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:46 CET
</em></small></p>
</body>
</html>
