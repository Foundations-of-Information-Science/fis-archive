<?xml version="1.0" encoding="windows-1252"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: [Fis] Physical Entropy</title>
<meta name="Author" content="Michael Devereux (dbar_x@cybermesa.com)" />
<meta name="Subject" content="[Fis] Physical Entropy" />
<meta name="Date" content="2004-05-11" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>[Fis] Physical Entropy</h1>
<!-- received="Tue May 11 07:43:26 2004" -->
<!-- isoreceived="20040511054326" -->
<!-- sent="Mon, 10 May 2004 23:42:15 -0600" -->
<!-- isosent="20040511054215" -->
<!-- name="Michael Devereux" -->
<!-- email="dbar_x@cybermesa.com" -->
<!-- subject="[Fis] Physical Entropy" -->
<!-- id="40A067B7.8000201@cybermesa.com" -->
<!-- charset="windows-1252" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1503.html" title="Robert Ulanowicz: &quot;[Fis] Entropy &amp; Info in Molecular Biology&quot;">Next message</a> ]
[ <a href="1501.html" title="Koichiro Matsuno: &quot;Re: [Fis] Re: quantum entropy&quot;">Previous message</a> ]
<!-- unextthread="start" -->
[ <a href="1504.html" title="Stanley N. Salthe: &quot;Re: [Fis] Physical Entropy&quot;">Next in thread</a> ]
 [ <a href="#replies">Replies</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Michael Devereux &lt;<a href="mailto:dbar_x&#64;cybermesa.com?Subject=Re:%20[Fis]%20Physical%20Entropy">dbar_x@cybermesa.com</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Tue 11 May 2004 - 07:42:15 CEST</span><br />
</address>
<p>
Dear Michel, Loet, Guy, Bob, Stan, and colleagues,
<br />
Thanks for all the valuable comments. I expect I’ve not been 
<br />
sufficiently precise about the equivalency I maintain exists between 
<br />
thermodynamic entropy and Shannon’s (informational) entropy. I mean by 
<br />
Shannon’s entropy just what Shannon meant in his original publication 
<br />
(Bell Sys. Tech. J. 27, 3, 379-423, 1948), but nothing more than that. 
<br />
He derived his formula, H = - Sum p log(p), as a measure of 
<br />
“information, choice and uncertainty” (p. 393) in a system of electrical 
<br />
pulses carrying information along a communications line such as a 
<br />
telegraph cable. I’ve emphasized before that this is an application to a 
<br />
physical (tangible, observable, real) system. And I find that Shannon 
<br />
did actually name H entropy: “We shall call H = - Sum p log (p) the 
<br />
entropy of the set of probabilities p1,....,pN” (p. 393)
<br />
I realize that the mathematical formula, H, is not itself the property 
<br />
of any physical system, but it represents such a property: the amount of 
<br />
information carried by electrical pulses along a communications line. 
<br />
Shannon wrote, “We wish to consider certain general properties involving 
<br />
communications systems. To do this it is first necessary to represent 
<br />
the various elements involved as mathematical entities, suitably 
<br />
idealized from their physical counterparts” (p. 381). I understand this 
<br />
to be the standard procedure for science. And I’ll show, below, that the 
<br />
information quantified by Shannon’s formula is the property of a 
<br />
physical system that actually can be measured in the laboratory.
<br />
I agree with you Michel, that “nobody would confuse the system with one 
<br />
of its mathematical models.” I suppose we all understand the distinction 
<br />
between a physical object and it’s mathematical representation. I think 
<br />
mathematics, in spite of its intimate connection to science, resides in 
<br />
the realm of pure ideas, and not where measured, observable results may 
<br />
be compared in the laboratory. I guess we could label that a part of 
<br />
metaphysics, Guy, along with such things as morality, theology, 
<br />
philosophy and such. I expect you’d agree with me that the validity of a 
<br />
mathematical theorem isn’t decided by a scientific measurement. Nor is 
<br />
the value of a poem. Or the difference between good and bad.
<br />
So, I’m sure, Loet, that physics can’t ultimately describe everything. I 
<br />
expect you were thinking much the same thing, but suggesting that 
<br />
physics does try to explain all of the natural world.
<br />
I believe I understand what you’re saying, Michel, that “I consider that 
<br />
the system and its model are two different entities. It is why, in some 
<br />
practical situations dealing with entropy, I consider that the 
<br />
informational entropy is able to modelize the physical entry, but is not 
<br />
the physical entropy.”
<br />
I agree, Michel, that the mathematical formula H, models a property of a 
<br />
physical system, and is not, itself, that property. But, I’m convinced 
<br />
the same can be said for every mathematical description used by science; 
<br />
every formula, including, for example. Clausius version of thermodynamic 
<br />
entropy, Delta Q / T. Clausius’ formula is a mathematical idealization 
<br />
that can be substantiated, for measured heat and temperature in the 
<br />
laboratory, to within some observed precision.
<br />
The question, then, I hope all will agree, is not whether scientists use 
<br />
mathematical models to idealize both thermodynamic entropy and the 
<br />
amount of information that can be transmitted through a telegraph cable, 
<br />
but whether the Shannon formula H describes (models, idealizes, etc.) 
<br />
the identical physical property that is described by the formulas for 
<br />
thermodynamic entropy.
<br />
For purposes of my argument, I mean by Shannon’s entropy, exclusively, 
<br />
the information carried by some physical system described by a 
<br />
probability distribution. My reading of his paper indicates that this is 
<br />
what Shannon meant also.
<br />
I accept, Bob, that the H formulation may also depict a “statistical 
<br />
entropy” which might both increase or decrease with time. But I’m sure 
<br />
that the information carried by a bounded, isolated, physical system, 
<br />
and described by the formula H, never increases. That’s one of Shannon’s 
<br />
conclusions for electrical signals on a telegraph cable, also. If the 
<br />
cable is noisy, some information is lost between the sender and 
<br />
receiver, but there’s never more information delivered than was 
<br />
originally sent.
<br />
I also accept, Loet, that your decomposition algorithm appropriately and 
<br />
usefully employs the H formula. But, I think it’s applied to a different 
<br />
system of reference, as you’ve written, and not to the amount of 
<br />
information described by the probability distribution of a physical 
<br />
system. As you say, the systems of reference determine the appropriate 
<br />
level of theorizing.
<br />
We believe that thermodynamic entropy is physical, so I’ve emphasized 
<br />
Landauer’s argument that all information is physical also. Notice that 
<br />
Shannon’s derivation of H confirms Landauer’s conclusion, explicitly 
<br />
treating information as a physical property of the distribution of 
<br />
electrical signals in a cable.
<br />
The bandwidth of an electrical cable can be determined from the measured 
<br />
physical characteristics of the cable. From the cable’s geometry, 
<br />
conductivity, capacitance, inductance, and such, one can calculate the 
<br />
bandwidth, which determines the maximum rate of information transmission 
<br />
through the cable. If one attempts to send information at a higher rate 
<br />
than this maximum (information being calculated from Shannon’s H formula 
<br />
for the given distribution of electrical pulses sent), noise in the 
<br />
cable diminishes the maximum rate of information received to this 
<br />
bandwidth value. Thus, Shannon’s information is, indeed, measurable.
<br />
And Jaynes (Phys. Rev. 106, 620-630, 1957) has shown that the 
<br />
thermodynamic entropy of a physical system, described by some 
<br />
probability distribution, is just that amount of information needed to 
<br />
remove the uncertainty from that distribution. Of course, information is 
<br />
calculated from Shannon’s formula, and thermodynamic entropy from one of 
<br />
the classical formulas of that discipline. Notably, Gibbs (Elementary 
<br />
Principles in Statistical Mechanics, Ox Bow, Woodbridge, CT, USA, 1981, 
<br />
first published in 1902) reached essentially the same result when he 
<br />
minimized his “average index of probability of phase” subject to 
<br />
constraints on average total energy.
<br />
So, that’s my argument, and I’m sticking to it. At least until someone 
<br />
can point to the errors in it. Thanks so much for all the stimulating ideas.
<br />
Cordially,
<br />
Michael Devereux
<br />
<p>_______________________________________________
<br />
fis mailing list
<br />
fis&#64;listas.unizar.es
<br />
<a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
<br />
<span id="received"><dfn>Received on</dfn> Tue May 11 07:43:26 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1503.html" title="Next message in the list">Robert Ulanowicz: "[Fis] Entropy &amp; Info in Molecular Biology"</a></li>
<li><dfn>Previous message</dfn>: <a href="1501.html" title="Previous message in the list">Koichiro Matsuno: "Re: [Fis] Re: quantum entropy"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="1504.html" title="Next message in this discussion thread">Stanley N. Salthe: "Re: [Fis] Physical Entropy"</a></li>
<li><a name="replies" id="replies"></a>
<dfn>Reply</dfn>: <a href="1504.html" title="Message sent in reply to this message">Stanley N. Salthe: "Re: [Fis] Physical Entropy"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1502" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1502" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1502" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1502" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:46 CET
</em></small></p>
</body>
</html>
