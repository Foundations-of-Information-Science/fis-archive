<?xml version="1.0" encoding="us-ascii"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: [Fis] Entropy and information</title>
<meta name="Author" content="Karl Javorszky (javorszky@eunet.at)" />
<meta name="Subject" content="[Fis] Entropy and information" />
<meta name="Date" content="2004-04-17" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>[Fis] Entropy and information</h1>
<!-- received="Sat Apr 17 19:06:10 2004" -->
<!-- isoreceived="20040417170610" -->
<!-- sent="Sat, 17 Apr 2004 18:54:01 +0200" -->
<!-- isosent="20040417165401" -->
<!-- name="Karl Javorszky" -->
<!-- email="javorszky@eunet.at" -->
<!-- subject="[Fis] Entropy and information" -->
<!-- id="LDEAKGIDMKMIFBDIPEJNEEBNPAAA.javorszky@eunet.at" -->
<!-- charset="us-ascii" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1436.html" title="John Collier: &quot;Re: [Fis] Szilard's Engine and Information&quot;">Next message</a> ]
[ <a href="1434.html" title="Shu-Kun Lin: &quot;Re: [Fis] Szilard's Engine and Information&quot;">Previous message</a> ]
<!-- unextthread="start" -->
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Karl Javorszky &lt;<a href="mailto:javorszky&#64;eunet.at?Subject=Re:%20[Fis]%20Entropy%20and%20information">javorszky@eunet.at</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Sat 17 Apr 2004 - 18:54:01 CEST</span><br />
</address>
<p>
Please allow me to offer terminology in this highly hilosophical discussion.
<br />
Bob has unfolded the semantic parallels and dissonances between the concepts
<br />
&quot;entropy&quot;, &quot;information&quot;, &quot;thermodynamics&quot; and &quot;statistical approaches&quot;. The
<br />
question is, as I read the red line of the conversation, what we understand
<br />
under these words and how the concepts understood form a pattern within our
<br />
brain to which we say: this is pleasant, this is how the world can be
<br />
understood well, this is a right explanation of the world; and then, in the
<br />
best case we find measurements with Nature that support our (newly, by means
<br />
of th explanation) established way of relating concepts to each other.
<br />
Nothing goes above numbers when offering something to agree on. We work now
<br />
on the membrane separating and joining mathematics, philosophy, information
<br />
theory, biotheory and fundamental physics. Let me try to offer a re-wording
<br />
of Bob's contribution by talking about numbers. The neutral language may
<br />
help.
<br />
<p>Neumann and Shannon and many more refer to the everyday experience that
<br />
things tend to come in a most idle, convenient, effortless state. This is
<br />
the subjective, psychological way of putting the idea of entropy. The same
<br />
concept can be seen as a statistical trend for logical relations on a set to
<br />
be in or migrate towards their most probable state. Entropy means the most
<br />
probable state of a set.
<br />
<p>Relative to the most usual (probable) state of the set, it is in any
<br />
realisation (moment) in a specific deviation to the idealised (most
<br />
probable) state. This is information. We set the information content of a
<br />
set in its most probable state to Zero and describe each actual state as (a
<br />
collection of ) extents of difference relative to the most probable state.
<br />
The deviation from the mean is opposed to the mean as information is opposed
<br />
to entropy (Shu Kun).
<br />
<p>The phenomenology behind things has its roots in the organsational structure
<br />
of our brain: we treat tactile information (nervous excitement mitigated by
<br />
receptors in the skin) differently to non-tactile (thought-up) mental
<br />
excitements (concepts). Thus we treat &quot;objects&quot; and &quot;logical relations&quot; as
<br />
radically differing concepts. But, if we have (and we do have) the maximal
<br />
number of logical relations representable on n objects, the we also know the
<br />
minimal fraction of one object among n needed to represent a logical
<br />
relation on. This would be a statistical approach to the matter-energy
<br />
equivalence. Each logical relation counts as a fraction of an object. It has
<br />
been shown that the density of logical relations depends more on the inner
<br />
differentiation of objects of a set (how many distinct symbols are present
<br />
in the set: how individated the obejcts are) than on the number of objects
<br />
(within some circumstances). The proposal is to make one more abstractional
<br />
step, away from statistical mechanics and statistical thermodynamics and
<br />
statistical whatever towards pure statistical statistics. That is where the
<br />
stew brews.
<br />
<p>If the numbers (the logical relations represented on a most probable set
<br />
consisting of n objects) themselves coagulate and densify into an object,
<br />
and then enforce distances and properties on the set, then we shall have
<br />
found a model for a statistical whatever. The abstract set can be made to
<br />
sing and dance (of course within constraints!) and do all of &quot;uncertainty&quot;,
<br />
&quot;indeterminacy&quot; or even (heaven forbid!) &quot;complexity&quot;, and one shall have a
<br />
relaxed and detached opportunity to decide what to call &quot;entropy&quot; and what
<br />
not.
<br />
<p>As to constraints: The economical aspect of information theory has been
<br />
emphasized by Pedro Marijuan's counting of all available logical expressions
<br />
in the totality of logical interrelations on a set. As we deal with the
<br />
autoregulation of one (1) system, we are basically in a very non-infinite
<br />
multitude. The elementar expressions describing the system the functioning
<br />
of which we are trying to understand can be too few to maintain a logical
<br />
statement over a statistically relevant succession of time ticks (linear
<br />
neighbourhood steps). In his model one can envision competition for logical
<br />
constants among possible places within expressions. Pedro's curve shows the
<br />
availability of constants. Probability and economy meet in this idea.
<br />
<p>In my understanding of the debate, the contributors have targeted a concept
<br />
of the most probable (usual) state of a set and its tendencies to deviate in
<br />
any of k dimensions away from the expectation value and reapproach it. This
<br />
is indeed a deeply probabilistic approach and there is a way to look at
<br />
natural numbers that will show the concepts in a clear fashion.
<br />
<p>Karl
<br />
-----Ursprungliche Nachricht-----
<br />
Von: fis-bounces&#64;listas.unizar.es [mailto:fis-bounces&#64;listas.unizar.es]Im
<br />
Auftrag von Robert Ulanowicz
<br />
Gesendet: Dienstag, 13. April 2004 16:51
<br />
An: fis&#64;listas.unizar.es
<br />
Betreff: Re: [Fis] FIS / introductory text / 5 April 2004
<br />
<p>Please excuse my tardiness in joining the discussion initiated by
<br />
Michel. Perhaps when I am done with my idiosyncratic comments, several
<br />
of you will wish I had simply remained silent. :)
<br />
<p>To begin with, most of us recall that Shannon named his measure &quot;entropy&quot;
<br />
after the humorous suggestion by von Neumann. Von Neumann was asked by
<br />
Shannon what to name the formula, H=-Sum(i) pi log(pi)? Von Neumann
<br />
replied rather coyly that he should name it entropy, because (1) the
<br />
measure is formally the same as that used by Boltzmann in his statistical
<br />
mechanical treatment of irreversibility, and (2) &quot;Nobody really
<br />
understands what entropy is, so you will be at an advantage in any
<br />
discusssion!&quot; It was a clever, mischievous response, but Shannon
<br />
apparently took him seriously, and it became a bad joke on all of us!
<br />
<p>I was educated as a chemical engineer, and one of the shibboleths of
<br />
our upbringing was that thermodynamics is a purely phenomenological and
<br />
macroscopic endeavor. Any student attempting to break this taboo paid a
<br />
drastic price for his/her impertinence. Hence, at any formal exam, if a
<br />
student were responding to a question in thermodynamics and chanced to
<br />
mention the term &quot;atom&quot; or &quot;molecule&quot; a virtual gong would sound, the
<br />
student would be interrupted and told to leave the room. He/she had
<br />
failed and would be banished to where there is weeping and gnashing of
<br />
teeth! :)
<br />
<p>The reasoning was that thermodynamics rests on a phenomenological
<br />
foundation that is far more solid than any atomic &quot;hypothesis&quot;. After all,
<br />
in 1820 when Sadi Carnot described the behavior of engines that pumped
<br />
water from mines, he did so from a purely phenomenological perspective.
<br />
That his conception of irreversibility happened to collide with the
<br />
prevailing notions of conservation and temporal symmetry (which Aemelie
<br />
Noether later told us were equivalent) was of little consequence to an
<br />
engineer. But it put the physicists' view of the world in serious
<br />
jeopardy. For the next 30 to 50 years it was the atomic hypothesis, not
<br />
the second law (which, incidentally, was discovered before the first) that
<br />
was truly at risk.
<br />
<p>We all know that this conflict was finally resolved when Boltzmann and
<br />
Gibbs constructed statistical mechanics as a bridge between
<br />
thermodynamics and physics -- or at least we think we know. As Michel
<br />
suggested, it was an exceedingly narrow bridge, indeed! Consider the
<br />
following: The positivist notion of science is that one should be
<br />
continually trying to disprove hypotheses -- to subject them to all
<br />
manner of rigorous tests. Well, the atomic hypothesis was clearly at
<br />
risk and what did we do? We discovered a miniscule range of phenomena
<br />
across which there was some correspondence between the two conflicting
<br />
disciplines and we immediately called a halt to the dialogue! We
<br />
considered the hypothesis proven.
<br />
<p>I would submit that there is more to these rantings than mere fancy on
<br />
my part. Consider that whenever a physicist is asked about
<br />
thermodynamics, the response is almost invariably from the perspective
<br />
of statistical mechanics. For most physicists, STATISTICAL MECHANICS *IS*
<br />
THERMODYNAMICS. Thermodynamics in its phenomenological guise continues
<br />
to pose a major challenge to physics. It is the blackberry seed in the
<br />
wisdom tooth of the physicist. The response is to envelop
<br />
thermodynamics within the safe cocoon of stat mech.
<br />
<p>As Shu-Kun pointed out, entropy and information are opposites, and we
<br />
would do well not to confound them. It is possible to measure entropy
<br />
purely in terms of energetic changes. In fact, the variables of
<br />
thermodynamics are all naturally skewed towards the energetic. Most
<br />
constraints enter thermo via external boundaries (although some
<br />
constraints are embedded to an extent in state variables like the Gibbs
<br />
free energy.)
<br />
<p>Information, on the other hand, is dominated by constraint. (In fact, I
<br />
argued earlier that information could be understood purely in terms of
<br />
constraint.) Not that it is always necessary to know the details of the
<br />
constraints explicitly. Just as one can phenomenologically measure
<br />
thermodynamical variables in ignorance of molecular movements, one can
<br />
also gauge the level of information according to their effects and in
<br />
abstraction from knowing exactly how the constraints are working.
<br />
<p>So I would conclude with a plea to respect the autonomy of
<br />
thermodynamics from statistical mechanics -- that whatever overlap
<br />
entropy might have with probabilistic notions pales in comparison with
<br />
its meaning in a physical, energetic sense. None of which is to
<br />
diminish the value and utility of Shannon's probabilistic measure,
<br />
which has been enormous. It's just a plea to call it something like
<br />
&quot;uncertainty&quot;, &quot;indeterminacy&quot; or even (heaven forbid!) &quot;complexity&quot;,
<br />
but at all costs to avoid calling it &quot;entropy&quot;!
<br />
<p>Regards to all,
<br />
Bob
<br />
<p>-------------------------------------------------------------------------
<br />
Robert E. Ulanowicz                |  Tel: (410) 326-7266
<br />
Chesapeake Biological Laboratory   |  FAX: (410) 326-7378
<br />
P.O. Box 38                        |  Email &lt;ulan&#64;cbl.umces.edu&gt;
<br />
1 Williams Street                  |  Web &lt;<a href="../../www.cbl.umces.edu/~ulan">http://www.cbl.umces.edu/~ulan</a>&gt;
<br />
Solomons, MD 20688-0038            |
<br />
--------------------------------------------------------------------------
<br />
<p><p><p>_______________________________________________
<br />
fis mailing list
<br />
fis&#64;listas.unizar.es
<br />
<a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
<br />
<p>_______________________________________________
<br />
fis mailing list
<br />
fis&#64;listas.unizar.es
<br />
<a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
<br />
<span id="received"><dfn>Received on</dfn> Sat Apr 17 19:06:10 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1436.html" title="Next message in the list">John Collier: "Re: [Fis] Szilard's Engine and Information"</a></li>
<li><dfn>Previous message</dfn>: <a href="1434.html" title="Previous message in the list">Shu-Kun Lin: "Re: [Fis] Szilard's Engine and Information"</a></li>
<!-- lnextthread="start" -->
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1435" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1435" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1435" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1435" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:46 CET
</em></small></p>
</body>
</html>
