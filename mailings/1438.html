<?xml version="1.0" encoding="us-ascii"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: Re: [Fis] Szilard's Engine and Information</title>
<meta name="Author" content="Stanley N. Salthe (ssalthe@binghamton.edu)" />
<meta name="Subject" content="Re: [Fis] Szilard's Engine and Information" />
<meta name="Date" content="2004-04-19" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>Re: [Fis] Szilard's Engine and Information</h1>
<!-- received="Mon Apr 19 14:07:10 2004" -->
<!-- isoreceived="20040419120710" -->
<!-- sent="Mon, 19 Apr 2004 08:28:17 -0500" -->
<!-- isosent="20040419132817" -->
<!-- name="Stanley N. Salthe" -->
<!-- email="ssalthe@binghamton.edu" -->
<!-- subject="Re: [Fis] Szilard's Engine and Information" -->
<!-- id="l03130300bca9844f432b@[128.226.180.93]" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="[Fis] Szilard's Engine and Information" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1439.html" title="Loet Leydesdorff: &quot;RE: [Fis]  Data, observations and distributions / was: Re: Probabilistic Entropy&quot;">Next message</a> ]
[ <a href="1437.html" title="Michel Petitjean: &quot;[Fis]  Data, observations and distributions / was: Re: Probabilistic Entropy&quot;">Previous message</a> ]
[ <a href="1400.html" title="Michael Devereux: &quot;[Fis] Szilard's Engine and Information&quot;">Maybe in reply to</a> ]
<!-- unextthread="start" -->
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Stanley N. Salthe &lt;<a href="mailto:ssalthe&#64;binghamton.edu?Subject=Re:%20[Fis]%20Szilard's%20Engine%20and%20Information">ssalthe@binghamton.edu</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Mon 19 Apr 2004 - 15:28:17 CEST</span><br />
</address>
<p>
Below I respond to Shu-Kun's most interesting posting.
<br />
<p><em class="quotelev1">&gt;Dear Stan,
</em><br />
<em class="quotelev1">&gt;1. Thanks for considering &quot;order&quot; and &quot;constraint&quot;.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;Let us forget symmetry first: &quot;more constraints, more order&quot; should be
</em><br />
<em class="quotelev1">&gt;accepted.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;To be complete, a system has higher &quot;order&quot; may imply the following: if
</em><br />
<em class="quotelev1">&gt;there are more constraints, there will be &gt;less freedom, less dynamic,
</em><br />
<em class="quotelev1">&gt;more static and more confined; (or less symmetric), more information, less
</em><br />
<em class="quotelev1">&gt;entropy
</em><br />
<em class="quotelev1">&gt;and higher &quot;order&quot;. (I guess this is what you meant by &quot;constraints&quot; in
</em><br />
<em class="quotelev1">&gt;another thread where you said &quot;further &gt;constraints are bearing upon some
</em><br />
<em class="quotelev1">&gt;systems, hemming them in with a greater burden of information&quot;.
</em><br />
<em class="quotelev1">&gt;Constraints &gt;made a structure with a higher amount of information.
</em><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: Yes, we agree here.  I like to add, however, that a highly
<br />
constrained system also has a very high potential H.  That is, with many
<br />
constraints, many things can go wrong!  A system must assume unusual
<br />
configurations in order to heal insults. That is, if a system is an
<br />
adaptable one, adaptability will involve taking up less frequent behaviors.
<br />
So, in a fluctuating eviroment, I think what we want to say is that with
<br />
increasing information bearing upon (or within) a system, the fewer are its
<br />
routine behaviors.  That is, S actual = S maximum - I  (See Brooks and
<br />
Wiley, &quot;Evolution As Entropy&quot;).
<br />
<p><em class="quotelev1">&gt;Keep this in mind, let us examine two examples: 1. the constraint is the
</em><br />
<em class="quotelev1">&gt;wall separating two ideal gases (oxygen &gt;and nitrogen).
</em><br />
<em class="quotelev1"> &gt;2. the constraint is the wall separating oil and water.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;Excuse me but let me try to sell my humble theory called &quot;similarity
</em><br />
<em class="quotelev1">&gt;principle&quot; here. It says that entropy &gt;increases with the increase in the
</em><br />
<em class="quotelev1">&gt;property similarity of the components (see my S-Z plot at
</em><br />
<em class="quotelev2">&gt;&gt;<a href="../../www.mdpi.org/lin/default.htm">http://www.mdpi.org/lin/</a>). This is the basis of my revision of the
</em><br />
<em class="quotelev1">&gt;information theory where information and &gt;entropy are defined and their
</em><br />
<em class="quotelev1">&gt;general properties are expressed as three laws similar to thermodynamic
</em><br />
<em class="quotelev1">&gt;laws. For &gt;example, Energy (E) divided by (kT, k is Boltzmann constant, T
</em><br />
<em class="quotelev1">&gt;is temperature) will give a dimensionless measure called L. The first law
</em><br />
<em class="quotelev1">&gt;of thermodynamics  E= kTS+F (F is free energy) will become L= S+I
</em><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: Yes.  I note that this is quite close to Brillouin's negentropy
<br />
principle of information (NPI).  That is, S actual = S max - I.  So, I see
<br />
L as referring to S actual, or average routine behavior of the system.
<br />
<p>(This is my unpublished work. Thermodynamics is a special case of
<br />
information theory criteria for conservation of certain structural
<br />
feature).
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: Yes.  Formally we have: {Shannon concept { Boltzmann concept}}.
<br />
<p><em class="quotelev1">&gt;During a process, the increased similarity among two parts of a system
</em><br />
<em class="quotelev1">&gt;will increase entropy and reduce information.
</em><br />
<em class="quotelev1">     &gt;
</em><br />
<em class="quotelev1">&gt;1.1. The two parts of ideal gases may have other difference in (P),
</em><br />
<em class="quotelev1">&gt;temperature (T) or chemical concentration or &gt;in density, and will be a
</em><br />
<em class="quotelev1">&gt;spontaneous process if the constraint is removed. At the end, the two
</em><br />
<em class="quotelev1">&gt;ideal gas parts &gt;become the same (in P, T and density, etc.): the system
</em><br />
<em class="quotelev1">&gt;seeks such sameness, driving by information loss
</em><br />
<em class="quotelev1">&gt;(in thermodynamics, potential or free energy reduction) or entropy increase.
</em><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: So, &quot;sameness&quot; relates to the disorder concept, which can be
<br />
applied anywhere.
<br />
<p><em class="quotelev1">&gt;They (oxygen and nitrogen) would like to aggregate because they are both
</em><br />
<em class="quotelev1">&gt;the same kind of substances: both are &gt;ideal gas. However, a constraint
</em><br />
<em class="quotelev1">&gt;can maintain other differences (P, T, etc., or one side has concentration
</em><br />
<em class="quotelev1">&gt;of oxygen
</em><br />
<em class="quotelev1">&gt;100% and nitrogen 0%, the other side has oxygen 0% and nitrogen 100%) and
</em><br />
<em class="quotelev1">&gt;order.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;1.2. Unlike ideal gases (nitrogen and oxygen) which are intrinsically the
</em><br />
<em class="quotelev1">&gt;same to a device measuring P and T (or &gt;kinetic energy of the molecule),
</em><br />
<em class="quotelev1">&gt;oil and water are regarded as intrinsically different. A constraint is not
</em><br />
<em class="quotelev1">&gt;required &gt;to separate the two phases. Therefore, the arrangement of oil
</em><br />
<em class="quotelev1">&gt;and water droplets may be used to record data in a &gt;solid device.
</em><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: That is to say, they interact in a thermal fashion.
<br />
<p><em class="quotelev1">&gt;Truly different substances spontaneously separate because the same
</em><br />
<em class="quotelev1">&gt;substances O and O would like to aggregate and &gt;W and W would like to
</em><br />
<em class="quotelev1">&gt;aggregate. As a result, O and W are separated. This follows my similarity
</em><br />
<em class="quotelev1">&gt;principle pretty &gt;well. Now the constraints are needed to stop O and O
</em><br />
<em class="quotelev1">&gt;droplets from aggregation to maintain the information.
</em><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: I only qibble with your use of &quot;spontaneous&quot; here.  I think the
<br />
separation of different substances requires energy input (which was applied
<br />
when O and W were stirred up to begin with).
<br />
<p><em class="quotelev1">&gt;The property similarity can be controlled by certain operation (I remember
</em><br />
<em class="quotelev1">&gt;a colleague just discussed the word &gt;&quot;operation&quot; in another thread):
</em><br />
<em class="quotelev1">&gt;pressure (P) and temperature (T) control. At certain very high P and very
</em><br />
<em class="quotelev1">&gt;high &gt;T, water (W) and oil (O) will spontaneously mix to form a
</em><br />
<em class="quotelev1">&gt;homogeneous fluid phase. Then, only constraint (a &gt;wall) can separate them
</em><br />
<em class="quotelev1">&gt;and can maintain the &quot;order&quot; or the orderly structure and maintain the
</em><br />
<em class="quotelev1">&gt;final 2 bits of &gt;information.
</em><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: So, here we have the thermal application of pressure replacing the
<br />
stirring.
<br />
<p><em class="quotelev1">&gt;The gravity of the Earth will further reduce the similarity between water
</em><br />
<em class="quotelev1">&gt;and oil and will facilitate their &gt;separation at low temperature. I got my
</em><br />
<em class="quotelev1">&gt;Ph.D. from an organic chemistry laboratory and I was a postdoc doing
</em><br />
<em class="quotelev2">&gt;&gt;organic chemical synthesis (preparing all kinds of &quot;oil&quot;, the so-called
</em><br />
<em class="quotelev1">&gt;organic  compounds) in a medicinal &gt;chemistry labor where the W/O
</em><br />
<em class="quotelev1">&gt;separation in a separation  funnel, the so-called hydrophobic effect, was
</em><br />
<em class="quotelev1">&gt;used &gt;daily. I also add salts to the water phase to make the aqueous phase
</em><br />
<em class="quotelev1">&gt;more aqueous and more different from the oil &gt;phase. I find everything I
</em><br />
<em class="quotelev1">&gt;observed fits into my &quot;similarity principle&quot;.
</em><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: This looks good to me.
<br />
<p><em class="quotelev1">&gt;2. Szilard's Engine and Information
</em><br />
<em class="quotelev1">&gt;Mixing of different ideal gases (oxygen and nitrogen) originally separated
</em><br />
<em class="quotelev1">&gt;in two parts of a rigid container at the &gt;same and unchanged T and P
</em><br />
<em class="quotelev1">&gt;cannot generate any mechanical work. This is for sure. If Szilard's Engine
</em><br />
<em class="quotelev1">&gt;works, the
</em><br />
<em class="quotelev1">&gt;2nd law will be no law at all. Of course Szilard's discussion contributed
</em><br />
<em class="quotelev1">&gt;tremendously to information theory.
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;However, if the constraints separating an ideal gas into two or more parts
</em><br />
<em class="quotelev1">&gt;in flexible containers (gas bubbles in a &gt;liquid phase), the aggregation
</em><br />
<em class="quotelev1">&gt;to form a bulky gaseous phase can generate mechanical work if there is a
</em><br />
<em class="quotelev1">&gt;suitable &gt;machine. This is similar to oil droplets aggregation in water to
</em><br />
<em class="quotelev1">&gt;form a bulky oil phase. Oil/water separation at a &gt;lower temperature can
</em><br />
<em class="quotelev1">&gt;create mechanical work.
</em><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: And this supports my idea that the O/W separation is a thermal
<br />
one, NOT spontaneous.
<br />
<p><em class="quotelev1">&gt;This might be the very mechanism of our muscle contraction and relaxation
</em><br />
<em class="quotelev1">&gt;cycle to do animal work. The machine (an Engine) is an animal muscle cell.
</em><br />
<em class="quotelev1">&gt;Gas bubbles in water and oil droplets in water have the same kind of
</em><br />
<em class="quotelev1">&gt;constraints and obeys the same kind of &quot;order&quot;, in my humble opinion.
</em><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: Hmm.  This seems a great simplification in undertsanding, if true.
<br />
I cannot comment because of my limited knowledge of muscle contraction.
<br />
<p><em class="quotelev1">&gt;(Ideal gas molecules by definition do not have molecular interaction. Here
</em><br />
<em class="quotelev1">&gt;we ignore molecular interaction among &gt;molecules in a bulk condensed phase
</em><br />
<em class="quotelev1">&gt;and on the interfaces. Hydrophobic effect has been explained mainly by
</em><br />
<em class="quotelev1">&gt;surface free energy consideration.)
</em><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: So, it reprsents work.
<br />
<p><em class="quotelev1">&gt;3. Information (I) is the measure of the compressed data
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;Finally, as a chemist, my interest is to use information (I) to
</em><br />
<em class="quotelev1">&gt;characterize the structures and their stability of a &gt;molecular system. I
</em><br />
<em class="quotelev1">&gt;am sure different structures with different constraints (either
</em><br />
<em class="quotelev1">&gt;macroscopic or microscopic
</em><br />
<em class="quotelev1">&gt;constraints) represent different amount of information. The difference and
</em><br />
<em class="quotelev1">&gt;the information change can be &gt;experimentally (operationally) measured by
</em><br />
<em class="quotelev1">&gt;monitoring certain chemical and physical properties before and &gt;after the
</em><br />
<em class="quotelev1">&gt;constraints are removed. To make any progress on this direction, it is
</em><br />
<em class="quotelev1">&gt;necessary to define information &gt;(I). My definition of information (I) is
</em><br />
<em class="quotelev1">&gt;the measure of the compressed data. If the amount of data cannot be
</em><br />
<em class="quotelev1">&gt;compressed any further, that amount is called information (I). Then, the
</em><br />
<em class="quotelev1">&gt;maximum data is L.  Entropy S= L-I. &gt;Lewis, Brillouin and many other
</em><br />
<em class="quotelev1">&gt;people accepted &quot;information loss = entropy increase&quot; relation.
</em><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: Yes, this is the basis of my thinking about information.
<br />
<p><em class="quotelev1">&gt;Shannon's &gt;consideration is a dynamic channel of communication. Shannon
</em><br />
<em class="quotelev1">&gt;basically call entropy something like &gt;&quot;entropy of information &quot; (or
</em><br />
<em class="quotelev1">&gt;entropy of signal, entropy of a set of symbol, etc.). Loet's comments are
</em><br />
<em class="quotelev1">&gt;correct for &gt;other area of studies where we may tolerate if the difference
</em><br />
<em class="quotelev1">&gt;between entropy and information concepts are not &gt;told. However, because
</em><br />
<em class="quotelev1">&gt;we are doing science, it is always very important (at least harmless) to
</em><br />
<em class="quotelev1">&gt;define clearly the &gt;obvious. If information (I) is defined, we can work.
</em><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: OK.  Energy gradients all represent information.
<br />
<p><em class="quotelev1">&gt;My interest as a chemist is a static structure (not a communication
</em><br />
<em class="quotelev1">&gt;channel) and there is always an information &gt;amount. Even for an ideal gas
</em><br />
<em class="quotelev1">&gt;where molecules are always in kinetic motion, if it is confined in a
</em><br />
<em class="quotelev1">&gt;chamber, the &gt;chamber (a constraint) defines a static structure for the
</em><br />
<em class="quotelev1">&gt;ideal gas.  Electrons and nuclei are confined in a static &gt;structure
</em><br />
<em class="quotelev1">&gt;called molecule. Therefore we can consider individual molecule and its
</em><br />
<em class="quotelev1">&gt;stability by information theory.
</em><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: Yes. I will forward this posting to a biochemist who think
<br />
aboutthis kind of thing a lot.
<br />
<p><em class="quotelev1">&gt;Excuse me also for my repeated presentation because I think I expressed
</em><br />
<em class="quotelev1">&gt;these ideas many times before, at least &gt;when we were discussing molecular
</em><br />
<em class="quotelev1">&gt;recognition.
</em><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SS: No excuses necessary!  It is always revealing to see a lot of
<br />
thought concisely presented.
<br />
<p>STAN
<br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;Best regards,
</em><br />
<em class="quotelev1">&gt;Shu-Kun
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev1">&gt;Stanley N. Salthe wrote:
</em><br />
<em class="quotelev1">&gt;
</em><br />
<em class="quotelev2">&gt;&gt;Shu-Kun said:
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev3">&gt;&gt;&gt;Regarding the related entropy of mixing (Delta S), it is certain that &gt;the
</em><br />
<em class="quotelev3">&gt;&gt;&gt;entropy of mixing is an information theoretical entropy &gt;because there is
</em><br />
<em class="quotelev3">&gt;&gt;&gt;no heat involved. It should not be taken as a typical &gt;thermodynamic
</em><br />
<em class="quotelev3">&gt;&gt;&gt;entropy (Delta S). Mixing of two chiral molecules &gt;gas R and gas L you
</em><br />
<em class="quotelev3">&gt;&gt;&gt;mentioned cannot be a thermal process. Therefore, &gt;it is not a
</em><br />
<em class="quotelev3">&gt;&gt;&gt;thermodynamic process in an heat engine. Mixing of R &gt;and L cannot be used
</em><br />
<em class="quotelev3">&gt;&gt;&gt;to generate mechanical work. This is a fact. &gt;When we discuss the engine
</em><br />
<em class="quotelev3">&gt;&gt;&gt;and related possibility of energy &gt;conservation, this fact must be kept in
</em><br />
<em class="quotelev3">&gt;&gt;&gt;mind. &gt; &gt;If the mixing of gas R and gas L would create work (a kind of
</em><br />
<em class="quotelev3">&gt;&gt;&gt;mechanical &gt;energy calculated as distance times force), one should be able
</em><br />
<em class="quotelev3">&gt;&gt;&gt;to also create &gt;mechanical work by mixing red color and black color.
</em><br />
<em class="quotelev3">&gt;&gt;&gt;
</em><br />
<em class="quotelev3">&gt;&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;to which I replied:
</em><br />
<em class="quotelev2">&gt;&gt;     SS:   I find this interesting in regard to the understanding of
</em><br />
<em class="quotelev2">&gt;&gt;physical entropy as disorder. This interpretation has been disputed because
</em><br />
<em class="quotelev2">&gt;&gt;of examples like mixtures of oil and water, which seem to spontaneously
</em><br />
<em class="quotelev2">&gt;&gt;separate, making a more orderly result than was present in the mixed state.
</em><br />
<em class="quotelev2">&gt;&gt;But Shu-Kun's posting here suggests why this understanding is specious.
</em><br />
<em class="quotelev2">&gt;&gt;What is neglected in this view is that energy-utilizing work had to be done
</em><br />
<em class="quotelev2">&gt;&gt;to mix the oil and water to begin with, in a thermal process. This process
</em><br />
<em class="quotelev2">&gt;&gt;set up a curious kind of dispersed energy gradient, which then dissipated,
</em><br />
<em class="quotelev2">&gt;&gt;producing the separation, and giving off heat again. That is to say, the
</em><br />
<em class="quotelev2">&gt;&gt;unmixing of oil and water is a kind of work (not unlike the unwinding of
</em><br />
<em class="quotelev2">&gt;&gt;many wound up rubber bands!), and, as such, would not be expected to
</em><br />
<em class="quotelev2">&gt;&gt;produce disorder. Put another way, the unmixing of oil and water is NOT
</em><br />
<em class="quotelev2">&gt;&gt;SPONTANEOUS, but is instead a massive amount of microscopic work.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;then Victoras said:
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev3">&gt;&gt;&gt;       How would behave a mixture of oil and water if Earth's field of
</em><br />
<em class="quotelev3">&gt;&gt;&gt;gravitation was absent - would it mix or separate ? Can this be described
</em><br />
<em class="quotelev3">&gt;&gt;&gt;by a conditional sentence like the one below ?  if (gravity
</em><br />
<em class="quotelev3">&gt;&gt;&gt;present){separation} else {mixing}  Then it seems like an algorithmic
</em><br />
<em class="quotelev3">&gt;&gt;&gt;process where entropy produces opposite results given different initial
</em><br />
<em class="quotelev3">&gt;&gt;&gt;conditions. Thousands, millions, mirriads of similar conditional branches
</em><br />
<em class="quotelev3">&gt;&gt;&gt;take place every moment everywhere in the Universe. Thence everything can
</em><br />
<em class="quotelev3">&gt;&gt;&gt;be described/modelled as causal chains of events. That's how our
</em><br />
<em class="quotelev3">&gt;&gt;&gt;(natural) and artificial inteligence (rule based or case
</em><br />
<em class="quotelev3">&gt;&gt;&gt;(experience) based reasoning) work. Just interesting parralels...
</em><br />
<em class="quotelev3">&gt;&gt;&gt;
</em><br />
<em class="quotelev3">&gt;&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;to which Shu-Kun replied:  They would separate.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;     SS: So, Victoras sees the energy gradient being used to power the work
</em><br />
<em class="quotelev2">&gt;&gt;of separating oil and water as being gravitational energy, while Shu-Kun
</em><br />
<em class="quotelev2">&gt;&gt;implies that it is instead some kind of chemical gradient.  Either way my
</em><br />
<em class="quotelev2">&gt;&gt;point is not being disputed.  Therefore, even though the energy gradient
</em><br />
<em class="quotelev2">&gt;&gt;dissipation view of entropy production is historically prior, and
</em><br />
<em class="quotelev2">&gt;&gt;empirically measurable, the increasing disorder view is not falsified by
</em><br />
<em class="quotelev2">&gt;&gt;this oil/water example in the way that some folks thought it was.
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;STAN
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<em class="quotelev2">&gt;&gt;
</em><br />
<p><p><p>_______________________________________________
<br />
fis mailing list
<br />
fis&#64;listas.unizar.es
<br />
<a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
<br />
<span id="received"><dfn>Received on</dfn> Mon Apr 19 14:07:10 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1439.html" title="Next message in the list">Loet Leydesdorff: "RE: [Fis]  Data, observations and distributions / was: Re: Probabilistic Entropy"</a></li>
<li><dfn>Previous message</dfn>: <a href="1437.html" title="Previous message in the list">Michel Petitjean: "[Fis]  Data, observations and distributions / was: Re: Probabilistic Entropy"</a></li>
<li><dfn>Maybe in reply to</dfn>: <a href="1400.html" title="Message to which this message replies">Michael Devereux: "[Fis] Szilard's Engine and Information"</a></li>
<!-- lnextthread="start" -->
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1438" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1438" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1438" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1438" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:46 CET
</em></small></p>
</body>
</html>
