<?xml version="1.0" encoding="windows-1252"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: [Fis] Physical Information is Shannon Entropy, Part V.</title>
<meta name="Author" content="Michael Devereux (dbar_x@cybermesa.com)" />
<meta name="Subject" content="[Fis] Physical Information is Shannon Entropy, Part V." />
<meta name="Date" content="2004-06-21" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>[Fis] Physical Information is Shannon Entropy, Part V.</h1>
<!-- received="Mon Jun 21 00:36:23 2004" -->
<!-- isoreceived="20040620223623" -->
<!-- sent="Sun, 20 Jun 2004 16:34:38 -0600" -->
<!-- isosent="20040620223438" -->
<!-- name="Michael Devereux" -->
<!-- email="dbar_x@cybermesa.com" -->
<!-- subject="[Fis] Physical Information is Shannon Entropy, Part V." -->
<!-- id="40D610FE.2000905@cybermesa.com" -->
<!-- charset="windows-1252" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1608.html" title="Viktoras Didziulis: &quot;Re: [Fis] teleology of entropy&quot;">Next message</a> ]
[ <a href="1606.html" title="Stanley N. Salthe: &quot;Re: [Fis] teleology of entropy&quot;">Previous message</a> ]
<!-- unextthread="start" -->
[ <a href="1613.html" title="Loet Leydesdorff: &quot;RE: [Fis] Physical Information is Shannon Entropy, Part V.&quot;">Next in thread</a> ]
 [ <a href="#replies">Replies</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Michael Devereux &lt;<a href="mailto:dbar_x&#64;cybermesa.com?Subject=Re:%20[Fis]%20Physical%20Information%20is%20Shannon%20Entropy,%20Part%20V.">dbar_x@cybermesa.com</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Mon 21 Jun 2004 - 00:34:38 CEST</span><br />
</address>
<p>
Dear Loet, Aleks and colleagues,
<br />
<p>Science is a very limited method of trying to understand anything. We 
<br />
scientists deal exclusively with those physical things that are 
<br />
observable and measurable. We only wish, Loet, that we could 
<br />
scientifically evaluate the meaning conveyed by different forms of 
<br />
communication. And, I think the most important issues we humans would 
<br />
like to understand are entirely beyond the scope of science.
<br />
<p>But science is uniquely precise and definitive about its laws and 
<br />
conclusions because its subject matter and methods are so carefully 
<br />
limited. One reason science is progressive, and need not continually 
<br />
reevaluate Newton’s Laws or Einstein’s Special Theory (within the range 
<br />
of their applicability), for example, is the exact, observable 
<br />
definitions we give for those properties we investigate.
<br />
<p>Notice that if it’s not physical, observable entropy, then that thing is 
<br />
not a subject for science. I don’t know any of the physicists, Aleks, 
<br />
who you say wish to distinguish Clausius' empirical entropy, Boltzman’s 
<br />
model entropy, and the Shannon logical entropy “which doesn’t concern 
<br />
itself with the empirical entropy.” If these are not all the same 
<br />
tangible, observable entropy, then physicists don’t include it in our 
<br />
theories and measurements.
<br />
<p>When scientists talk about energy, for example, they mean precisely that 
<br />
thing which is measured so carefully by the Bureau of Standards. 
<br />
Likewise, for the physical entropy that we scientists evaluate, and 
<br />
which is the subject of the Second Law of Thermodynamics. I think it’s 
<br />
true, Aleks, that we scientists would like to keep the term entropy (and 
<br />
energy, and mass, and force, etc.) just for ourselves, so that it only 
<br />
means something physical and measurable. But, that won’t happen, and 
<br />
hasn’t, in fact, happened. However, as scientists, we must always 
<br />
remember the distinction.
<br />
<p>You write, Aleks, “that information, ultimately is physical. But not 
<br />
Shannon information, as it is understood by the grand majority of 
<br />
practitioners.” I certainly agree that information is physical, and so a 
<br />
fit subject for science. And it may be perfectly correct that most 
<br />
researchers mean something non-physical when they say Shannon 
<br />
information. My continued argument is that we scientists mean only 
<br />
observable, physical information when we’re analyzing information 
<br />
scientifically (or we’ve forgotten the distinction). And that Shannon 
<br />
obviously meant physical information also, since that’s exactly what he 
<br />
wrote.
<br />
<p><p>Aleks, if “nobody is questioning the connection between S and H when we 
<br />
speak about physical models”, then the question must be whether 
<br />
Shannon’s entropy, H, depicts a physical model. If information is 
<br />
physical, as you say, then obviously, Shannon was describing a physical 
<br />
thing, and Shannon’s entropy, H, is the physical entropy, S.
<br />
<p>Here’s why it matters to scientists whether the Shannon information 
<br />
described by Shannon (and by Shannon and Weaver) is physical 
<br />
information. It’s not just a choice of words to describe physical 
<br />
information, but we also recognize that Shannon’s subject, his 
<br />
derivation, and his equation all described a physical system. So, we may 
<br />
properly use his model, his methods and his equation to describe our own 
<br />
physical systems.
<br />
<p>And, that equation is, indeed, appropriate in scientific work. 
<br />
Scientists have routinely and continuously used Shannon’s formula, and 
<br />
von Neumann’s formula and Boltzmann’s formula before that, to describe 
<br />
physical systems. What may be called a probabilistic equation, describes 
<br />
a physical system, or else Boltsmann, von Neumann, and every scientist 
<br />
in between, was mistaken in so applying it. (They weren’t.)
<br />
<p>I think Jaynes greatest contribution to a real understanding of the 
<br />
connection between information and physical entropy is his Principle of 
<br />
Maximum Entropy (reviewed by Grandy in his resource letter). In the 
<br />
article you cite on image reconstruction, Jaynes defines H to be “the 
<br />
maximum entropy per dot, H = log(W)/N....(which) goes asymptotically 
<br />
into the Shannon entropy -Sum (N-sub-i/N)log(N-sub-i/N).” It’s 
<br />
reasonable, in this context, to call H the maximum entropy because it 
<br />
becomes Shannon’s entropy in the limit. Boltzmann’s H is not the maximum 
<br />
entropy, in general, but rather, the system entropy which becomes 
<br />
maximum at equilibrium. (That’s what Boltzmann’s H theorem tells us.) 
<br />
Thus, the Sum P log (P) expression you mention, is not an approximation 
<br />
to thermodynamic entropy, but is actually thermodynamic entropy itself. 
<br />
That is Boltzmann’s H (and Shannon’s H), and both are approximated (in 
<br />
the limit) by the symbol H chosen by Jaynes here.
<br />
<p>The accepted distinction between physical entropy and probabilistic 
<br />
entropy would solve the problem, I believe, Loet, if we also recognize 
<br />
that Shannon’s equation (sometimes called probabilistic) is entirely 
<br />
appropriate for physical entropy as well as for the other analyses to 
<br />
which non-scientists usefully apply it. (These other analyses, I assume, 
<br />
Aleks, are the non-physical probabilistic models to which you refer.)
<br />
<p>When we scientists use Shannon information in our research, Aleks, it 
<br />
means, as Shannon and Weaver wrote, “the quantity which uniquely meets 
<br />
the natural requirements that one sets up for ‘information’ (and) turns 
<br />
out to be exactly that which is known in thermodynamics as entropy.” 
<br />
Though I can’t point to an example now, I’m sure, of course, that some 
<br />
scientist, at some time, has forgotten (or ignored) Shannon’s 
<br />
publication, and has misapplied the term ‘Shannon entropy’.
<br />
<p>You write Aleks, that you’ve “seen no indication that Shannon would mean 
<br />
what” I’ve said he means. Have you read the book by Shannon and Weaver? 
<br />
They were scientists writing about a technical subject for an audience 
<br />
of other sophisticated researchers. Authors, in such cases, try to be 
<br />
precise and clear about what they mean. And, the publication, of course, 
<br />
is designed to provide a permanent record of the author’s ideas.
<br />
<p>The english language has not changed so much in fifty-five years that 
<br />
one can reasonably misinterpret the lucid statement “the quantity which 
<br />
uniquely meets the natural requirements that one sets up for 
<br />
‘information’ turns out to be exactly that which is known in 
<br />
thermodynamics as entropy.” And, I continue to mean that Shannon’s 
<br />
information is the same thing as thermodynamic entropy. One doesn’t have 
<br />
to be a “theologist or literary critic” to recognize that Shannon and 
<br />
Weaver meant the same thing.
<br />
<p>My thanks to the FIS forum for this opportunity to defend these 
<br />
important ideas.
<br />
<p>Cordially,
<br />
<p>Michael Devereux
<br />
<p>_______________________________________________
<br />
fis mailing list
<br />
fis&#64;listas.unizar.es
<br />
<a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
<br />
<span id="received"><dfn>Received on</dfn> Mon Jun 21 00:36:23 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1608.html" title="Next message in the list">Viktoras Didziulis: "Re: [Fis] teleology of entropy"</a></li>
<li><dfn>Previous message</dfn>: <a href="1606.html" title="Previous message in the list">Stanley N. Salthe: "Re: [Fis] teleology of entropy"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="1613.html" title="Next message in this discussion thread">Loet Leydesdorff: "RE: [Fis] Physical Information is Shannon Entropy, Part V."</a></li>
<li><a name="replies" id="replies"></a>
<dfn>Reply</dfn>: <a href="1613.html" title="Message sent in reply to this message">Loet Leydesdorff: "RE: [Fis] Physical Information is Shannon Entropy, Part V."</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1607" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1607" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1607" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1607" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:47 CET
</em></small></p>
</body>
</html>
