<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: Re: [Fis]: Re: CONSILIENCE: When separate inductions jump t</title>
<meta name="Author" content="Stanley N. Salthe (ssalthe@binghamton.edu)" />
<meta name="Subject" content="Re: [Fis]: Re: CONSILIENCE: When separate inductions jump together" />
<meta name="Date" content="2004-09-26" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>Re: [Fis]: Re: CONSILIENCE: When separate inductions jump together</h1>
<!-- received="Sun Sep 26 22:38:00 2004" -->
<!-- isoreceived="20040926203800" -->
<!-- sent="Sun, 26 Sep 2004 16:43:40 -0500" -->
<!-- isosent="20040926214340" -->
<!-- name="Stanley N. Salthe" -->
<!-- email="ssalthe@binghamton.edu" -->
<!-- subject="Re: [Fis]: Re: CONSILIENCE: When separate inductions jump together" -->
<!-- id="l03130300bd7b90ff037f@[128.226.180.84]" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="200409241106.i8OB6un2385758&#64;ds10.itodys.jussieu.fr" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1690.html" title="Pedro C. Marijuán: &quot;RE: [Fis] CONSILIENCE: When separate inductions jump together&quot;">Next message</a> ]
[ <a href="1688.html" title="Loet Leydesdorff: &quot;[Fis] CONSILIENCE &amp; Interdisciplinarity&quot;">Previous message</a> ]
[ <a href="1686.html" title="Michel Petitjean: &quot;[Fis]: Re: CONSILIENCE: When separate inductions jump together&quot;">In reply to</a> ]
<!-- unextthread="start" -->
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Stanley N. Salthe &lt;<a href="mailto:ssalthe&#64;binghamton.edu?Subject=Re:%20[Fis]:%20Re:%20CONSILIENCE:%20When%20separate%20inductions%20jump%20together">ssalthe@binghamton.edu</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Sun 26 Sep 2004 - 23:43:40 CEST</span><br />
</address>
<p>
Michel said:
<br />
<p><em class="quotelev1">&gt;When Boltzmann proposed a model to compute the Clausius entropy,
</em><br />
<em class="quotelev1">&gt;he created a bridge between two worlds: the &quot;micro&quot; and the &quot;macro&quot;
</em><br />
<em class="quotelev1">&gt;Undoubtly this was a major victory of science. It linked also
</em><br />
<em class="quotelev1">&gt;two areas of science, relatively to the approximate classification
</em><br />
<em class="quotelev1">&gt;of disciplines in vigor at the end of the 19th century. Now the
</em><br />
<em class="quotelev1">&gt;intersection of the two disciplines is recognized as a full
</em><br />
<em class="quotelev1">&gt;subdiscipline: statistical mechanics (it's a fun that the name
</em><br />
<em class="quotelev1">&gt;evokes statistics and mechanics but not chemistry). The main
</em><br />
<em class="quotelev1">&gt;point here about entropy is that we refer to only one concept:
</em><br />
<em class="quotelev1">&gt;a measurable parameter, and its calculation from a physical model.
</em><br />
<em class="quotelev1">&gt;The situation is different after 1948, when Shannon picked
</em><br />
<em class="quotelev1">&gt;the entropy word and used it in communication sciences.
</em><br />
<em class="quotelev1">&gt;Despite the formal similarity in the equations, entropy
</em><br />
<em class="quotelev1">&gt;exists now as a mathematical concept outside any reference to
</em><br />
<em class="quotelev1">&gt;thermodynamics (except historically). E.g. you can compute
</em><br />
<em class="quotelev1">&gt;the entropy of some distribution you like, even if it makes
</em><br />
<em class="quotelev1">&gt;sense in a context far from thermodynamics or statistical
</em><br />
<em class="quotelev1">&gt;mechanics. Going back to the consilience viewed by Whewell,
</em><br />
<em class="quotelev1">&gt;or by Wilson, I would agree with you, Malcolm, that there is
</em><br />
<em class="quotelev1">&gt;a danger to count entropy as an example of consilience:
</em><br />
<em class="quotelev1">&gt;owing from what I have read about the term (but I am not an
</em><br />
<em class="quotelev1">&gt;expert), it seems abusive to exhibit an example of consilience
</em><br />
<em class="quotelev1">&gt;when a (meaningless) formal analogy is found somewhere.
</em><br />
<em class="quotelev1">&gt;Ihe historical motivation of Shannon to choose the term entropy
</em><br />
<em class="quotelev1">&gt;is neglected here: it adds confusion to the debate.
</em><br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Since Michel raised the entropy problem here once more, I will propose
<br />
a consilience concerning it by sharing a short paper of mine (General
<br />
Systems Bulletin 32:5-12, 2003), which gives my view on the evolution of
<br />
concepts as well.
<br />
<p>STAN
<br />
<p>Entropy: What does it really mean?
<br />
S.N. Salthe
<br />
<p>Analytical Preliminary
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Systematicity is a concept.  Concepts are systems.  In my view,
<br />
concepts are never fully circumscribed by explicit definitions.  As
<br />
different definitions of the same phenomenon accrue, it could even turn out
<br />
that the historically first attempt at one winds up a &quot;standard deviation&quot;
<br />
away from the &quot;intersection&quot; of all versions of a concept.  Some might say
<br />
that this shows that a concept evolves.  I follow Foucault (1972) in seeing
<br />
instead a gradual clarification of a concept that was only roughly
<br />
adumbrated at first.  That is, I see a development of its meaning, which
<br />
therefore must be fixed from the beginning in a system, or ecology, of
<br />
society-specific concepts.  Fixed, but vague.  The definitions we prefer in
<br />
science are supposed to be explicit and crisp, so that they may be made
<br />
operational when mediated by equations.  But each of these may be somewhat
<br />
of a falsification of the actual idea, which, if it gives rise to several
<br />
definitions, must be richer, and vaguer.  Here I will seek the full meaning
<br />
of entropy by examining its four major manifestations in modern science:
<br />
Carnot's entropie (Carnot 1824 / 1960); Boltzmann's most probable
<br />
condition, S, usually interpreted as disorder (Boltzmann, 1886 / 1974);
<br />
Prigogine's delta S, or change in entropy (Prigogine, 1955); and Shannon's
<br />
H, or information carrying capacity (Shannon &amp; Weaver, 1949).  The
<br />
&quot;intersection&quot; of these definitions would be close to the real meaning of
<br />
entropy -- but can it be stated explicitly in words?
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;First we examine:
<br />
(1) The Carnot-Clausius entropy -- entropie -- which could be defined as a
<br />
measure of the negefficiency of work.  Its production increases as the rate
<br />
of work, or power, increases above the rate at greatest energy efficiency
<br />
for the dissipative system involved.  It is commonly estimated in actual
<br />
cases by the heat energy transferred from an available gradient to the
<br />
environment as a result of doing work.  I believe that, more generally,
<br />
other waste products produced during consumption of an energy gradient
<br />
should be taken (since they are of lesser energy quality than the original
<br />
gradient) to be part of the entropy produced by the work, along with the
<br />
heat generated.  For an obvious example, sound waves emanating from some
<br />
effort or event are not likely to be able to be tapped as an energy
<br />
gradient before being dissipated.  Nor would the free energy emanating from
<br />
a natural open fire (here we have at least one exception -- the opening of
<br />
pine cones in trees adapted to fire disclimaxes).  So,I suggest that all
<br />
energy gone from a gradient, and not accounted for by what is considered to
<br />
be the work done by its consumer after some time period, should be taken as
<br />
entropic, if not as entropy itself, which is usually held to be represented
<br />
only by the completely disordered energy produced by frictions resisting
<br />
the work.  That is, any diminution of energy from a gradient during its
<br />
utilization for work that cannot be accounted for by that work would be
<br />
interpreted as part of its dissipation.  This approach seems to me to
<br />
follow from Boltzmann's interpretation of entropy as disorder (see below)
<br />
since the lesser gradients produced are relatively disordered from the
<br />
point of view of the consumer responsible for producing them (again, see
<br />
below).  This approach would also allow us in actual cases to use gradient
<br />
diminution as a stand in for the entropy produced, given that we know the
<br />
amount of energy that went into work (the exergy).
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This aspect of the entropy concept -- also called physical entropy --
<br />
has traditionally been constructed so as to refer exclusively to
<br />
equilibrium conditions in isolated systems.  Only in those conditions might
<br />
it be estimated, from the increase in ambient temperature after work has
<br />
been done.  Hence one hears that entropy is a state function defined only
<br />
for equilibrium conditions in isolated systems.  This stricture follows
<br />
from entropie's technical definition as a change in heat from one state to
<br />
another in an isolated system when the change in state is reversible (i.e.,
<br />
at equilibrium, irrespective of the path taken to make the change).  The
<br />
present approach takes this as an historico- pragmatic constraint on the
<br />
physical entropy concept making it applicable to engineering systems, and
<br />
therefore, from the point of view of seeking the broader meanings of the
<br />
concept, merely one attempt among others.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;One point to note about the engineering version is that entropy is
<br />
therein measured as a change in temperature divided by the average
<br />
temperature of the system prior to the new equilibrium.  This means that a
<br />
given production of heat in a warmer system would represent less entropy
<br />
increase than the same amount produced in a cooler system.  Thus, while
<br />
hotter systems are greater generators of entropy, cooler ones are more
<br />
receptive to its accumulation.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Since the production of entropy during work can theoretically be
<br />
reduced arbitrarily near to almost none by doing the work increasingly
<br />
slowly (up to the point where it becomes reversible), physical entropy is
<br />
the aspect of the entropy concept evoked by the observation that haste
<br />
makes waste.  Of course, it is also the aspect of the concept evoked by the
<br />
fact that all efforts of any kind in the material world meet resistance,
<br />
(which generates friction, which dissipates available energy as heat).
<br />
<p>(2) The Boltzmann (or statistical) entropy, S, which is generally taken to
<br />
be the variety of possible states, any one of which a system could be found
<br />
to occupy upon observing it at random.  This is often called the degree of
<br />
disorder of a system.  It is usually inferred to imply that, at
<br />
equilibrium, any state contributing to S in a system could be reached with
<br />
equal likelihood from any other state, but this is not the only
<br />
possibility, as it has been pointed out that the states might be governed
<br />
instead by a power law (Tsallis entropy).  In principle, S, a global state
<br />
of a system, spontaneously goes to a maximum in any isolated system, as
<br />
visualized in the physical process of free diffusion.  However, since it is
<br />
an extensive property, it may decline locally providing that a compensating
<br />
equal or greater increase occurs elsewhere in the system.  At global
<br />
equilibrium such local orderings would naturally occur as fluctuations, but
<br />
would be damped out sooner or later unless some energetic reinforcement
<br />
were applied to preserve them.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This aspect of the entropy concept establishes one of its key defining
<br />
features -- that, in an isolated system, if it changes it is extremely
<br />
likely to increase because, failing energy expenditure to maintain them, a
<br />
system's idiosyncratic, asymmetrical, historically acquired configurations
<br />
would spontaneously decay toward states more likely to be randomly
<br />
accessed.  This likelihood is so great that it is often held that S must
<br />
increase if it changes.  We may note in this context that, as a
<br />
consequence, heat should tend to flow from hotter to cooler areas (which it
<br />
does), tending toward an equilibrium lukewarmness.  This would be one
<br />
example of a general principle that energy gradients are unstable, or at
<br />
best, metastable, and will spontaneously dissipate as rapidly as possible
<br />
under most boundary conditions.  Entropie in the narrow(est)  sense would
<br />
be assessed by the heat produced during such dissipation when it is
<br />
assisted by, or harnessed to, some (reversible) work.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;But, then, will the negefficiency of work also spontaneously increase?
<br />
Or, under what circumstances must it increase?  With faster work; but also
<br />
with more working parts involved in the work.  Therefore, as a system
<br />
grows, it should tend to become less energy efficient, and so grow at
<br />
increasingly slower specific (per unit) rates.  This is born out by
<br />
observations on many kinds of systems (Aoki, 2001).  But then, must
<br />
dissipative systems grow?  It seems so (Salthe, 1993; Ulanowicz, 1997).
<br />
This is one mode by which they produce entropie, which they must do.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Boltzmann's interpretation of physical entropy as disorder assumes
<br />
major importance in the context of the Big Bang theory of cosmogony,
<br />
because that theory has the Universe producing, by way of its accelerating
<br />
expansion, extreme disequilibrium universally. This means among other
<br />
things, that there will be many energy gradients awaiting demolition, as
<br />
matter is the epitome of non-equilibrial energy.  This disequilibrium would
<br />
be the cause of the tendency for S to spontaneously increase (known as the
<br />
Second Law of thermodynamics), as by, e,g., wave front spreading and
<br />
diffusion.  Note again that the engineering model of physical entropy has
<br />
entropy increasing more readily (per unit of heat produced) in cooler
<br />
systems.  This suggests that the cooling of the universe accompanying its
<br />
expansion has increased the effectiveness of the Second Law even as the
<br />
system's overall entropy production must have diminished as it cooled.
<br />
Note also that one of the engineering limitations on the entropy concept
<br />
can be accommodated very nicely in this context by making the -- not
<br />
unreasonable, perhaps even necessary -- assumption that the universe is an
<br />
isolated system (but, of course, not at equilibrium, at least locally,
<br />
where we observers are situated).
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Statistical entropy, S, is the aspect of the entropy concept evoked by
<br />
the observation that, failing active preservation, things tend to fall
<br />
apart, or become disorderly, as well as the observation that warmth tends
<br />
to be lost unless regenerated (a point of importance to homeotherms like
<br />
us).
<br />
<p>&nbsp;(3) Next we consider the Prigogine (1955) change in entropy, delta S, of a
<br />
local system, or at some locale.  Delta S may be either positive or
<br />
negative, depending upon relations between S production within a system
<br />
(which, as the vicar of the Second Law in nonequilibrial systems, must
<br />
always be positive locally within an isolated system), and S flow through
<br />
the system.  In general, because of fluctuations, a local system would not
<br />
be equilibrated even within global equilibrium conditions.  So, if more
<br />
entropy flows out of a system than is produced therein because some is
<br />
incoming and flowing through as well, then delta S would be negative, and
<br />
the system would be becoming more orderly (negentropic), as when a
<br />
dissipative structure is self-organizing.  This can be the case when the
<br />
internal work done by a system (producing entropy during work while
<br />
consuming an energy gradient) results in associated decreases in other
<br />
embodied free energies (which decreases also produce entropy, but not from
<br />
the gradient powering the work), followed by most of the entropy flowing
<br />
out (as heat and waste products of lesser energy quality than the
<br />
precursors).  More generally, if entropy flows through a system, and some
<br />
is produced therein as well, then more would tend to flow out than was
<br />
produced internally (if the system remains intact), and so its entropy
<br />
change would be negative.  In this case, the system would be maintaining
<br />
itself despite taking in entropy (as, e.g., heat, buffeting, toxins and
<br />
free radicals), which could have disrupted it.  The work done internally
<br />
maintains the system.  Note that this gives us the simplest model of a
<br />
dissipative structure.  In it, a managed flow of entropy intruding from
<br />
outside the system is a necessary condition to allow the entropy produced
<br />
inside (during the work of entropy management), when added to that flowing
<br />
through, to promote self-organization.  That is, a flow through of entropy
<br />
is a necessary prerequisite for self -organization. This is crucial for
<br />
understanding the origin of life.  A proto living system must therefore be
<br />
located within a larger scale dissipative structure, driven by its flows.
<br />
When such a protosystem finds a way to temporarily manage impinging
<br />
disorder, it can self-organize into a system that, by producing its own
<br />
disorder from available gradients during internal work, comes to be able to
<br />
more effectively manage impinging disorder by way of the forms produced by
<br />
that internal work.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Delta S is an extensive property of a given locale or subsystem within
<br />
a system wherein, globally, S must increase.  I take much of delta S to
<br />
represent local changes in internal free energy, but it could also signal
<br />
changes in system / energy gradient negefficiency by way of configurational
<br />
alterations in the work space, or, put in a more Prigoginean way, changes
<br />
in energy flows relative to the forces focused from the energy gradients
<br />
being consumed.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The necessity for positive local entropy production in a dissipative
<br />
structure is the aspect of the entropy concept evoked by the observation
<br />
that one has to keep &quot;running&quot; (producing entropy internally and shipping
<br />
it out) in order just to maintain oneself as is.  Or, that making and
<br />
maintaining a system is always taxed by having to expend some available
<br />
energy on entropy -- which, of course connects to the observation, relative
<br />
to entropie, that all efforts meet resistance.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It has been pointed out to me that the units of delta S are different
<br />
from those of entropie.  Here is a good place to remind the reader that I
<br />
am seeking vaguer definitions.  I seek a concept that would define both a
<br />
substance and its changes, being vaguer than both, as in {basic concept
<br />
{substance {changes in substance}}}.
<br />
<p>(4)  Finally there is the Shannon, or informational, entropy, H, which is
<br />
the potential variety of messages that a system might mediate (its capacity
<br />
for generating variety), or more generally, the uncertainty of system
<br />
behavior.  The exact mathematical form of H is identical to that of S, but
<br />
with physical constraints removed, thereby generalizing it.  Thus, {H {S
<br />
}}.  That is, physical entropy, when interpreted statistically as disorder
<br />
(S), is a special kind of entropy compared to informational entropy.
<br />
Disorder in the physical sense is just the variety of configurations a
<br />
system can be observed to take over some standard time period, or {variety
<br />
{disorder }} = {variety per se {variety of accessed physical
<br />
configurations}}.  Note that one of the added constraints at the enclosed
<br />
level here is that disorder will spontaneously increase, so we might better
<br />
have written 'disordering' as in {variety {disorder {disordering }}.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It is often asserted that physical entropy in the negefficiency sense
<br />
(entropie) has nothing whatever to do with H.  In my view, negefficiency
<br />
measures the lack of fittingness between an energy gradient and its
<br />
consumer, and therefore must be informational in character.  That is, as
<br />
negefficiency increases, the lack of information about its gradient in a
<br />
consumer is increasing, and therefore the unreliability of their 'fit' must
<br />
increase.  This means that as a gradient is dissipated increasingly
<br />
rapidly, the information a system has concerning its use to do work,
<br />
embodied in its forms and/or behavior, becomes increasingly uncertain in
<br />
effect.  Since natural selection is working on systems most intensely when
<br />
they are pushing the envelope (Salthe, 1975), efficiency at faster rates
<br />
could evolve via selection in more complex systems, modifying the
<br />
information a system has concerning its gradients.  But, at whatever rate a
<br />
system establishes its maximum efficiency, (a) work rates faster than that
<br />
will still become increasingly inefficient, and (b) now slower rates (away
<br />
from the reversible range, of course) would be less efficient than the
<br />
evolved optimum rate.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the Negentropy Principle of Information (NPI) of Brillouin (1956),
<br />
information, (I), is just any limitation on H,  (with I  + H = k), and is
<br />
generated by a reduction in degrees of freedom, or by symmetry breaking.
<br />
On the other hand, H is also generated out of information, most simply by
<br />
permutation of behavioral interactions informed by an array of fixed
<br />
informational constraints.  I take information to be any constraint on the
<br />
behavior of a system, leaving a degree of uncertainty (H) to quantitatively
<br />
characterize the remaining capacity of the system for generating behavior,
<br />
functional as well as pathological (characteristic as well as unusual).
<br />
So, H and I seem to be aspects of some single &quot;substance&quot;.  I suggest that
<br />
this &quot;substance&quot; is configuration, which would be either static (as part of
<br />
I) or in motion, which, in turn, would be either predictable (assimilable
<br />
to I) or not (as H).
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A point of confusion between H and S should be dealt with here. That
<br />
is that one must keep in mind the scalar level where the H is imputed to be
<br />
located.  So, if we refer to the biological diversity of an ecosystems as
<br />
entropic, with an H of a certain value, we are not dealing with the fact
<br />
that the units used to calculate the entropy -- populations of organisms --
<br />
are themselves quite negentropic when viewed from a different level and/or
<br />
perspective.  More classically, the molecules of a gas dispersing as
<br />
described by Boltzmann are themselves quite orderly, and that order is not
<br />
supposed to break down as the collection moves toward equilibrium (of
<br />
course, this might occur as well in some kinds of more interesting
<br />
systems).
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It must be noted that, as a bonafide entropy should, H necessarily
<br />
tends to increase within any expanding (e.g., the Big Bang) or growing
<br />
system (Brooks and Wiley, 1988), as well as, (Salthe, 1990), in the
<br />
environment of any active local system capable of learning about its
<br />
environment, including its energy gradients.  The variety of an expanding
<br />
system's behavior (H) tends to increase toward the maximum capacity for
<br />
supporting variety of that kind of system. This may be compared to S, which
<br />
tends to increase in any isolated system.  I assume the Universe is both
<br />
isolated and expanding, and therefore S and H both must tend to increase
<br />
within it.  H materially increases in the Universe basically because, as it
<br />
expands and cools at accelerating rate, some of the increasingly
<br />
nonequilibrium energy precipitates as matter.  Note that, with information,
<br />
(I),  tending to produce S by way of mediating work (no work without an
<br />
informed system to do it), it does this only by having diminished H, (I = k
<br />
- H), so that, when a system does internal work it is, while becoming more
<br />
orderly, shedding some of its own internal informational entropy (H) for
<br />
externalized physical entropy (S).
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Informational entropy is the aspect of the entropy concept evoked by
<br />
the observation that the behavior of more complicated (information rich)
<br />
systems (say, a horse) tends to be more uncertain than that of simpler
<br />
systems (say, a tractor).  This is so, generally, even if a more
<br />
complicated system functions with its number of states reduced by
<br />
informational constraints, because one needs to add in unusual and even
<br />
pathological states to get all of system H, and these should have increased
<br />
as the number of  functional states was restricted by imposed information.
<br />
With its habitual behavioral capacity reduced, a system could appear more
<br />
orderly to an observer, who could obtain more information about it.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As a coda on informational entropy, we should consider whether
<br />
Kolmogorov complexity (Kolmogorov,1975) is an entropy.  Here, a string of
<br />
tokens is complex if it cannot be generated by a simpler string.  As it
<br />
happens, sequences that cannot be reduced to smaller ones are random
<br />
mathematically.  More generally, something that cannot be represented by
<br />
less than a complete duplicate of itself is random / complex.  But is it
<br />
&quot;entropic&quot;?  Unless it tended irreversibly to replace more ordered
<br />
configurations, I would say 'no'.  Not everything reckoned random can be an
<br />
entropy, even though entropic configurations tend to be randomized.  The
<br />
entropy concept is best restricted to randomization that must increase
<br />
under at least some conditions.
<br />
<p>Preliminary Analysis
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Now, is the unpredictability of complicated systems related to the
<br />
fact that one needs to keep running just to stay intact, and are these
<br />
related to the fact that things have a tendency to disintegrate, and are
<br />
these in turn related to the observations (a) that all efforts meet
<br />
resistance, and (b) that haste makes waste?
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Put otherwise (and going the other way), is negefficiency related to
<br />
disorder and is that related to variety?  The question put this way was
<br />
answered briefly in the affirmative above (in 4).
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;So, we have imperfect links between consumers and energy gradients
<br />
mapped to disorder.  Of course, the disorder in question is then,
<br />
&quot;subjective&quot;, in that it can characterize energy gradients only with
<br />
respect to particular consumers.  Ant eaters can process ants nicely, but
<br />
rabbits cannot;  steam engines can work with the energy freed by a focused
<br />
fire, but elephants cannot.  More generally, however, no fit between
<br />
gradient and consumer can be perfect.  Unless work is being done so slowly
<br />
as to be reversible, it is taxed by entropy production.  As well, when the
<br />
work in question gets hastier and hastier -- and most animals are fast food
<br />
processors! -- consumption becomes increasingly deranged, meeting greater
<br />
and greater resistance.  And, of course, abiotically, the dissipative
<br />
structures that get most of a gradient are those that degrade it fastest.
<br />
Because these have minimal form to preserve, they are closer to being able
<br />
to maximize their rate of entropy production than are living things, which,
<br />
hasty withal, still need to preserve their complicated forms. Haste is
<br />
imposed upon living systems by the acute need to heal and recover from
<br />
perturbations as quickly as possible. They are also in competition for
<br />
gradient with others of their own and other kinds, and, as Darwinians tell
<br />
us, they are as well in reproductive competition with their conspecifics.
<br />
Abiotic dissipative structures can almost maximize their entropy
<br />
production, biotics can do so only to an extent allowed by their more
<br />
complicated embodiments (of course, all systems can maximize only to an
<br />
extent governed by bearing constraints).  In any case, I conclude that
<br />
disorder causes physical entropy production as well as being, as S, an
<br />
indicator of the amount produced.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The disorder of negefficiency is configurational (even if the
<br />
configurations are behavioral and fleeting).  And so is Boltzmann's S.
<br />
Negefficiency is generated by friction when interacting configurations are
<br />
not perfectly fitted, while S is generated by random walks.  An orderly
<br />
walk would have some pattern to it, and, importantly, could then be
<br />
harnessed to do work.  Random walks accomplish nothing, reversing direction
<br />
as often as not.  The link between physical entropy and S is that the
<br />
former is generated to the degree that the link between energy gradient and
<br />
consumer is contaminated by unanticipated irregularities -- by disorder --
<br />
causing resistance to the work being done.  Conventionally, S is held to
<br />
just indicate the degree of randomness of scattered particles -- a result
<br />
of negefficiency.  My point is that randomness can be viewed also as the
<br />
source of the friction which scatters the particles, as caused by
<br />
disorderliness in the configurations of gradient-consumer links.  I
<br />
conclude that physical entropy is easily mapped to disorder, both as
<br />
Boltzmann saw long ago, and in a generative sense as well.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;An important part of Boltzmann's formulation is that S will increase
<br />
if it changes -- that is, that disorder will increase -- unless energies
<br />
are spent preventing that.  While not conventionally thought of in this
<br />
way, this is highly likely at all scales.  No matter what order is
<br />
constructed or discerned by what subjectivity, it will become disordered
<br />
eventually, the rate being tailored to the scale of change (this fact is a
<br />
major corroboration that the universe we are in must be out of
<br />
equilibrium).  A sugar cube will become diffused throughout a glass of
<br />
water in a period of weeks, while, absent warfare, a building will
<br />
gradually collapse over a period of decades, and a forest will gradually
<br />
fail to regenerate over a period of eons.  And so we can indeed match the
<br />
notion that things tend to fall apart unless preserved with the
<br />
negefficient notion that all efforts at preservation meet resistance, which
<br />
is in turn linked to haste makes waste.  And the necessary falling apart
<br />
is, of course, easily related to the notion embodied in Prigogine's delta
<br />
S, that one must keep running just to stay put.  And, of course the harder
<br />
one runs, the less efficient is the running!  Furthermore, the more
<br />
complicated a running system is, the more likely some of its joints will
<br />
fluctuate toward being more frictional at any given moment.  Furthermore,
<br />
because of growth, all systems tend to become more complicated
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The inability to work very efficiently at significant loadings makes
<br />
our connection to H, informational entropy.  I suggest, as above, that
<br />
inefficiency is fundamentally a problem of lack of mutual information
<br />
between consumer and energy gradient, a problem that increases with hasty
<br />
work as well as with more working parts.  It should be noted again that the
<br />
acquisition of information by carving it out of possibilities is itself
<br />
taxed by the same necessary inefficiency.  The prior possibilities are the
<br />
embodiment of H, which can be temporarily reduced by the acquisition of
<br />
information.  As possibilities are reduced, system informational entropy is
<br />
exchanged for discarded physical entropy by way of the work involved.
<br />
Learning (of whatever kind in whatever system) is subject to the same
<br />
negefficiency that restrains all work.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;So, we need a word that signifies a general lack of efficiency,
<br />
increasing disorder, and the need to be continually active, with
<br />
fluctuating uncertainty.  Some word that means at the same time increasing
<br />
difficulty, messiness, uncertainty (confusion) and, yes, weariness! That
<br />
word is 'entropy'.  And I would guess the concept it stands for has still
<br />
not been fully excavated.
<br />
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I thank John Collier and Bob Ulanowicz for helpful comments on the text.
<br />
<p>References
<br />
Aoki, I.  2001.  Entropy and the exergy principles in living systems. in
<br />
S.E. Jørgensen,
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;editor. Thermodynamics and Ecological Modelling.  Lewis Publishers,
<br />
Boca Raton,
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Florida, USA.
<br />
Boltzmann, L. 1886/1974.  The second law of thermodynamics.  Reprinted in
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B. McGuinness, editor. Ludwig Boltzmann, Theoretical Physics and
<br />
Philosophical
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Problems. D. Reidel, Dordrecht.
<br />
Brillouin, L. 1962.  Science and Information Theory.  Academic Press, New York.
<br />
Brooks, D.R. and E.O. Wiley, 1986.  Evolution As Entropy: Toward a Unified
<br />
Theory of
<br />
&nbsp;&nbsp;&nbsp;&nbsp;Biology.  University of Chicago Press, Chicago.
<br />
Carnot, S. 1824 / 1960. Reflections on the motive power of fire, and on
<br />
machines fitted
<br />
&nbsp;&nbsp;&nbsp;&nbsp;to develop that power.  Reprinted in E. Mendoza, editor.  Reflections
<br />
on the Motive
<br />
&nbsp;&nbsp;&nbsp;&nbsp;Power of Fire and Other Papers.  Dover Publications, New York.
<br />
Foucault, M. 1972.  The Archaeology of Knowledge.  Tavistock Publications,
<br />
London.
<br />
Kolmogorov, A.N. 1975.  Three approaches to the quantitative definition of
<br />
information.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;Problems of Information Transmission 1: 1-7.
<br />
Prigogine, I. 1955. Introduction to Thermodynamics of Irreversible Processes.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;Interscience Publishers, New York.
<br />
Salthe, S.N., 1975. Problems of macroevolution (molecular evolution, phenotype
<br />
&nbsp;&nbsp;&nbsp;&nbsp;definition, and canalization) as seen from a hierarchical viewpoint.
<br />
American
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Zoologist 15: 295-314.
<br />
Salthe, S.N., 1990.  Sketch of a logical demonstration that the global
<br />
information
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;capacity of a macroscopic system must behave entropically when viewed
<br />
internally.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Journal of Ideas 1: 51-56.
<br />
Salthe, S.N., 1993. Development and Evolution: Complexity and Change in
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Biology.  MIT Press, Cambridge, MA.
<br />
Shannon, C. E., and  W. Weaver. 1949. The Mathematical Theory of Communication.
<br />
&nbsp;&nbsp;&nbsp;&nbsp;University of Illinois Press, Urbana, Illinois.
<br />
Ulanowicz, R.E., 1997.  Ecology, The Ascendent Perspective.  Columbia
<br />
University
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Press, New York.
<br />
<p><p><p><p><p>_______________________________________________
<br />
fis mailing list
<br />
fis&#64;listas.unizar.es
<br />
<a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
<br />
<span id="received"><dfn>Received on</dfn> Sun Sep 26 22:38:00 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1690.html" title="Next message in the list">Pedro C. Marijuán: "RE: [Fis] CONSILIENCE: When separate inductions jump together"</a></li>
<li><dfn>Previous message</dfn>: <a href="1688.html" title="Previous message in the list">Loet Leydesdorff: "[Fis] CONSILIENCE &amp; Interdisciplinarity"</a></li>
<li><dfn>In reply to</dfn>: <a href="1686.html" title="Message to which this message replies">Michel Petitjean: "[Fis]: Re: CONSILIENCE: When separate inductions jump together"</a></li>
<!-- lnextthread="start" -->
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1689" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1689" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1689" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1689" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:47 CET
</em></small></p>
</body>
</html>
