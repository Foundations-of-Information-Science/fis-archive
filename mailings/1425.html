<?xml version="1.0" encoding="us-ascii"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: RE: [Fis] entropy as average expected loss</title>
<meta name="Author" content="Loet Leydesdorff (loet@leydesdorff.net)" />
<meta name="Subject" content="RE: [Fis] entropy as average expected loss" />
<meta name="Date" content="2004-04-14" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>RE: [Fis] entropy as average expected loss</h1>
<!-- received="Wed Apr 14 03:14:33 2004" -->
<!-- isoreceived="20040414011433" -->
<!-- sent="Wed, 14 Apr 2004 03:12:55 +0200" -->
<!-- isosent="20040414011255" -->
<!-- name="Loet Leydesdorff" -->
<!-- email="loet@leydesdorff.net" -->
<!-- subject="RE: [Fis] entropy as average expected loss" -->
<!-- id="003101c421bd$9f65c900$1302a8c0@loet" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="20040413194826.6E3F07DA5&#64;mail.fri.uni-lj.si" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1426.html" title="jakulin@acm.org: &quot;[Fis] A decision-theoretic view of entropy and information&quot;">Next message</a> ]
[ <a href="1424.html" title="jakulin@acm.org: &quot;[Fis] entropy as average expected loss&quot;">Previous message</a> ]
[ <a href="1424.html" title="jakulin@acm.org: &quot;[Fis] entropy as average expected loss&quot;">In reply to</a> ]
<!-- unextthread="start" -->
[ <a href="1426.html" title="jakulin@acm.org: &quot;[Fis] A decision-theoretic view of entropy and information&quot;">Next in thread</a> ]
 [ <a href="#replies">Replies</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Loet Leydesdorff &lt;<a href="mailto:loet&#64;leydesdorff.net?Subject=RE:%20[Fis]%20entropy%20as%20average%20expected%20loss">loet@leydesdorff.net</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Wed 14 Apr 2004 - 03:12:55 CEST</span><br />
</address>
<p>
<em class="quotelev1">&gt; Clausius' (thermodynamic) entropy is very different from
</em><br />
<em class="quotelev1">&gt; Shannon's entropy. Thermodynamic entropy is a narrow concept.
</em><br />
<em class="quotelev1">&gt; Thermodynamic entropy is something one can measure. Shannon's
</em><br />
<em class="quotelev1">&gt; entropy is instead a general one. It is a property of a
</em><br />
<em class="quotelev1">&gt; probabilistic model. Boltzmann's work was trying to explain
</em><br />
<em class="quotelev1">&gt; phenomenological entropy with a probabilistic model.
</em><br />
<p>Yes, the Shannon entropy is the more interesting part and let me explain
<br />
why. I derived in my email of last Sunday the equation:
<br />
<p>S = k(B) N H
<br />
<p>H (the Shannon entropy) is a measure for the dividedness (or order if
<br />
one adds a minus sign). It is a mathematical property of the system. N
<br />
and k(B) are constants that serve to relate the dividedness to the size
<br />
of a physical system (N) and the dimensionality of the thermodynamic
<br />
entropy k(B) (measured in J/K), respectively. These two constants make
<br />
the mathematical (statistical) entropy to a physical entropy.
<br />
<p>The Shannon entropy is more general because it can be applied to systems
<br />
other than the physical one. It abstracts from the dimensionality of the
<br />
thermodynamic system (J/K) and from the size (N) and provides us with a
<br />
general measure of order. Before we can apply this measure of order,
<br />
however, we need a system of reference, just like we chose the physical
<br />
system as the system of reference by introducing N and k(B). The choice
<br />
of the system of reference provides the measurement of the information
<br />
(using the Shannon formulas) with meaning. For example, if we choose the
<br />
physical system of reference and use S instead of H, we provide the
<br />
concepts with meaning using the discourse of physics. We can then
<br />
measure the thermodynamic entropy; the factor k(B) warrants that the
<br />
macroscopic entropy (Clausius-Clappeyron) is equal to the one in
<br />
statistical mechanics. The discourse of physics is thus made consistent
<br />
by the Boltzmann constant.
<br />
<p>When one applies the formalisms of Shannon's entropy to other systems of
<br />
reference, these systems of reference provide substance to the otherwise
<br />
only formal equations. For example, one can apply the formalisms to the
<br />
circulation of money in an economy. The particles are then not physical
<br />
particles, but quantities of money. One may wish to measure quantities
<br />
of money, for example, in US dollars or Euros and then one needs to
<br />
introduce other constants for providing the equations with a substantive
<br />
interpretation. Similarly, we can use the measure to indicate the
<br />
dividedness in the school system (e.g., segregation) and then because of
<br />
its marvellous mathematical richness the formalisms (of statistical
<br />
decomposition analysis) allow us to disaggregate the dividedness
<br />
precisely in terms of classes in the school, or in terms of regions,
<br />
etc. The mathematical richness is available because Shannon chose H and
<br />
the algorithms of physics can thus be used as a heuristic for the
<br />
computation in other systems (without having to be derived anew). 
<br />
<p>For example, one can &quot;freeze&quot; the system and thus determine the number
<br />
of heterogeneous circulations that interact in the system under study.
<br />
In a previous email I already mentioned this application with a
<br />
reference to Smolensky (1986). For example, in automatic reading one is
<br />
able to construct a so-called Boltzmann machine which is able to read in
<br />
terms of (e.g.) 26 distinct characters because 26 circulations are
<br />
orthogonal in this system (as attractors). The orthogonal dimensions
<br />
co-vary at temperatures higher than &quot;zero degrees K&quot; and this
<br />
interaction can be measured by using the mutual information. I placed
<br />
quotation marks around &quot;zero degrees K&quot; because we would still have to
<br />
specify the equivalent to this physical notion in terms of the specific
<br />
system(s) under study. This specification requires a substantive
<br />
discourse or equivalently a specific theory of communication about the
<br />
system under study (a hypothesis).
<br />
<p>Thus, Shannon's choice to equate H with the notion of entropy was very
<br />
fortunate. It made the rich domain of equations and algorithms that had
<br />
been studied in the century before available to the study of the
<br />
dividedness/order/organization of systems other than physics. All these
<br />
forms of organization (and self-organization) can be studied in terms of
<br />
their probabilistic entropy. The formal measurement remains in bits of
<br />
information, but the substantive interpretation requires the
<br />
specification of the substance which is distributed (circulating) in the
<br />
system under study. Each specific substance circulating provides us with
<br />
a special theory of communication (a hypothesis), while the information
<br />
calculus provides us with a mathematical formalism to study these
<br />
processes of communication. From this perspective, physics can be
<br />
considered as the special theory of communication in which particles are
<br />
assumed to be circulating and colliding in terms of their (conserved)
<br />
energy and momenta. The formalism then enables us to compute the
<br />
dissipation in these otherwise conservative systems. Other systems
<br />
(e.g., living systems) may be non-conservative by their very nature. The
<br />
substantive theories can be expected to be different. They form the
<br />
monads which are connected by the formalisms that provide us with a
<br />
heuristics.
<br />
<p>With kind regards, 
<br />
<p>Loet
<br />
<p>&nbsp;&nbsp;_____  
<br />
<p>Loet Leydesdorff 
<br />
Amsterdam School of Communications Research (ASCoR)
<br />
Kloveniersburgwal 48, 1012 CX Amsterdam
<br />
Tel.: +31-20- 525 6598; fax: +31-20- 525 3681 
<br />
&nbsp;&lt;mailto:loet&#64;leydesdorff.net&gt; loet&#64;leydesdorff.net ;
<br />
&lt;<a href="../../www.leydesdorff.net/default.htm">http://www.leydesdorff.net/</a>&gt; <a href="../../www.leydesdorff.net/default.htm">http://www.leydesdorff.net/</a> 
<br />
&nbsp;
<br />
&nbsp;&lt;<a href="../../www.upublish.com/books/leydesdorff-sci.htm">http://www.upublish.com/books/leydesdorff-sci.htm</a>&gt; The Challenge of
<br />
Scientometrics ;  &lt;<a href="../../www.upublish.com/books/leydesdorff.htm">http://www.upublish.com/books/leydesdorff.htm</a>&gt; The
<br />
Self-Organization of the Knowledge-Based Society
<br />
<span id="received"><dfn>Received on</dfn> Wed Apr 14 03:14:33 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1426.html" title="Next message in the list">jakulin@acm.org: "[Fis] A decision-theoretic view of entropy and information"</a></li>
<li><dfn>Previous message</dfn>: <a href="1424.html" title="Previous message in the list">jakulin@acm.org: "[Fis] entropy as average expected loss"</a></li>
<li><dfn>In reply to</dfn>: <a href="1424.html" title="Message to which this message replies">jakulin@acm.org: "[Fis] entropy as average expected loss"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="1426.html" title="Next message in this discussion thread">jakulin@acm.org: "[Fis] A decision-theoretic view of entropy and information"</a></li>
<li><a name="replies" id="replies"></a>
<dfn>Reply</dfn>: <a href="1426.html" title="Message sent in reply to this message">jakulin@acm.org: "[Fis] A decision-theoretic view of entropy and information"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1425" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1425" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1425" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1425" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:46 CET
</em></small></p>
</body>
</html>
