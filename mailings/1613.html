<?xml version="1.0" encoding="us-ascii"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: RE: [Fis] Physical Information is Shannon Entropy, Part V.</title>
<meta name="Author" content="Loet Leydesdorff (loet@leydesdorff.net)" />
<meta name="Subject" content="RE: [Fis] Physical Information is Shannon Entropy, Part V." />
<meta name="Date" content="2004-06-28" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>RE: [Fis] Physical Information is Shannon Entropy, Part V.</h1>
<!-- received="Mon Jun 28 05:58:53 2004" -->
<!-- isoreceived="20040628035853" -->
<!-- sent="Mon, 28 Jun 2004 05:57:07 +0200" -->
<!-- isosent="20040628035707" -->
<!-- name="Loet Leydesdorff" -->
<!-- email="loet@leydesdorff.net" -->
<!-- subject="RE: [Fis] Physical Information is Shannon Entropy, Part V." -->
<!-- id="001201c45cc3$fea375c0$1c02a8c0@laptop" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="40D610FE.2000905&#64;cybermesa.com" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1614.html" title="hoelzer@unr.edu: &quot;Re: [Fis] 2004 FIS session: concluding comments&quot;">Next message</a> ]
[ <a href="1612.html" title="Stanley N. Salthe: &quot;Re: [Fis] 2004 FIS session: concluding comments&quot;">Previous message</a> ]
[ <a href="1607.html" title="Michael Devereux: &quot;[Fis] Physical Information is Shannon Entropy, Part V.&quot;">In reply to</a> ]
<!-- unextthread="start" -->
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Loet Leydesdorff &lt;<a href="mailto:loet&#64;leydesdorff.net?Subject=RE:%20[Fis]%20Physical%20Information%20is%20Shannon%20Entropy,%20Part%20V.">loet@leydesdorff.net</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Mon 28 Jun 2004 - 05:57:07 CEST</span><br />
</address>
<p>
<em class="quotelev1">&gt;Dear Loet, Aleks and colleagues,
</em><br />
<p><em class="quotelev1">&gt;Science is a very limited method of trying to understand anything. We
</em><br />
scientists deal exclusively with those physical things that are
<br />
observable and measurable. We only wish, Loet, that we could
<br />
scientifically evaluate the meaning conveyed by different forms of
<br />
communication. And, I think the most important issues we humans would
<br />
like to understand are entirely beyond the scope of science.
<br />
<p>Dear Michael, 
<br />
<p>Perhaps, you too easily equate science with physics. The &quot;unity of
<br />
science&quot; idea has been advocated in the early 20th century by many
<br />
natural sciences (e.g., the Vienna circle), but this idea cannot be
<br />
substantiated. The social sciences, for example, do not study physical
<br />
things, but the meaning of these physical things, for example, in terms
<br />
of their value (economics). The humanities use other models and even
<br />
upon closer inspection, the biological sciences do not always study
<br />
physical &quot;things&quot;. 
<br />
<p>The beauty of the mathematical theory of communication (Shannon, 1948)
<br />
is that it precedes the reference to a system that is defined
<br />
theoretically. We can use it for any system that is distributed. This
<br />
distribution can be in terms of the particles that are studied in
<br />
physics, but it does not have to be so. Thus, we first have to ask &quot;what
<br />
is distributed&quot; in a specific science or, more easily, what is
<br />
communicated when this substance is communicated (redistributed). A
<br />
system like the economy, for example, communicates in terms of economic
<br />
transactions. However, if one counts the flows of banknotes and coins as
<br />
physical things (that is, without attention to their economic value) one
<br />
misses the point. The specification of the system of reference is the
<br />
first theoretical task. 
<br />
<p>It is theoretical because one has to specify &quot;what is communicated when
<br />
the system operates&quot; first and thereafter one can ask the question how
<br />
one can measure this communication.  If the hypothesized substance is
<br />
communicated, how can this communication then be indicated. In the
<br />
economic example, it may then for example be useful not to study the
<br />
fluxes of banknotes and coins, but the value of the accounts of the
<br />
communicators. These are just numbers. But if the numbers are correctly
<br />
specified, we can use the change in these numbers as input to our
<br />
calculations and use the mathematical theory of communication as a
<br />
method (that is, one among possible methods). 
<br />
<p>The arguments for using the mathematical theory of communication as a
<br />
method for studying communication systems can be many. For example, this
<br />
operationalization enables us to study both the complexity of the system
<br />
under study and its dynamics in a single framework. Moreover, it allows
<br />
us to import formalisms from other sciences like physics because of the
<br />
strict analogy--as you correctly point out--between Shannon's H and
<br />
Boltzmann's H. For example, in the study of complex systems we can
<br />
&quot;freeze&quot; the system (Smolensky, 1986). However, the concepts have to be
<br />
specified in the other context in which the notions from physics are
<br />
then provided with meaning. 
<br />
<p>The sciences develop as discursive systems of rationalized expectations
<br />
side-by-side. Physics is one of them. Each science provides a specific
<br />
meaning to the things under study. While some sciences share the
<br />
calculus, it seems that there are no differences among those sciences,
<br />
but they can be found upon more careful inspection. The beauty of the
<br />
mathematical theory of communication is it more general applicability to
<br />
other sciences than the traditional ones. For example, one can for the
<br />
first time begin to study how meaning is communicated, for example,
<br />
among physicists (in the sociology of science). Weaver (1949) has been
<br />
the first to see this possibility in the second part of &quot;The
<br />
Mathematical Theory of Communication&quot;. 
<br />
<p>Each science studies a system which communicates what it communicates
<br />
(e.g., energy and momenta). One can expect this communication to
<br />
dissipate probabilistic entropy, but it can be useful to make
<br />
assumptions about what is conserved in the communication. When it is
<br />
properly specified what is communicated and how the constraints of the
<br />
conservation are operating, one can use the mathematical theory of
<br />
communication as a calculus. This formal framework facilitates the
<br />
comparison among the non-linear dynamics in different systems of
<br />
communication. 
<br />
<p>With kind regards,
<br />
<p>Loet
<br />
<p>&nbsp;&nbsp;_____  
<br />
<p>Loet Leydesdorff
<br />
Science &amp; Technology Dynamics, University of Amsterdam
<br />
Amsterdam School of Communications Research (ASCoR)
<br />
Kloveniersburgwal 48, 1012 CX  Amsterdam
<br />
Tel.: +31-20-525 6598; fax: +31-20-525 3681
<br />
&nbsp;&lt;mailto:loet&#64;leydesdorff.net&gt; loet&#64;leydesdorff.net;
<br />
&lt;<a href="../../www.leydesdorff.net/default.htm">http://www.leydesdorff.net/</a>&gt; <a href="../../www.leydesdorff.net/default.htm">http://www.leydesdorff.net</a> 
<br />
<p>. 
<br />
<p>But science is uniquely precise and definitive about its laws and
<br />
conclusions because its subject matter and methods are so carefully
<br />
limited. One reason science is progressive, and need not continually
<br />
reevaluate Newton's Laws or Einstein's Special Theory (within the range
<br />
of their applicability), for example, is the exact, observable
<br />
definitions we give for those properties we investigate.
<br />
<p>Notice that if it's not physical, observable entropy, then that thing is
<br />
not a subject for science. I don't know any of the physicists, Aleks,
<br />
who you say wish to distinguish Clausius' empirical entropy, Boltzman's
<br />
model entropy, and the Shannon logical entropy &quot;which doesn't concern
<br />
itself with the empirical entropy.&quot; If these are not all the same
<br />
tangible, observable entropy, then physicists don't include it in our
<br />
theories and measurements.
<br />
<p>When scientists talk about energy, for example, they mean precisely that
<br />
thing which is measured so carefully by the Bureau of Standards.
<br />
Likewise, for the physical entropy that we scientists evaluate, and
<br />
which is the subject of the Second Law of Thermodynamics. I think it's
<br />
true, Aleks, that we scientists would like to keep the term entropy (and
<br />
energy, and mass, and force, etc.) just for ourselves, so that it only
<br />
means something physical and measurable. But, that won't happen, and
<br />
hasn't, in fact, happened. However, as scientists, we must always
<br />
remember the distinction.
<br />
<p>You write, Aleks, &quot;that information, ultimately is physical. But not
<br />
Shannon information, as it is understood by the grand majority of
<br />
practitioners.&quot; I certainly agree that information is physical, and so a
<br />
fit subject for science. And it may be perfectly correct that most
<br />
researchers mean something non-physical when they say Shannon
<br />
information. My continued argument is that we scientists mean only
<br />
observable, physical information when we're analyzing information
<br />
scientifically (or we've forgotten the distinction). And that Shannon
<br />
obviously meant physical information also, since that's exactly what he
<br />
wrote.
<br />
<p><p>Aleks, if &quot;nobody is questioning the connection between S and H when we
<br />
speak about physical models&quot;, then the question must be whether
<br />
Shannon's entropy, H, depicts a physical model. If information is
<br />
physical, as you say, then obviously, Shannon was describing a physical
<br />
thing, and Shannon's entropy, H, is the physical entropy, S.
<br />
<p>Here's why it matters to scientists whether the Shannon information
<br />
described by Shannon (and by Shannon and Weaver) is physical
<br />
information. It's not just a choice of words to describe physical
<br />
information, but we also recognize that Shannon's subject, his
<br />
derivation, and his equation all described a physical system. So, we may
<br />
properly use his model, his methods and his equation to describe our own
<br />
physical systems.
<br />
<p>And, that equation is, indeed, appropriate in scientific work.
<br />
Scientists have routinely and continuously used Shannon's formula, and
<br />
von Neumann's formula and Boltzmann's formula before that, to describe
<br />
physical systems. What may be called a probabilistic equation, describes
<br />
a physical system, or else Boltsmann, von Neumann, and every scientist
<br />
in between, was mistaken in so applying it. (They weren't.)
<br />
<p>I think Jaynes greatest contribution to a real understanding of the
<br />
connection between information and physical entropy is his Principle of
<br />
Maximum Entropy (reviewed by Grandy in his resource letter). In the
<br />
article you cite on image reconstruction, Jaynes defines H to be &quot;the
<br />
maximum entropy per dot, H = log(W)/N....(which) goes asymptotically
<br />
into the Shannon entropy -Sum (N-sub-i/N)log(N-sub-i/N).&quot; It's
<br />
reasonable, in this context, to call H the maximum entropy because it
<br />
becomes Shannon's entropy in the limit. Boltzmann's H is not the maximum
<br />
entropy, in general, but rather, the system entropy which becomes
<br />
maximum at equilibrium. (That's what Boltzmann's H theorem tells us.)
<br />
Thus, the Sum P log (P) expression you mention, is not an approximation
<br />
to thermodynamic entropy, but is actually thermodynamic entropy itself.
<br />
That is Boltzmann's H (and Shannon's H), and both are approximated (in
<br />
the limit) by the symbol H chosen by Jaynes here.
<br />
<p>The accepted distinction between physical entropy and probabilistic
<br />
entropy would solve the problem, I believe, Loet, if we also recognize
<br />
that Shannon's equation (sometimes called probabilistic) is entirely
<br />
appropriate for physical entropy as well as for the other analyses to
<br />
which non-scientists usefully apply it. (These other analyses, I assume,
<br />
Aleks, are the non-physical probabilistic models to which you refer.)
<br />
<p>When we scientists use Shannon information in our research, Aleks, it
<br />
means, as Shannon and Weaver wrote, &quot;the quantity which uniquely meets
<br />
the natural requirements that one sets up for 'information' (and) turns
<br />
out to be exactly that which is known in thermodynamics as entropy.&quot;
<br />
Though I can't point to an example now, I'm sure, of course, that some
<br />
scientist, at some time, has forgotten (or ignored) Shannon's
<br />
publication, and has misapplied the term 'Shannon entropy'.
<br />
<p>You write Aleks, that you've &quot;seen no indication that Shannon would mean
<br />
what&quot; I've said he means. Have you read the book by Shannon and Weaver?
<br />
They were scientists writing about a technical subject for an audience
<br />
of other sophisticated researchers. Authors, in such cases, try to be
<br />
precise and clear about what they mean. And, the publication, of course,
<br />
is designed to provide a permanent record of the author's ideas.
<br />
<p>The english language has not changed so much in fifty-five years that
<br />
one can reasonably misinterpret the lucid statement &quot;the quantity which
<br />
uniquely meets the natural requirements that one sets up for
<br />
'information' turns out to be exactly that which is known in
<br />
thermodynamics as entropy.&quot; And, I continue to mean that Shannon's
<br />
information is the same thing as thermodynamic entropy. One doesn't have
<br />
to be a &quot;theologist or literary critic&quot; to recognize that Shannon and
<br />
Weaver meant the same thing.
<br />
<p>My thanks to the FIS forum for this opportunity to defend these
<br />
important ideas.
<br />
<p>Cordially,
<br />
<p>Michael Devereux
<br />
<p>_______________________________________________
<br />
fis mailing list
<br />
<a href="mailto:fis&#64;listas.unizar.es?Subject=RE:%20[Fis]%20Physical%20Information%20is%20Shannon%20Entropy,%20Part%20V.">fis@listas.unizar.es</a> <a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
<br />
<span id="received"><dfn>Received on</dfn> Mon Jun 28 05:58:53 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1614.html" title="Next message in the list">hoelzer@unr.edu: "Re: [Fis] 2004 FIS session: concluding comments"</a></li>
<li><dfn>Previous message</dfn>: <a href="1612.html" title="Previous message in the list">Stanley N. Salthe: "Re: [Fis] 2004 FIS session: concluding comments"</a></li>
<li><dfn>In reply to</dfn>: <a href="1607.html" title="Message to which this message replies">Michael Devereux: "[Fis] Physical Information is Shannon Entropy, Part V."</a></li>
<!-- lnextthread="start" -->
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1613" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1613" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1613" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1613" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:47 CET
</em></small></p>
</body>
</html>
