<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: Nature of Complexity</title>
<meta name="Author" content="jlrchand@erols.com (jlrchand@erols.com)" />
<meta name="Subject" content="Nature of Complexity" />
<meta name="Date" content="2002-10-13" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>Nature of Complexity</h1>
<!-- received="Sun Oct 13 18:26:56 2002" -->
<!-- isoreceived="20021013162656" -->
<!-- sent="Sun, 13 Oct 2002 18:26:42 +0200 (MET DST)" -->
<!-- isosent="20021013162642" -->
<!-- name="jlrchand@erols.com" -->
<!-- email="jlrchand@erols.com" -->
<!-- subject="Nature of Complexity" -->
<!-- id="v04210101b9cf4c4a18cc@[66.44.64.217]" -->
<!-- charset="iso-8859-1" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="0800.html" title="elohimjl: &quot;Systems Thinking? Yes, but holistically conceived&quot;">Next message</a> ]
[ <a href="0798.html" title="John Collier: &quot;virus&quot;">Previous message</a> ]
<!-- unextthread="start" -->
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: &lt;<a href="mailto:jlrchand&#64;erols.com?Subject=Re:%20Nature%20of%20Complexity">jlrchand@erols.com</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Sun 13 Oct 2002 - 18:26:42 CEST</span><br />
</address>
<p>
Dear Colleagues:
<br />
<p>The ongoing discussions of WESSbook have stimulated evaluations of 
<br />
basic aspects of metaphysics and science.  The following essay by 
<br />
Steven Weinberg overlaps some aspects of our WESSbook discussion and 
<br />
provides an articulate view of relations between physics and 
<br />
computation.  Has Weinberg captured the essence of the distinction 
<br />
between mathematics, theoretical physics and computation?
<br />
<p>This email also serves as a preliminary notice of the upcoming 
<br />
WESSbook meeting. The next WESSbook discussion is being organized for 
<br />
Nov. 16, 2002.  We are seeking volunteers to address the issue of the 
<br />
Nature of Mind.  Please call me at 703-790-1651 if you would like 
<br />
further information.
<br />
<p><p>Cheers
<br />
<p>Jerry
<br />
<p><p><p>The New York Review of Books
<br />
October 24, 2002
<br />
<p>Review
<br />
<p>Is the Universe a Computer?
<br />
<p>By Steven Weinberg
<br />
A New Kind of Science
<br />
by Stephen Wolfram
<br />
Wolfram Media, 1,197 pp., $44.95
<br />
<p>1.
<br />
<p>Everyone knows that electronic computers have enormously helped the 
<br />
work of science. Some scientists have had a grander vision of the 
<br />
importance of the computer. They expect that it will change our view 
<br />
of science itself, of what it is that scientific theories are 
<br />
supposed to accomplish, and of the kinds of theories that might 
<br />
achieve these goals.
<br />
<p>I have never shared this vision. For me, the modern computer is only 
<br />
a faster, cheaper, and more reliable version of the teams of clerical 
<br />
workers (then called &quot;computers&quot;) that were programmed at Los Alamos 
<br />
during World War II to do numerical calculations. But neither I nor 
<br />
most of the other theoretical physicists of my generation learned as 
<br />
students to use electronic computers. That skill was mostly needed 
<br />
for number crunching by experimentalists who had to process huge 
<br />
quantities of numerical data, and by theorists who worked on problems 
<br />
like stellar structure or bomb design. Computers generally weren't 
<br />
needed by theorists like me, whose work aimed at inventing theories 
<br />
and calculating what these theories predict only in the easiest cases.
<br />
<p>Still, from time to time I have needed to find the numerical solution 
<br />
of a differential equation,[1] and with some embarrassment I would 
<br />
have to recruit a colleague or a graduate student to do the job for 
<br />
me. It was therefore a happy day for me when I learned to use a 
<br />
program called Mathematica, written for personal computers under the 
<br />
direction of Stephen Wolfram. All one had to do was to type out the 
<br />
equations to be solved in the prescribed code, press shift-enter, 
<br />
and, presto, the answer would pop up on the monitor screen. The 
<br />
Mathematica user's manual now sits on my desk, so fat and heavy that 
<br />
it does double duty as a bookend for the other books I keep close at 
<br />
hand.
<br />
<p>Now Wolfram has written another book that is almost as heavy as the 
<br />
Mathematica user's manual, and that has attracted much attention in 
<br />
the press. A New Kind of Science describes a radical vision of the 
<br />
future of science, based on Wolfram's long love affair with 
<br />
computers. The book's publisher, Wolfram Media, announces &quot;a whole 
<br />
new way of looking at the operation of our universe&quot; and &quot;a series of 
<br />
dramatic discoveries never before made public.&quot; Wolfram claims to 
<br />
offer a revolution in the nature of science, again and again 
<br />
distancing his work from what he calls traditional science, with 
<br />
remarks like &quot;If traditional science was our only guide, then at this 
<br />
point we would probably be quite stuck.&quot; He stakes his claim in the 
<br />
first few lines of the book: &quot;Three centuries ago science was 
<br />
transformed by the dramatic new idea that rules based on mathematical 
<br />
equations could be used to describe the natural world. My purpose in 
<br />
this book is to initiate another such transformation....&quot;
<br />
<p>Usually I put books that make claims like these on the crackpot shelf 
<br />
of my office bookcase. In the case of Wolfram's book, that would be a 
<br />
mistake. Wolfram is smart, winner of a MacArthur Fellowship at age 
<br />
twenty-two, and the progenitor of the invaluable Mathematica, and he 
<br />
has lots of stimulating things to say about computers and science. I 
<br />
don't think that his book comes close to meeting his goals or 
<br />
justifying his claims, but if it is a failure it is an interesting 
<br />
one.
<br />
------------------------------------------------------------------------
<br />
<p>The central theme of the book is easily stated. It is that many 
<br />
simple rules can lead to complex behavior. The example that is used 
<br />
repeatedly to illustrate this theme is a favorite toy of complexity 
<br />
theorists known as the cellular automaton, so I will have to say a 
<br />
bit about what cellular automata are.
<br />
<p>Take a piece of white graph paper that has been cross-hatched into 
<br />
little squares. These are the &quot;cells.&quot; Blacken one or more of the 
<br />
cells in the top row, chosen any way you like, leaving all the others 
<br />
white. This is your input. Now blacken some cells in the second row, 
<br />
according to some fixed rule that tells you to make any cell black or 
<br />
leave it white depending on the colors of its three neighboring cells 
<br />
in the first row (that is, the cells in the first row that are either 
<br />
immediately above the cell in the second row or one cell over to the 
<br />
right or left.) Then use the same rule, whatever it is, automatically 
<br />
to color each cell in the third row according to the colors of its 
<br />
three neighboring cells in the second row, and keep going 
<br />
automatically in the same way to the rows below. The coloring rule 
<br />
used in this way is an elementary cellular automaton.
<br />
<p>This may seem like a solitaire variation on tick-tack-toe, only not 
<br />
as exciting. Indeed, most of the 256 possible[2] elementary cellular 
<br />
automata of this sort are pretty boring. For instance, consider rule 
<br />
254, which dictates that a cell is made black if the cell immediately 
<br />
above it, or above it and one space over to the left or right, is 
<br />
black, and otherwise it is left white. Whatever the input pattern of 
<br />
black cells in the top row, the black cells will spread in the rows 
<br />
below, eventually filling out an expanding black triangle, so that 
<br />
the cells in any given column will all be black once you get to a 
<br />
low-enough row.
<br />
<p>But wait. Wolfram's prize automaton is number 110 in his list of 256. 
<br />
Rule 110 dictates that a cell in one row is left white if the three 
<br />
neighboring cells in the row above are all black or all white or 
<br />
black-white-white, and otherwise it is made black. Figure A shows the 
<br />
result of applying this rule twenty times with a very simple input, 
<br />
in which just one cell is made black in the top row. Not much 
<br />
happening here. Wolfram programmed a computer to run this automaton, 
<br />
and he ran it for millions of steps. After a few hundred steps 
<br />
something surprising happened: the rule began to produce a remarkably 
<br />
rich structure, neither regular nor completely random. The result 
<br />
after 700 steps is shown in Figure B. A pattern of black cells 
<br />
spreads to the left, with a foamy strip furthest to the left, then a 
<br />
periodic alternation of regions of greater and lesser density of 
<br />
black cells which moves to the right, followed by a jumble of black 
<br />
and white cells. It is a dramatic demonstration of Wolfram's 
<br />
conclusion, that even quite simple rules and inputs can produce 
<br />
complex behavior.
<br />
------------------------------------------------------------------------
<br />
<p>Wolfram is not the first to have worked with cellular automata. They 
<br />
had been studied for decades by a group headed by Edward Fredkin at 
<br />
MIT, following the ground-breaking work of John Von Neumann and 
<br />
Stanislas Ulam in the 1950s. Wolfram is also not the first to have 
<br />
seen complexity coming out of simple rules in automata or elsewhere. 
<br />
Around 1970 the Princeton mathematician John Horton Conway invented 
<br />
&quot;The Game of Life,&quot; a two-dimensional cellular automaton in which 
<br />
cells are blackened according to a rule depending on the colors of 
<br />
all the surrounding cells, not just the cells in the row above. 
<br />
Running the game produces a variety of proliferating structures 
<br />
reminiscent of microorganisms seen under a microscope. For a while 
<br />
the Game of Life was dangerously addictive for graduate students in 
<br />
physics. A decade later another mathematician, Benoit Mandelbrot, the 
<br />
inventor of fractals, gave a simple algebraic prescription for 
<br />
constructing the famous Mandelbrot set, a connected two-dimensional 
<br />
figure that shows an unbelievable richness of complex detail when 
<br />
examined at smaller and smaller scales.
<br />
<p>There are also well-known examples of complexity emerging from simple 
<br />
rules in the real world. Suppose that a uniform stream of air is 
<br />
flowing in a wind tunnel past some simply shaped obstacle, like a 
<br />
smooth solid ball. If the air speed is sufficiently low then the air 
<br />
flows in a simple smooth pattern over the surface of the ball. 
<br />
Aerodynamicists call this laminar flow. If the air speed is increased 
<br />
beyond a certain point, vortices of air appear behind the ball, 
<br />
eventually forming a regular trail of vortices called a &quot;Von Karman 
<br />
street.&quot; Then as the air speed is increased further the regularity of 
<br />
the pattern of vortices is lost, and the flow begins to be turbulent. 
<br />
The air flow is then truly complex, yet it emerges from the simple 
<br />
differential equations of aerodynamics and the simple setup of wind 
<br />
flowing past a ball.
<br />
<p>What Wolfram has done that seems to be new is to study a huge number 
<br />
of simple automata of all types, looking specifically for those that 
<br />
produce complex structures. There are cellular automata with more 
<br />
than two colors, or with coloring rules like the Game of Life that 
<br />
change the colors of cells in more than one row at a time, or with 
<br />
cells in more than two dimensions. Beyond cellular automata, there 
<br />
are also automata with extra features like memory, including the 
<br />
Turing machine, about which more later. From his explorations of 
<br />
these various automata, Wolfram has found that the patterns they 
<br />
produce fall into four classes. Some are very simple, like the 
<br />
spreading black triangle in the rule 254 elementary cellular 
<br />
automaton that I mentioned first. Other patterns are repetitive, such 
<br />
as nested patterns that repeat themselves endlessly at larger and 
<br />
larger scales. Still others seem entirely random. Most interesting 
<br />
are automata of the fourth class, of which rule 110 is a paradigm. 
<br />
These automata produce truly complex patterns, neither repetitive nor 
<br />
fully random, with complicated structures appearing here and there in 
<br />
an unpredictable way.
<br />
<p>So what does this do for science? The answer depends on why one is 
<br />
interested in complexity, and that depends in turn on why one is 
<br />
interested in science.
<br />
<p>2.
<br />
<p>Some complex phenomena are studied by scientists because the 
<br />
phenomena themselves are interesting. They may be important to 
<br />
technology, like the turbulent flow of air past an airplane, or 
<br />
directly relevant to our own lives, like memory, or just so lovely or 
<br />
strange that we can't help being interested in them, like snowflakes. 
<br />
Unfortunately, as far as I can tell, there is not one real-world 
<br />
complex phenomenon that has been convincingly explained by Wolfram's 
<br />
computer experiments.
<br />
<p>Take snowflakes. Wolfram has found cellular automata in which each 
<br />
step corresponds to the gain or loss of water molecules on the 
<br />
circumference of a growing snowflake. After adding a few hundred 
<br />
molecules some of these automata produce patterns that do look like 
<br />
real snowflakes. The trouble is that real snowflakes don't contain a 
<br />
few hundred water molecules, but more than a thousand billion billion 
<br />
molecules. If Wolfram knows what pattern his cellular automaton would 
<br />
produce if it ran long enough to add that many water molecules, he 
<br />
does not say so.
<br />
<p>Or take complex systems in biology, like the human nervous or immune 
<br />
systems. Wolfram proposes that the complexity of such systems is not 
<br />
built up gradually in a complicated evolutionary history, but is 
<br />
rather a consequence of some unknown simple rules, more or less in 
<br />
the way that the complex behavior of the pattern produced by cellular 
<br />
automaton 110 is a consequence of its simple rules. Maybe so, but 
<br />
there is no evidence for this. In any case, even if Wolfram's 
<br />
speculation were correct it would not mean that the complexity of 
<br />
biological systems has little to do with Darwinian evolution, as 
<br />
Wolfram contends. We would still have to ask why organisms obey some 
<br />
simple rules and not other rules, and the only possible answer would 
<br />
be that natural selection favors those rules that generate the kind 
<br />
of complex systems that improve reproductive fitness.
<br />
<p>Wolfram even tackles the old conflict between belief in a 
<br />
deterministic view of nature and in the existence of free will. He 
<br />
suggests that free will is an illusion that arises from the apparent 
<br />
unpredictability of the complex behavior produced by those simple 
<br />
rules of biology that he imagines to govern the human organism. This 
<br />
is odd, because we certainly don't attribute free will to other 
<br />
unpredictable complex phenomena like earthquakes or thunderstorms. 
<br />
Surely the impression of free will arises instead from our personal 
<br />
experience of actually deciding what to do, an experience that we are 
<br />
unwilling to suppose is enjoyed by earthquakes or thunderstorms. This 
<br />
is not to say that I have any enlightenment to offer about free will 
<br />
either, because I have never been able to understand the 
<br />
inconsistency that other people find between free will and a 
<br />
completely deterministic view of nature. Free will to me means only 
<br />
that we sometimes decide what we do, and we know that this is true by 
<br />
the same sort of mental experience that convinced Descartes that he 
<br />
existed, but we have no mental experience that tells us that our 
<br />
decisions are not inevitable consequences of past conditions and the 
<br />
laws of nature.
<br />
<p>3.
<br />
<p>Other scientists like myself study phenomena that may not be 
<br />
intrinsically very interesting, because they think that studying 
<br />
these phenomena will help them to understand the laws of nature, the 
<br />
rules that govern all phenomena. In such work, we tend to study the 
<br />
simplest possible phenomena, because it is in these cases that we can 
<br />
most easily calculate what our theories predict, and compare the 
<br />
results with experimental data to decide whether our theories are 
<br />
right or wrong. Wolfram makes it seem that physicists choose simple 
<br />
rather than complex phenomena to study because of long habit or 
<br />
mathematical flabbiness, but in seeking the laws of nature it is the 
<br />
essence of the art of science to avoid complexity.
<br />
<p>My own work has been mostly on the theory of elementary particles, 
<br />
but I have never found these particles very interesting in 
<br />
themselves. An electron is featureless, and much like every other 
<br />
electron. Most of those of us who study elementary particles do so 
<br />
because we think that at this moment in the history of science it is 
<br />
the best way to discover the laws that govern all natural phenomena. 
<br />
Because of this special motivation, we don't generally care whether 
<br />
we can calculate everything that happens to elementary particles in 
<br />
complicated situations, only whether we can calculate enough to check 
<br />
the validity of our theories. In collisions of elementary particles 
<br />
at moderate energy the energy of the collision goes into complex 
<br />
showers of particles. No one can predict the details of these 
<br />
showers, even where the underlying theory is known, and hardly anyone 
<br />
cares. At higher energies things are simpler: the energy goes into 
<br />
well-defined jets, each containing particles that travel in the same 
<br />
direction, in a way that can be calculated theoretically and compared 
<br />
with experiment to test our theories of elementary particles. It is 
<br />
the laws, not the phenomena, that interest us.
<br />
<p>Unlike elementary particles, planets have historically seemed 
<br />
interesting for religious and astrological reasons. But it was the 
<br />
simplicity of planetary motions that allowed Newton to discover the 
<br />
laws of motion and gravitation. The planets move in empty space and 
<br />
to a good approximation under the influence of a single motionless 
<br />
body, the sun. Newton would never have discovered his laws by 
<br />
studying turbulence or snowflakes.
<br />
<p>Wolfram himself is a lapsed elementary particle physicist, and I 
<br />
suppose he can't resist trying to apply his experience with digital 
<br />
computer programs to the laws of nature. This has led him to the view 
<br />
(also considered in a 1981 article by Richard Feynman) that nature is 
<br />
discrete rather than continuous. He suggests that space consists of a 
<br />
network of isolated points, like cells in a cellular automaton, and 
<br />
that even time flows in discrete steps. Following an idea of Edward 
<br />
Fredkin, he concludes that the universe itself would then be an 
<br />
automaton, like a giant computer. It's possible, but I can't see any 
<br />
motivation for these speculations, except that this is the sort of 
<br />
system that Wolfram and others have become used to in their work on 
<br />
computers. So might a carpenter, looking at the moon, suppose that it 
<br />
is made of wood.
<br />
<p>&nbsp;
<br />
<p>There is another reason for studying complex phenomena-not because 
<br />
the phenomena are interesting, which they sometimes are, or because 
<br />
studying complex phenomena is a good way to learn the laws of nature, 
<br />
which it isn't, but because complexity itself is interesting. Maybe 
<br />
there is a theory of complexity waiting to be discovered, that says 
<br />
simple things about complex behavior in general, not just about the 
<br />
rule 110 cellular automaton or turbulent air flow or the human 
<br />
nervous system.
<br />
<p>There are other examples of what I like to call free-floating 
<br />
theories, theories that are applicable in a wide (though not 
<br />
unlimited) variety of very different contexts. The theory of chaos, 
<br />
which has captured the public imagination, deals with systems, from 
<br />
the weather to the pebbles in the rings of Saturn, whose behavior 
<br />
exhibits an exquisite sensitivity to initial conditions. 
<br />
Thermodynamics, the science of heat, is a less trendy example. 
<br />
Concepts of thermodynamics like temperature and entropy are 
<br />
applicable to black holes as well as to steam boilers. A less 
<br />
familiar example is the theory of broken symmetry. Many very 
<br />
different substances, including superconductors, magnetized iron, and 
<br />
liquid helium, are governed by equations that have some symmetry, in 
<br />
the sense that the equations look the same from certain different 
<br />
points of view, and yet the substances exhibit phenomena that do not 
<br />
respect this symmetry.
<br />
<p>There is a low-intensity culture war going on between scientists who 
<br />
specialize in free-floating theories of this sort and those (mostly 
<br />
particle physicists) who pursue the old reductionist dream of finding 
<br />
laws of nature that are not explained by anything else, but that lie 
<br />
at the roots of all chains of explanation. The conflict usually comes 
<br />
to public attention only when particle physicists are trying to get 
<br />
funding for a large new accelerator. Their opponents are exasperated 
<br />
when they hear talk about particle physicists searching for the 
<br />
fundamental laws of nature. They argue that the theories of heat or 
<br />
chaos or complexity or broken symmetry are equally fundamental, 
<br />
because the general principles of these theories do not depend on 
<br />
what kind of particles make up the systems to which they are applied. 
<br />
In return, particle physicists like me point out that, although these 
<br />
free-floating theories are interesting and important, they are not 
<br />
truly fundamental, because they may or may not apply to a given 
<br />
system; to justify applying one of these theories in a given context 
<br />
you have to be able to deduce the axioms of the theory in that 
<br />
context from the really fundamental laws of nature.
<br />
<p>This debate is unfortunate, for both kinds of science are valuable, 
<br />
and they often have much to teach each other. My own work in 
<br />
elementary particle physics has benefited tremendously from the idea 
<br />
of broken symmetry, which originated in the study of the solid state 
<br />
but turned out to be the key both to understanding reactions 
<br />
involving particles called pi mesons at low energy and to the 
<br />
unification of some of the forces acting on elementary particles. The 
<br />
theory of complexity might also have lessons for elementary particle 
<br />
theory (or vice versa) but it is not likely to be fundamental in the 
<br />
same sense as elementary particle physics.
<br />
<p>Lately particle physicists have been having trouble holding up their 
<br />
end of this debate. Progress toward a fundamental theory has been 
<br />
painfully slow for decades, largely because the great success of the 
<br />
&quot;Standard Model&quot; developed in the 1960s and 1970s has left us with 
<br />
fewer puzzles that could point to our next step. Scientists studying 
<br />
chaos and complexity also like to emphasize that their work is 
<br />
applicable to the rich variety of everyday life, where elementary 
<br />
particle physics has no direct relevance.
<br />
<p>Scientists studying complexity are particularly exuberant these days. 
<br />
Some of them discover surprising similarities in the properties of 
<br />
very different complex phenomena, including stock market 
<br />
fluctuations, collapsing sand piles, and earthquakes. Often these 
<br />
phenomena are studied by simulating them with cellular automata, such 
<br />
as Conway's Game of Life. This work is typically done in university 
<br />
physics departments and in the interdisciplinary Santa Fe Institute. 
<br />
Other scientists who call themselves complexity theorists work in 
<br />
university departments of computer science and mathematics and study 
<br />
the way that the number of steps in a computer calculation of the 
<br />
behavior of various systems increases with the size of the systems, 
<br />
often using automata like the Turing machine as specific examples of 
<br />
computers. Some of the systems they study, such as the World Wide 
<br />
Web, are quite complex. But all this work has not come together in a 
<br />
general theory of complexity. No one knows how to judge which complex 
<br />
systems share the properties of other systems, or how in general to 
<br />
characterize what kinds of complexity make it extremely difficult to 
<br />
calculate the behavior of some large systems and not others. The 
<br />
scientists who work on these two different types of problem don't 
<br />
even seem to communicate very well with each other. Particle 
<br />
physicists like to say that the theory of complexity is the most 
<br />
exciting new thing in science in a generation, except that it has the 
<br />
one disadvantage of not existing.
<br />
------------------------------------------------------------------------
<br />
<p>It is here I think that Wolfram's book may make a useful 
<br />
contribution. Wolfram and his co-workers have been able to show that 
<br />
numerous simple &quot;class four&quot; automata that produce complex behavior, 
<br />
like the rule 110 cellular automaton, are able to emulate each other. 
<br />
That is, by setting up a suitable input pattern of black and white 
<br />
cells in the rule 110 cellular automaton, one can produce the same 
<br />
complex pattern that would be produced by other class four automata, 
<br />
and vice versa. (In this emulation, blocks of cells in one automaton 
<br />
represent a single cell in the automaton being emulated.) What makes 
<br />
this particularly interesting is that one of the automata that can be 
<br />
emulated in this way is the universal Turing machine.[3]
<br />
<p>The Turing machine is the most important automaton in the history of 
<br />
computer science, and the forerunner of today's digital computers. It 
<br />
was invented in 1936 by Alan Turing, who in World War II became one 
<br />
of Britain's ace cipher-breakers and was later the hero of Hugh 
<br />
Whitemore's play Breaking the Code. Turing's purpose was to answer a 
<br />
classic question of mathematical logic known as the Decision Problem: 
<br />
given some deductive mathematical system like arithmetic or Euclidean 
<br />
geometry or symbolic logic, is there any logical method that, when 
<br />
applied mechanically to any statement of that system, is guaranteed 
<br />
to decide whether that statement can be proved by following the rules 
<br />
of that system?[4]
<br />
<p>To answer this question, the Turing machine was designed to capture 
<br />
the essence of mechanical logical methods. Just as a person going 
<br />
through a mathematical proof works with a string of symbols, focusing 
<br />
on just one at a time, the Turing machine works on a one-dimensional 
<br />
sequence of cells, each containing a symbol taken from some finite 
<br />
list, with only one &quot;active&quot; cell that can be read and possibly 
<br />
changed at each step. Also, to correspond to the fact that a person 
<br />
working out a proof would keep some memory of previous steps, Turing 
<br />
gave his machine a memory register, which can be in any one of a 
<br />
finite number of &quot;conditions.&quot;
<br />
<p>Each type of Turing machine obeys a fixed rule that tells it at each 
<br />
step how to change the symbol in the active cell, how to change the 
<br />
condition of the memory register, and whether to move the active cell 
<br />
one step to the left or right, according to the symbol in the active 
<br />
cell and the condition of the memory register. It makes no difference 
<br />
what symbols we choose to use or what conditions are possible for the 
<br />
memory register; their significance arises only from the rules of the 
<br />
machine. The problem to be solved and the data to be used are fed 
<br />
into the machine as an initial string of symbols, and the answer 
<br />
appears as the string of symbols found when the memory register 
<br />
reaches a condition that tells the machine to stop.
<br />
<p>Turing never actually built such a machine (though he did go on to 
<br />
build some special-purpose computers), but if you like you can think 
<br />
of the cells in a Turing machine as forming a paper tape, with the 
<br />
symbols just a sequence of colored dots on the tape, read and written 
<br />
by a scanning device that moves up or down the tape from one active 
<br />
cell to another. In this example the memory register is a simple 
<br />
mechanical pointer which can take any of two or more positions. The 
<br />
decision how to change the color of the active cell and the position 
<br />
of the memory register and how to move the tape is made according to 
<br />
the color of the cell being read and the position of the pointer, 
<br />
following rules that are hard-wired into the machine. A specific 
<br />
Turing machine is entirely characterized by the number of possible 
<br />
colors on the cells of the tape, the number of possible positions of 
<br />
the pointer, and by the rules wired into the machine.
<br />
------------------------------------------------------------------------
<br />
<p>The important thing about Turing machines is that some of them are 
<br />
universal. Turing was able to prove that any of these universal 
<br />
Turing machines could calculate or prove anything that could be 
<br />
calculated or proved by any other Turing machine. For instance, at 
<br />
least one of the huge number (96 to the power 48) of the possible 
<br />
Turing machines that use two colors and have a memory register with 
<br />
twenty-four positions is universal. Further, from the way that Turing 
<br />
machines were designed to imitate the way humans mechanically do 
<br />
calculations, Turing argued that universal Turing machines could 
<br />
calculate or prove anything that could be calculated or proved by any 
<br />
purely mechanical procedure. This is often called the Church-Turing 
<br />
thesis, because at about the same time the Princeton mathematician 
<br />
Alonzo Church reached similar conclusions about a more abstract but 
<br />
equivalent mathematical method of his own. Incidentally, Turing and 
<br />
Church found that the answer to the Decision Problem in most 
<br />
mathematical or logical systems is &quot;no&quot;: there is no mechanical 
<br />
procedure that is guaranteed to decide whether any given statement 
<br />
can be proved by following the rules of that system.
<br />
<p>Since universal Turing machines can be emulated by the rule 110 
<br />
cellular automaton, it follows that this cellular automaton, and all 
<br />
the other automata that can emulate it, are also universal- they can 
<br />
do any computation that can be done by any computer. The program for 
<br />
the calculation and the data to be used would be fed into a rule 110 
<br />
cellular automaton as a pattern of black cells in the top row, and 
<br />
the answer would appear as a pattern on a lower row. Wolfram says 
<br />
that all these automata are computationally equivalent, but that is 
<br />
only true in a limited sense. The simpler the design of a universal 
<br />
computer, the more steps it takes to emulate each single step of a 
<br />
practical computer. This is why Dell and Compaq do not sell Turing 
<br />
machines or rule 110 cellular automata.
<br />
<p>Wolfram goes on to make a far-reaching conjecture, that almost all 
<br />
automata of any sort that produce complex structures can be emulated 
<br />
by any one of them, so they are all equivalent in Wolfram's sense, 
<br />
and they are all universal. This doesn't mean that these automata are 
<br />
computationally equivalent (even in Wolfram's sense) to systems 
<br />
involving quantities that vary continuously. Only if Wolfram were 
<br />
right that neither space nor time nor anything else is truly 
<br />
continuous (which is a separate issue) would the Turing machine or 
<br />
the rule 110 cellular automaton be computationally equivalent to an 
<br />
analog computer or a quantum computer or a brain or the universe. But 
<br />
even without this far-reaching (and far- out) assumption, Wolfram's 
<br />
conjecture about the computational equivalence of automata would at 
<br />
least provide a starting point for a theory of any sort of complexity 
<br />
that can be produced by any kind of automaton.
<br />
------------------------------------------------------------------------
<br />
<p>The trouble with Wolfram's conjecture is not only that it has not 
<br />
been proved-a deeper trouble is that it has not even been stated in a 
<br />
form that could be proved. What does Wolfram mean by complex? If his 
<br />
conjecture is not to be a tautology, then we must have some 
<br />
definition of complex behavior independent of the notion of 
<br />
universality. The pattern produced by the rule 110 cellular automaton 
<br />
certainly looks complex, but what criterion for complexity can we use 
<br />
that would tell us that it is complex enough for Wolfram's conjecture 
<br />
to apply?
<br />
<p>There is a well-known parallel problem in defining randomness. The 
<br />
most common precise definition of the randomness of a string of 
<br />
digits or of a sequence of black and white cells on a tape is that it 
<br />
is random if there is no way of describing it with a string of 
<br />
shorter length. The trouble is that according to this definition the 
<br />
string of digits in a number like the square root of two would not 
<br />
qualify as random, because it can be described very simply -it is the 
<br />
square root of two-even though it surely looks random. (Mathematica 
<br />
gives the first thirty digits as 1.41421356237309504880168872420.) In 
<br />
the same way, it wouldn't do to define the output (shown in Figure B) 
<br />
of a cellular automaton like rule 110 with a single black cell in the 
<br />
top row as complex only if it can't be described in simple terms, 
<br />
because it can be described in simple terms-it is the output of rule 
<br />
110 with a single black cell in the top row. There are other 
<br />
definitions of randomness, such as the absence of correlations: the 
<br />
digits in the square root of two can be said to be random because, as 
<br />
far as is known, being given one digit at an unidentified decimal 
<br />
place tells you nothing at all about what the next digit is likely to 
<br />
be. Wolfram has not even begun to formulate a similar definition of 
<br />
complexity.
<br />
<p>In fact, as he admits, for Wolfram the real test of the complexity of 
<br />
a pattern is that it should look complex. Much of his discussion of 
<br />
complexity is anecdotal, relying on pictures of the patterns produced 
<br />
by specific automata that he has known. In this, Wolfram is allying 
<br />
himself with one side in the ancient struggle between what (with much 
<br />
oversimplification) one might call cultures of the image and cultures 
<br />
of the word. In our own time it has surfaced in the competition 
<br />
between television and newspapers and between graphical user 
<br />
interfaces and command line interfaces in computer operating systems.
<br />
<p>The culture of images has had the better of it lately. For a while 
<br />
the culture of the word had seemed to have scored a victory with the 
<br />
introduction of sound into motion pictures. In Sunset Boulevard, 
<br />
Norma Desmond recalls that in silent films, &quot;We didn't need dialogue. 
<br />
We had faces.&quot; But now movies can go on for long stretches with no 
<br />
words, only the thunk of cars running into each other and the sizzle 
<br />
of light sabers. The ascendancy of the culture of the image has been 
<br />
abetted by computers and the study of complexity, which have made 
<br />
possible the simulation of complex visual images.
<br />
<p>I am an unreconstructed believer in the importance of the word, or 
<br />
its mathematical analogue, the equation. After looking at hundreds of 
<br />
Wolfram's pictures, I felt like the coal miner in one of the comic 
<br />
sketches in Beyond the Fringe, who finds the conversation down in the 
<br />
mines unsatisfying: &quot;It's always just 'Hallo, 'ere's a lump of coal.'&quot;
<br />
<p>Wolfram's classification of the patterns produced by cellular 
<br />
automata dates from the early 1980s, and the discovery that the rule 
<br />
110 elementary cellular automaton is a universal computer was made in 
<br />
the early 1990s. Since then, none of this work has had much of an 
<br />
impact on the research of other scientists, aside from Wolfram's 
<br />
employees. The strongest reaction I have seen by scientists to this 
<br />
new book has been outrage at Wolfram's exaggeration of the importance 
<br />
of his own contributions to the study of complexity. Wolfram's survey 
<br />
of the complex patterns produced by automata may yet attract the 
<br />
attention of other scientists if it leads to some clear and simple 
<br />
mathematical statement about complexity. I doubt if even Wolfram 
<br />
cares what picture is produced by the rule 110 cellular automaton 
<br />
after a billion steps. But if Wolfram can give a precise statement of 
<br />
his conjecture about the computational equivalence of almost all 
<br />
automata that produce complex patterns and prove that it is true, 
<br />
then he will have found a simple common feature of complexity, which 
<br />
would be of real interest. In the study of anything outside human 
<br />
affairs, including the study of complexity, it is only simplicity 
<br />
that can be interesting
<br />
<p>Notes
<br />
<p>[1] A differential equation gives a relation between the value of 
<br />
some varying quantity and the rate at which that quantity is 
<br />
changing, and perhaps the rate at which that rate is changing, and so 
<br />
on. The numerical solution of a differential equation is a table of 
<br />
values of the varying quantity, that to a good approximation satisfy 
<br />
both the differential equation and some given conditions on the 
<br />
initial values of this quantity and of its rates of change.
<br />
<p>[2] The automaton must tell you the color of a cell in one row for 
<br />
each of the 2 x 2 x 2 = 8 possible color patterns of the three 
<br />
neighboring cells in the row above, and the number of ways of making 
<br />
these eight independent decisions between two colors is 28 = 256. In 
<br />
the same way, if there were 3 possible colors, then the number of 
<br />
coloring decisions that would have to be specified by an elementary 
<br />
cellular automaton would be 3 x 3 x 3 = 27, and the number of 
<br />
automata (calculated using Mathematica) would be 327 = 7625597484987.
<br />
<p>[3] Wolfram says that the main elements of the proof were found in 
<br />
1994 by one of his assistants, Matthew Cook, and he gives an 
<br />
unreadable updated version in this book, along with an admission that 
<br />
a few errors may still remain. I gather that the proof has not been 
<br />
published in a refereed journal. An article in Nature by Jim Giles 
<br />
titled &quot;What Kind of Science Is This?&quot; (May 16, 2002) reports that 
<br />
when Cook left his job with Wolfram in 1998, he gave a talk on his 
<br />
work at the Santa Fe Institute, but the talk did not appear in the 
<br />
conference proceedings; Wolfram took legal action against Cook, 
<br />
arguing that Cook was in breach of agreements that prevented him from 
<br />
publishing until after the publication of Wolfram's book.
<br />
<p><p>[4] Turing took pains to point out that this issue had not been 
<br />
settled by the famous 1931 theorem of Kurt Gödel, which states that 
<br />
there are statements in the general system of mathematics presented 
<br />
in the Principia Mathematica of Bertrand Russell and Alfred North 
<br />
Whitehead that can be neither proved nor disproved by following the 
<br />
rules of that system.
<br />
<p><p>---------------------------------------------------------------------- 
<br />
--Home · Your account · Current issue · Archives · Subscriptions · 
<br />
Calendar · Newsletters · Gallery · Books
<br />
<p>Copyright © 1963-2002 NYREV, Inc. All rights reserved. Nothing in 
<br />
this publication may be reproduced without the permission of the 
<br />
publisher. Illustrations copyright © David Levine unless otherwise 
<br />
noted; unauthorized use is strictly prohibited. Please contact 
<br />
web&#64;nybooks.com with any questions about this site. The cover date of 
<br />
the next issue will be November 7, 2002.
<br />
<span id="received"><dfn>Received on</dfn> Sun Oct 13 18:26:56 2002</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="0800.html" title="Next message in the list">elohimjl: "Systems Thinking? Yes, but holistically conceived"</a></li>
<li><dfn>Previous message</dfn>: <a href="0798.html" title="Previous message in the list">John Collier: "virus"</a></li>
<!-- lnextthread="start" -->
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#799" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#799" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#799" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#799" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:46 CET
</em></small></p>
</body>
</html>
