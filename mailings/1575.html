<?xml version="1.0" encoding="windows-1252"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252" />
<meta name="generator" content="hypermail 2.1.8, see http://www.hypermail.org/" />
<title>fis: [Fis] Shannon Entropy Redux</title>
<meta name="Author" content="Michael Devereux (dbar_x@cybermesa.com)" />
<meta name="Subject" content="[Fis] Shannon Entropy Redux" />
<meta name="Date" content="2004-06-10" />
<style type="text/css">
/*<![CDATA[*/
/* To be incorporated in the main stylesheet, don't code it in hypermail! */
body {color: black; background: #ffffff}
dfn {font-weight: bold;}
pre { background-color:inherit;}
.head { border-bottom:1px solid black;}
.foot { border-top:1px solid black;}
th {font-style:italic;}
table { margin-left:2em;}map ul {list-style:none;}
#mid { font-size:0.9em;}
#received { float:right;}
address { font-style:inherit ;}
/*]]>*/
.quotelev1 {color : #990099}
.quotelev2 {color : #ff7700}
.quotelev3 {color : #007799}
.quotelev4 {color : #95c500}
</style>
</head>
<body>
<div class="head">
<h1>[Fis] Shannon Entropy Redux</h1>
<!-- received="Thu Jun 10 08:38:06 2004" -->
<!-- isoreceived="20040610063806" -->
<!-- sent="Thu, 10 Jun 2004 00:34:09 -0600" -->
<!-- isosent="20040610063409" -->
<!-- name="Michael Devereux" -->
<!-- email="dbar_x@cybermesa.com" -->
<!-- subject="[Fis] Shannon Entropy Redux" -->
<!-- id="40C800E1.3030709@cybermesa.com" -->
<!-- charset="windows-1252" -->
<!-- expires="-1" -->
<map id="navbar" name="navbar">
<ul class="links">
<li>
<dfn>This message</dfn>:
[ <a href="#start" name="options1" id="options1" tabindex="1">Message body</a> ]
 [ <a href="#options2">More options</a> ]
</li>
<li>
<dfn>Related messages</dfn>:
<!-- unext="start" -->
[ <a href="1576.html" title="Pedro C. Marijuán: &quot;[Fis] FIS CALENDAR&quot;">Next message</a> ]
[ <a href="1574.html" title="Michel Petitjean: &quot;[Fis] Distinguishability (was: Re[2]: definition(s) of disorder/chaos: an example)&quot;">Previous message</a> ]
<!-- unextthread="start" -->
[ <a href="1580.html" title="Loet Leydesdorff: &quot;RE: [Fis] Shannon Entropy Redux&quot;">Next in thread</a> ]
 [ <a href="#replies">Replies</a> ]
<!-- ureply="end" -->
</li>
</ul>
</map>
</div>
<!-- body="start" -->
<div class="mail">
<address class="headers">
<span id="from">
<dfn>From</dfn>: Michael Devereux &lt;<a href="mailto:dbar_x&#64;cybermesa.com?Subject=Re:%20[Fis]%20Shannon%20Entropy%20Redux">dbar_x@cybermesa.com</a>&gt;
</span><br />
<span id="date"><dfn>Date</dfn>: Thu 10 Jun 2004 - 08:34:09 CEST</span><br />
</address>
<p>
Dear Loet, Aleks and colleagues,
<br />
<p>It seems to me Loet, that you are reinforcing my contention that the 
<br />
Shannon information is only one of many meanings of the term information 
<br />
in common usage among researchers. You recently wrote that we ought to 
<br />
“distinguish sharply between Shannon-type information which remains 
<br />
content-free (bits of information) and meaningful information which is 
<br />
the result of an interaction between the Shannon-type information and 
<br />
the system which provides the information with meaning.” I might even 
<br />
accuse you of a hint of reductionism in this argument, which I think is 
<br />
the correct one.
<br />
<p>I believe I understand better now what you mean by content-free 
<br />
information. I’ve continued to maintain that Shannon had something 
<br />
specific in mind by the information he modeled mathematically. One might 
<br />
interpret the Shannon information as carrying the context, or “content” 
<br />
of uncertainty in the probability distribution of a physical system, the 
<br />
same thing Boltzmann described. (Really, the information erases the 
<br />
uncertainty, or entropy, in that distribution, as Jaynes stipulated.) 
<br />
But I prefer your idea for content-free information as just the physical 
<br />
information bits. (I wish, Aleks, that exclusively physical information 
<br />
were my idea. It was Shannon’s idea way back in the early sixties.)
<br />
<p>So, it’s not just the mathematical form of Shannon’s equation (von 
<br />
Neumann’s equation, Boltzmann’s equation) which specifies what we mean 
<br />
by Shannon entropy. An equation of that form might profitably be applied 
<br />
in innumerable unrelated situations, and seems, in fact, to have been 
<br />
done so.
<br />
<p>By Shannon entropy we must mean what Shannon meant. (I take it, Loet, 
<br />
you wouldn’t want someone to name his suppositions Leyersdorf’s theory, 
<br />
unless it actually agreed with your own work. The words we choose really 
<br />
do matter, because they carry meaning and content, as you’ve explained. 
<br />
If we call it Shannon entropy then it ought to be exactly what Shannon 
<br />
meant by his entropy.)
<br />
<p>So, Shannon entropy is the uncertainty in the distribution of a physical 
<br />
system, just what von Neumann, and Boltzmann before him, were also 
<br />
modeling with that same equation. As distinct from incorrectly calling 
<br />
something Shannon entropy just because an equation of that form is used 
<br />
to analyze some system. If the uncertainty in the distribution doesn’t 
<br />
increase monotonically, it cannot be Shannon entropy that’s being 
<br />
described, for example.
<br />
<p>Shannon derived the form of his equation uniquely from the three 
<br />
postulates he lists in his article. We all understand, I suppose, that 
<br />
the uncertainty in the distribution of a physical system (I emphasize 
<br />
here, a PHYSICAL system) must increase monotonically with increasing N. 
<br />
That postulate is not just some arbitrary mathematical axiom. Instead, 
<br />
Shannon understood that it stipulates one of the essential traits of a 
<br />
PHYSICAL system with many freedom degrees. And, Shannon entropy is not a 
<br />
measure of any probabilistic model, thermodynamic or not, Aleks. 
<br />
Shannon’s postulate of monotonic increase depicts the probability 
<br />
distribution of a physical system.
<br />
<p>We know that Boltzmann also recognized exactly that characteristic. If 
<br />
the phase space of some physical system increases, the entropy of that 
<br />
system will increase too. It must do so as the number of freedom degrees 
<br />
provided by the expanding phase space grows. So Boltzmann taught us.
<br />
<p>I suppose, Loet, that if one were able to reduce meaningful information 
<br />
to it’s underlying Shannon information, as impractical and formidable as 
<br />
that task might be, that Shannon information would be found to decrease 
<br />
monotonically.
<br />
<p>And, I understand your skepticism, Loet, about my identification of 
<br />
Shannon entropy with Boltzmann’s entropy, since the form of the two 
<br />
equations can differ by a constant. The author of your citation at 
<br />
www.panspermia.org makes the same argument, Aleks. This is the constant 
<br />
Shannon labeled lambda, such that his equation uniquely describes the 
<br />
probability distribution of a physical system (electrical signals in a 
<br />
wire), to within that constant.
<br />
<p>Shannon was dismissive about the importance of this constant. “The 
<br />
constant K (or lambda) merely amounts to a choice of a unit of measure,” 
<br />
he wrote. Recall that the Shannon equation depicts the identical 
<br />
physical property described by von Neumann and Boltzmann, so long as it 
<br />
has the form of the Shannon equation to within a constant. That’s why I 
<br />
said Shannon entropy is identical with Boltzmann’s entropy, Loet. Not 
<br />
because the equations are identical in form, Aleks. But because the 
<br />
physical property they model is exactly the same thing.
<br />
<p>For such a description of a physical phenomenon, one may arbitrarily 
<br />
select units of measure. That’s what Boltzmann did with his constant k, 
<br />
also. Boltzmann’s entropy equation portrays a “pure” probability 
<br />
distribution, too, just like Shannon’s equation; until the constant k is 
<br />
appended.
<br />
<p>I realize that some of us here think of information in terms of the 
<br />
mathematical form of the Shannon equation, and thermodynamic entropy as 
<br />
a function of the heat and temperature of a physical object. So, I’d 
<br />
like to recommend again, if it doesn’t appear too self-serving, 
<br />
Szilard’s model engine. Seventy-five years ago Szilard made the direct 
<br />
connection between information and the Clausius entropy of a heat 
<br />
engine. His engine can only convert heat to work if the appropriate 
<br />
measurement produces operative information. I suppose that’s why I’ve 
<br />
been so sure, all along, that Shannon entropy means thermodynamic entropy.
<br />
<p>The Szilard engine is conceptually extremely simple. And I’ve found it 
<br />
extraordinarily effective for understanding the connection between 
<br />
measurement, information and thermodynamics. There is some quantum 
<br />
mechanics formalism required for a proper analysis of the engine, which 
<br />
may explain why Szilard’s proposed analysis was mistaken back in 1929. 
<br />
But for those interested in the powerful concepts explicated by his 
<br />
model, the quantum mechanics is ignorable.
<br />
<p>I appreciate this forum and the interesting comments that have prompted 
<br />
me to refine my ideas and understanding.
<br />
<p>Cordially,
<br />
<p>Michael Devereux
<br />
<p>_______________________________________________
<br />
fis mailing list
<br />
fis&#64;listas.unizar.es
<br />
<a href="../../webmail.unizar.es/mailman/listinfo/fis">http://webmail.unizar.es/mailman/listinfo/fis</a>
<br />
<span id="received"><dfn>Received on</dfn> Thu Jun 10 08:38:06 2004</span>
</div>
<!-- body="end" -->
<div class="foot">
<map id="navbarfoot" name="navbarfoot" title="Related messages">
<ul class="links">
<li><dfn>This message</dfn>: [ <a href="#start">Message body</a> ]</li>
<!-- lnext="start" -->
<li><dfn>Next message</dfn>: <a href="1576.html" title="Next message in the list">Pedro C. Marijuán: "[Fis] FIS CALENDAR"</a></li>
<li><dfn>Previous message</dfn>: <a href="1574.html" title="Previous message in the list">Michel Petitjean: "[Fis] Distinguishability (was: Re[2]: definition(s) of disorder/chaos: an example)"</a></li>
<!-- lnextthread="start" -->
<li><dfn>Next in thread</dfn>: <a href="1580.html" title="Next message in this discussion thread">Loet Leydesdorff: "RE: [Fis] Shannon Entropy Redux"</a></li>
<li><a name="replies" id="replies"></a>
<dfn>Reply</dfn>: <a href="1580.html" title="Message sent in reply to this message">Loet Leydesdorff: "RE: [Fis] Shannon Entropy Redux"</a></li>
<!-- lreply="end" -->
</ul>
<ul class="links">
<li><a name="options2" id="options2"></a><dfn>Contemporary messages sorted</dfn>: [ <a href="date.html#1575" title="Contemporary messages by date">By Date</a> ] [ <a href="index.html#1575" title="Contemporary discussion threads">By Thread</a> ] [ <a href="subject.html#1575" title="Contemporary messages by subject">By Subject</a> ] [ <a href="author.html#1575" title="Contemporary messages by author">By Author</a> ] [ <a href="attachment.html" title="Contemporary messages by attachment">By messages with attachments</a> ]</li>
</ul>
</map>
</div>
<!-- trailer="footer" -->
<p><small><em>
This archive was generated by <a href="../../www.hypermail.org/default.htm">hypermail 2.1.8</a> 
: Mon 07 Mar 2005 - 10:24:46 CET
</em></small></p>
</body>
</html>
